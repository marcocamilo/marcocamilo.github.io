<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.45">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="This project evaluates the performance of advanced NLP models and vectorization techniques for text classifcation using a resume dataset. Implementing Linear SVC, FNN, Encoder models, and BERT, the project achieved an accuracy of 91.67% with BERT. The project demonstrates how to build efficient preprocessing pipelines, optimize feature representation to enhance resource usage, and develop high-performing text classification models using Scikit-Learn and PyTorch.">

<title>BERT, Encoders and Linear Models for Resume Text Classification – marcocamilo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QQJ3PCTWK5"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QQJ3PCTWK5', { 'anonymize_ip': true});
</script>
<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/css/global.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">marcocamilo</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../portfolio.html"> 
<span class="menu-text">Portfolio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#key-findings" id="toc-key-findings" class="nav-link active" data-scroll-target="#key-findings">Key Findings</a></li>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#linear-svc" id="toc-linear-svc" class="nav-link" data-scroll-target="#linear-svc">Linear SVC</a>
  <ul>
  <li><a href="#import-packages-and-data" id="toc-import-packages-and-data" class="nav-link" data-scroll-target="#import-packages-and-data">Import Packages and Data</a></li>
  <li><a href="#baseline-linearsvc" id="toc-baseline-linearsvc" class="nav-link" data-scroll-target="#baseline-linearsvc">Baseline LinearSVC</a></li>
  <li><a href="#linearsvc-with-truncated-svd" id="toc-linearsvc-with-truncated-svd" class="nav-link" data-scroll-target="#linearsvc-with-truncated-svd">LinearSVC with Truncated SVD</a></li>
  </ul></li>
  <li><a href="#feedforward-neural-network" id="toc-feedforward-neural-network" class="nav-link" data-scroll-target="#feedforward-neural-network">Feedforward Neural Network</a>
  <ul>
  <li><a href="#import-packages-and-data-1" id="toc-import-packages-and-data-1" class="nav-link" data-scroll-target="#import-packages-and-data-1">Import Packages and Data</a></li>
  <li><a href="#dataset-and-dataloader" id="toc-dataset-and-dataloader" class="nav-link" data-scroll-target="#dataset-and-dataloader">Dataset and DataLoader</a></li>
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture">Model Architecture</a></li>
  <li><a href="#hyperparameters-and-training" id="toc-hyperparameters-and-training" class="nav-link" data-scroll-target="#hyperparameters-and-training">Hyperparameters and Training</a></li>
  <li><a href="#evaluation" id="toc-evaluation" class="nav-link" data-scroll-target="#evaluation">Evaluation</a></li>
  </ul></li>
  <li><a href="#encoder-model" id="toc-encoder-model" class="nav-link" data-scroll-target="#encoder-model">Encoder Model</a>
  <ul>
  <li><a href="#model-architecture-1" id="toc-model-architecture-1" class="nav-link" data-scroll-target="#model-architecture-1">Model Architecture</a></li>
  <li><a href="#hyperparameters-and-training-1" id="toc-hyperparameters-and-training-1" class="nav-link" data-scroll-target="#hyperparameters-and-training-1">Hyperparameters and Training</a></li>
  <li><a href="#evaluation-1" id="toc-evaluation-1" class="nav-link" data-scroll-target="#evaluation-1">Evaluation</a></li>
  </ul></li>
  <li><a href="#bert" id="toc-bert" class="nav-link" data-scroll-target="#bert">BERT</a>
  <ul>
  <li><a href="#import-packages" id="toc-import-packages" class="nav-link" data-scroll-target="#import-packages">Import Packages</a></li>
  <li><a href="#dataset-and-dataloader-1" id="toc-dataset-and-dataloader-1" class="nav-link" data-scroll-target="#dataset-and-dataloader-1">Dataset and DataLoader</a></li>
  <li><a href="#model-architecture-2" id="toc-model-architecture-2" class="nav-link" data-scroll-target="#model-architecture-2">Model Architecture</a></li>
  <li><a href="#hyperparameters-and-training-2" id="toc-hyperparameters-and-training-2" class="nav-link" data-scroll-target="#hyperparameters-and-training-2">Hyperparameters and Training</a></li>
  <li><a href="#evaluation-2" id="toc-evaluation-2" class="nav-link" data-scroll-target="#evaluation-2">Evaluation</a></li>
  </ul></li>
  <li><a href="#results-and-discussion" id="toc-results-and-discussion" class="nav-link" data-scroll-target="#results-and-discussion">Results and Discussion</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">BERT, Encoders and Linear Models for Resume Text Classification</h1>
<p class="subtitle lead">Exploring Performance of Advanced NLP Algorithms for Text Classification</p>
</div>

<div>
  <div class="description">
    This project evaluates the performance of advanced NLP models and vectorization techniques for text classifcation using a resume dataset. Implementing Linear SVC, FNN, Encoder models, and BERT, the project achieved an accuracy of 91.67% with BERT. The project demonstrates how to build efficient preprocessing pipelines, optimize feature representation to enhance resource usage, and develop high-performing text classification models using Scikit-Learn and PyTorch.
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="../assets/projects/resume-analyzer/resume-analyzer.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="../assets/projects/resume-analyzer/resume-analyzer.jpg" class="img-fluid"></a></p>
<section id="key-findings" class="level3">
<h3 class="anchored" data-anchor-id="key-findings">Key Findings</h3>
<ul>
<li><strong>Factors Influencing Model Performance</strong>: The quality of feature representations and the ability to capture contextual information were crucial in determining model effectiveness.</li>
<li><strong>Preprocessing and Vectorization</strong>: Robust preprocessing and advanced vectorization techniques significantly enhanced model performance, with all models surpassing 70% accuracy.</li>
<li><strong>Model performance</strong>
<ul>
<li><strong>BERT</strong>: Best performing model with an accuracy of <strong>91.67%</strong> with bidirectional transformers. Showcases the effectiveness of pre-trained models and transfer learning.</li>
<li><strong>Linear SVC</strong>: Achieved an accuracy of <strong>87.15%</strong> with TF-IDF vectors. Attributed to the model’s simplicity and use of effective feature representation.</li>
<li><strong>Encoder Model</strong>: Reached a <strong>74.54%</strong> accuracy. Suggests the need for further fine-tuning, given the large difference with its cousin transformer model.</li>
<li><strong>Feedforward Neural Network</strong>: Recorded the lowest accuracy at 73.15%. Highlights the model’s challenges with sequential dependencies and contextual nuances.</li>
</ul></li>
</ul>
</section>
<section id="dataset" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<p>The project utilizes the <a href="https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset">Resume Dataset</a> from LiveCareer, available at Kaggle. The dataset comprises over 2400 resumes in both string and HTML format, each labeled with their respective labeled categories. The dataset includes of the following variables:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="../assets/projects/resume-analyzer/resume-dataset.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="../assets/projects/resume-analyzer/resume-dataset.png" class="img-fluid"></a></p>
</div></div><ul>
<li><code>ID</code>: A unique identifier for each resume</li>
<li><code>Resume_str</code>: The textual content of the resume</li>
<li><code>Resume html</code>: The HTML content of the resume</li>
<li><code>Category</code>: The job field classification of each resume (e.g., Information Technology, Teaching, Advocacy, Business Development, Healthcare)</li>
</ul>
</section>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<p>Data preprocessing involved two tasks: text preprocessing and data rebalancing.</p>
<p>For text cleaning, I developed a custom&nbsp;<code>preprocessing</code>&nbsp;function that integrates several operations into a streamlined pipeline. This function is highly adaptable, with parameters to handle tasks such as converting text to lowercase, decoding HTML, removing emails and URLs, eliminating special characters, expanding contractions, applying custom regex cleaning, and performing tokenization, stemming, lemmatization, and stopword removal. In particular, I improved the text quality by eliminating noise like non-existent words and frequent, resume-specific stopwords such as months, function verbs, and section headings.</p>
<div id="df3b2b34" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bs4 <span class="im">import</span> BeautifulSoup</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> unidecode <span class="im">import</span> unidecode</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> contractions</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords, words</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem.porter <span class="im">import</span> PorterStemmer</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocessing(</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    text, </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    stem<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    lem<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    html<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    exist<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    remove_emails<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    remove_urls<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    remove_digits<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    remove_punct<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    expand_contractions<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    remove_special_chars<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    remove_stopwords<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    lst_stopwords<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    lst_regex<span class="op">=</span><span class="va">None</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">str</span> <span class="op">|</span> <span class="bu">list</span>[<span class="bu">str</span>]:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lowercase conversion</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> text.lower()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># HTML decoding</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> html:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        soup <span class="op">=</span> BeautifulSoup(cleaned_text, <span class="st">"html.parser"</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> soup.get_text()</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove Emails</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_emails:</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"([a-z0-9+._-]+@[a-z0-9+._-]+\.[a-z0-9+_-]+)"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># URL removal</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_urls:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"(http|https|ftp|ssh)://[\w_-]+(?:\.[\w_-]+)+[\w.,@?^=%&amp;:/~+#-]*[\w@?^=%&amp;/~+#-]?"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove escape sequences and special characters</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_special_chars:</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"[^\x00-\x7f]"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> unidecode(cleaned_text)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove multiple characters</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"(.)\1{3,}"</span>, <span class="vs">r"\1"</span>, cleaned_text)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Expand contractions</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> expand_contractions:</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> contractions.fix(cleaned_text)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="st">"'(?=[Ss])"</span>, <span class="st">""</span>, cleaned_text)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove digits</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_digits:</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"\d"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Punctuation removal</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_punct:</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="st">"[!</span><span class="ch">\"</span><span class="st">#$%&amp;</span><span class="ch">\\</span><span class="st">'()*+\,-./:;&lt;=&gt;?@\[\]\^_`{|}~]"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Line break and tab removal</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"[\n\t]"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Excessive spacing removal</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"\s+"</span>, <span class="st">" "</span>, cleaned_text).strip()</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regex (in case, before cleaning)</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> lst_regex: </span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> regex <span class="kw">in</span> lst_regex:</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>            compiled_regex <span class="op">=</span> re.<span class="bu">compile</span>(regex)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> re.sub(compiled_regex, <span class="st">''</span>, cleaned_text)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenization (if tokenization, stemming, lemmatization or custom stopwords is required)</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> stem <span class="kw">or</span> lem <span class="kw">or</span> remove_stopwords <span class="kw">or</span> tokenize:</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(cleaned_text, <span class="bu">str</span>):</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> cleaned_text.split()</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove stopwords</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> remove_stopwords:</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> lst_stopwords <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>                lst_stopwords <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'english'</span>))</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> cleaned_text <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> lst_stopwords]</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove non-existent words</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exist:</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>            english_words <span class="op">=</span> <span class="bu">set</span>(words.words())</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> cleaned_text <span class="cf">if</span> word <span class="kw">in</span> english_words]</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stemming</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> stem:</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>            stemmer <span class="op">=</span> PorterStemmer()</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> [stemmer.stem(word) <span class="cf">for</span> word <span class="kw">in</span> cleaned_text]</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Lemmatization</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> lem:</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>            lemmatizer <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> [lemmatizer.lemmatize(word) <span class="cf">for</span> word <span class="kw">in</span> cleaned_text]</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> tokenize:</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> <span class="st">' '</span>.join(cleaned_text)</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cleaned_text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Next, I converted the <code>Category</code> variable to a numerical format using Scikit-Learn’s <code>LabelEncoder</code> to meet the requirements of the algorithms. After cleaning, I saved the text separately for future use in tasks such as topic modeling and document similarity, where balancing is not required.</p>
<p>To address category imbalance for text classification, I employed random resampling using Scikit-Learn’s <code>resample</code> method. This approach ensures balanced representation across all categories, enhancing accuracy and reducing bias, thereby improving overall model performance.</p>
</section>
<section id="linear-svc" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="linear-svc">Linear SVC</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="../assets/projects/resume-analyzer/svc_files/svm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="../assets/projects/resume-analyzer/svc_files/svm.png" class="img-fluid"></a></p>
</div><div id="fn1"><p><sup>1</sup>&nbsp;Linear Support Vector Classifier (SVC) is a classification algorithm that seeks to find the <em>maximum-margin hyperplane</em>, that is, the hyperplane that most clearly classifies observations</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Truncated Singular Value Decomposition (SVD) is a dimensionality reduction technique that decomposes a matrix into three smaller matrices, retaining only the most significant features of the original matrix.</p></div></div><p>For this first model, I trained a baseline Linear SVC<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> using the TF-IDF vectors. Then, I performed Latent Semantic Analysis (LSA) by applying Truncated Singular Value Decomposition (SVD)<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> to reduce the TF-IDF matrix to a lower-dimensional space. The performance of both models provides a baseline for comparison with more advanced models in subsequent sections.</p>
<blockquote class="blockquote">
<h3 id="takeaways" class="anchored">Takeaways</h3>
<ul>
<li>The baseline model delivers <strong>robust accuracy while optimizing resource usage</strong>, particularly when dimensionality reduction is applied.</li>
<li>Linear SVC achieved an <strong>accuracy of 84%</strong> on the test set.</li>
<li>Linear SVC achieved <strong>81% accuracy with Truncated SVD</strong> on the test set, utilizing only 5% of the original features.</li>
</ul>
</blockquote>
<section id="import-packages-and-data" class="level3">
<h3 class="anchored" data-anchor-id="import-packages-and-data">Import Packages and Data</h3>
<p>In addition to standard libraries, I imported two custom functions: <code>classifier_report</code> to generate classification reports and confusion matrices, and <code>save_performance</code> to store model performance metrics in a JSON file for future analysis. I also loaded a pre-trained Label Encoder to label the encoded categories in subsequent visualizations.</p>
<div id="a9f838f1" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.ml <span class="im">import</span> classifier_report</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.utils <span class="im">import</span> save_performance</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the data with engineered features</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_parquet(<span class="st">'./data/3-processed/resume-features.parquet'</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Label model to label categories</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>le <span class="op">=</span> joblib.load(<span class="st">'./models/le-resumes.gz'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="baseline-linearsvc" class="level3">
<h3 class="anchored" data-anchor-id="baseline-linearsvc">Baseline LinearSVC</h3>
<p>I split the dataset into 70% for training and 30% for testing. I used the <code>tfidf_vectors</code> column as the feature matrix, stacking the vectors into a single matrix with <code>np.vstack</code>. The encoded <code>Category</code> column served as the target variable, and I extracted the variables into a NumPy array using the <code>.values</code> method to improve performance. To verify the split, I printed the shapes of the training and testing sets.</p>
<div id="1cfbb37b" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack(df[<span class="st">'tfidf_vectors'</span>].values)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'Category'</span>].values</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                                                    y, </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>                                                    test_size<span class="op">=</span><span class="fl">0.30</span>, </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>                                                    random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing set: </span><span class="sc">{</span>X_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Training set: (2016, 11618), (2016,)
Testing set: (864, 11618), (864,)</code></pre>
<p>Next, I trained a Linear SVC model with default hyperparameters and evaluated its performance using the <code>classifier_report</code> function, which generated a classification report and confusion matrix.</p>
<div id="9a5d8daf" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>svc <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> classifier_report([X_train, X_test, y_train, y_test], svc, le.classes_, <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>SVC accruacy score 87.15%</code></pre>
<p><a href="../assets/projects/resume-analyzer/svc_files/figure-markdown_strict/cell-4-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="../assets/projects/resume-analyzer/svc_files/figure-markdown_strict/cell-4-output-2.png" class="img-fluid"></a></p>
<p>The model achieved an <strong>accuracy of 87%</strong> on the test set, which was a strong result for a baseline model. However, TF-IDF vectors often lead to sparse, high-dimensional representations with low information density. To address this, I employed Truncated Singular Value Decomposition (SVD).</p>
</section>
<section id="linearsvc-with-truncated-svd" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="linearsvc-with-truncated-svd">LinearSVC with Truncated SVD</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="../assets/projects/resume-analyzer/svc_files/truncated-svd.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="../assets/projects/resume-analyzer/svc_files/truncated-svd.png" class="img-fluid"></a></p>
</div><div id="fn3"><p><sup>3</sup>&nbsp;For an in-depth explanation, see Manning, C.D., Raghavan, P., &amp; Schütze, H. (2008) <a href="https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">‘Matrix Decompositions and Latent Semantic Indexing’</a>, <em>Introduction to Information Retrieval</em>, 1.</p></div></div><p>Transforming TF-IDF matrices with Truncated SVD is known as Latent Semantic Analysis (LSA). This technique extracts the <span class="math inline">\(n\)</span> largest eigenvalues to capture the most significant semantic relationships between terms and documents, while filtering out noise and low-information features<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>I used <code>TruncatedSVD</code> to reduce the number of components to 500, which is less than 5% of the original number of features. I then applied this transformation, split the resulting feature matrix into training and testing sets, and proceeded to train the new model.</p>
<div id="cf9095ff" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>t_svd <span class="op">=</span> TruncatedSVD(n_components<span class="op">=</span><span class="dv">500</span>, algorithm<span class="op">=</span><span class="st">'arpack'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>X_svd <span class="op">=</span> t_svd.fit_transform(X)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X_train_svd, X_test_svd, y_train_svd, y_test_svd <span class="op">=</span> train_test_split(X_svd, </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                                                                    y, </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                                                                    test_size<span class="op">=</span><span class="fl">0.30</span>, </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                                                                    random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: </span><span class="sc">{</span>X_train_svd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y_train_svd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing set: </span><span class="sc">{</span>X_test_svd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y_test_svd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Training set: (2016, 500), (2016,)
Testing set: (864, 500), (864,)</code></pre>
<p>I trained a new Linear SVC model using the SVD-transformed feature matrix and generated a classification report and confusion matrix.</p>
<div id="fec44205" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>svc_svd <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> classifier_report([X_train_svd, X_test_svd, y_train_svd, y_test_svd], svc_svd, le.classes_, <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>SVC accruacy score 84.94%</code></pre>
<p><a href="../assets/projects/resume-analyzer/svc_files/figure-markdown_strict/cell-6-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="../assets/projects/resume-analyzer/svc_files/figure-markdown_strict/cell-6-output-2.png" class="img-fluid"></a></p>
<p>The model achieved an <strong>84% accuracy</strong> on the test set, which is slightly lower than the baseline model. However, this performance comes from a model <strong>trained on less than 5%</strong> of the original features, demonstrating that despite the reduced dimensionality, the model retains a high level of accuracy. This highlights the sparsity of the TF-IDF vectors while showcasing that effective accuracy can be maintained with a significantly smaller feature set.</p>
<p>Before proceeding to the next model, I saved the performance metrics of the baseline model for future comparison.</p>
<div id="527d92ae" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>save_performance(model_name<span class="op">=</span><span class="st">'LinearSVC'</span>,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>                 architecture<span class="op">=</span><span class="st">'default'</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                 embed_size<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>                 criterion<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>                 accuracy<span class="op">=</span><span class="fl">87.15</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="feedforward-neural-network" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="feedforward-neural-network">Feedforward Neural Network</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="../assets/projects/resume-analyzer/nn_files/fnn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="../assets/projects/resume-analyzer/nn_files/fnn.png" class="img-fluid"></a></p>
</div><div id="fn4"><p><sup>4</sup>&nbsp;Feedforward Neural Networks (FNN) are a type of neural networks where information moves in only one direction—forward—from the input nodes, through hidden layers, and to the output nodes.</p></div></div><p>The next model is a Feedforward Neural Network (FNN)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> built using PyTorch. I created an iterator for the dataset with the <code>DataLoader</code> class, which tokenized and numericalized the resumes, dynamically padded the sequences, and batched the data for training, thus saving memory and computation time. I then constructed a simple neural network architecture featuring an embedding layer, followed by three fully connected layers with ReLU activation functions. The model was trained using the Adam optimizer and Cross Entropy Loss criterion, and its performance was evaluated on the test set with accuracy as the metric.</p>
<blockquote class="blockquote">
<h3 id="takeaways-1" class="anchored">Takeaways</h3>
<ul>
<li>The Feedforward Neural Network achieved an accuracy of <strong>73.15%</strong> with a loss of <strong>1.1444</strong> on the test set.</li>
<li>Performance suggests <strong>minimal overfitting</strong>, indicated by the small gap between training and validation accuracies.</li>
<li>The model demonstrates <strong>robust generalization</strong>, with test accuracy closely aligning with validation accuracy.</li>
</ul>
</blockquote>
<section id="import-packages-and-data-1" class="level3">
<h3 class="anchored" data-anchor-id="import-packages-and-data-1">Import Packages and Data</h3>
<p>In addition to standard PyTorch and pandas imports, I also imported three custom functions:</p>
<ul>
<li><strong><code>train_model</code></strong>: Trains the model using the provided hyperparameters and data, prints real-time training and validation loss and accuracy, and saves the best model based on the lowest validation loss. It also offers the option to visualize the training progress with the <code>PlotLosses</code> library or <code>matplotlib</code>.</li>
<li><strong><code>test_model</code></strong>: Evaluates the model on the test set using the best model saved during training and returns the test accuracy.</li>
<li><strong><code>save_performance</code></strong>: Saves the model’s performance metrics to a JSON file for future analysis.</li>
</ul>
<div id="d988fbec" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> CrossEntropyLoss</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.utils.rnn <span class="im">import</span> pad_sequence</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> ReduceLROnPlateau</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Dataset, random_split</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data.utils <span class="im">import</span> get_tokenizer</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> build_vocab_from_iterator</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> livelossplot <span class="im">import</span> PlotLosses</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.dl <span class="im">import</span> train_model, test_model</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.utils <span class="im">import</span> save_performance</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_parquet(<span class="st">'./data/3-processed/resume-features.parquet'</span>, columns<span class="op">=</span>[<span class="st">'Category'</span>, <span class="st">'cleaned_resumes'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>When training deep learning models, I always include the option to use a GPU if available and set the <code>device</code> variable accordingly. This approach not only enables the model to utilize parallel computing resources if available, but also makes the code reproducible across different device setups. The snippet below checks for GPU availability and assigns the <code>device</code> variable, which is then used by the <code>DataLoader</code> and the model.</p>
<div id="b215e318" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Using </span><span class="sc">{}</span><span class="st">."</span>.<span class="bu">format</span>(device))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Using cpu.</code></pre>
</section>
<section id="dataset-and-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="dataset-and-dataloader">Dataset and DataLoader</h3>
<p>Before constructing the <code>Dataset</code> class, I defined a <code>tokenization</code> function that initializes the tokenizer, tokenizes the text data, and builds a vocabulary using PyTorch’s <code>get_tokenizer</code> and <code>build_vocab_from_iterator</code> functions. This function returns the tokenized texts for indexing by the <code>DataLoader</code> during training and the vocabulary for determining vocabulary size.</p>
<div id="926fe1cd" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenization(texts, tokenizer_type<span class="op">=</span><span class="st">'basic_english'</span>, specials<span class="op">=</span>[<span class="st">'&lt;unk&gt;'</span>], device<span class="op">=</span>device):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Instantiate tokenizer</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> get_tokenizer(tokenizer_type)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize text data</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> [tokenizer(text) <span class="cf">for</span> text <span class="kw">in</span> texts]</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build vocabulary</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> build_vocab_from_iterator(tokens, specials<span class="op">=</span>specials)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set default index for unknown tokens</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    vocab.set_default_index(vocab[<span class="st">'&lt;unk&gt;'</span>])</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tokenized texts to a tensor</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    tokenized_texts <span class="op">=</span> [torch.tensor([vocab[token] <span class="cf">for</span> token <span class="kw">in</span> text], dtype<span class="op">=</span>torch.int64, device<span class="op">=</span>device) <span class="cf">for</span> text <span class="kw">in</span> tokens]</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized_texts, vocab</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Next, I created the <code>ResumeDataset</code> iterator, which preprocesses the text data using the <code>tokenization</code> function and indexes samples for the <code>DataLoader</code> during training. The class includes: - <strong><code>__len__</code></strong>: Returns the length of the dataset. - <strong><code>vocab_size</code></strong>: Provides the size of the vocabulary. - <strong><code>num_class</code></strong>: Returns the number of unique classes in the dataset. - <strong><code>__getitem__</code></strong>: Returns a sample of text and its corresponding label from the dataset.</p>
<div id="d9bc8164" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResumeDataset(Dataset):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dataset initialization and preprocessing</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize dataset attributes</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text <span class="op">=</span> data.iloc[:,<span class="dv">1</span>]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> data.iloc[:,<span class="dv">0</span>]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenized_texts, <span class="va">self</span>.vocab <span class="op">=</span> tokenization(<span class="va">self</span>.text)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get length of dataset</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get vocabulary size</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> vocab_size(<span class="va">self</span>):</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.vocab)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get number of classes</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> num_class(<span class="va">self</span>):</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels.unique())</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get item from dataset</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        sequence <span class="op">=</span> <span class="va">self</span>.tokenized_texts[idx]</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="va">self</span>.labels[idx]</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sequence, label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I also defined a <code>collate_fn</code> function to implement dynamic padding when batching the data. Dynamic padding is a technique used to adjusts sequence lengths to match the longest sequence in each batch, rather than to the longest sequence in the entire dataset. Dynamic padding allows the model to process sequences of varying lengths more efficiently, saving memory and computation time.</p>
<div id="0f51798d" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collate_fn(batch):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    sequences, labels <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>batch)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pad sequences to the longest sequence in the batch</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    sequences_padded <span class="op">=</span> pad_sequence(sequences, batch_first<span class="op">=</span><span class="va">True</span>, padding_value<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert labels to tensor</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.tensor(labels, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sequences_padded, labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Finally, I instantiated the <code>ResumeDataset</code> class and split the dataset into 70% training, 15% validation, and 15% test sets using the <code>random_split</code> function. I created <code>DataLoader</code> iterators for each set, applying dynamic padding through the <code>collate_fn</code> function.</p>
<div id="e5d43ace" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> ResumeDataset(data)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset, test_dataset <span class="op">=</span> random_split(dataset, [<span class="fl">0.7</span>, <span class="fl">0.15</span>, <span class="fl">0.15</span>])</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_fn)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_fn)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="model-architecture" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="model-architecture">Model Architecture</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="../assets/projects/resume-analyzer/nn_files/fnn-architecture.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="../assets/projects/resume-analyzer/nn_files/fnn-architecture.jpg" class="img-fluid"></a></p>
</div></div><p>The model is a simple FNN with an embedding layer, followed by two fully connected layers with ReLU activation functions, and a final fully connected layer with output size matching the number of classes. The architecture is defined in the <code>SimpleNN</code> class, which takes the vocabulary size, embedding size, number of classes, and an <code>expansion_factor</code> parameter (set to 2) to determine the hidden dimension size.</p>
<p>The <code>EmbeddingBag</code> function efficiently computes embeddings by first creating embeddings for the input indices and then averaging the output across the sequence dimension. This approach accommodates sequences of varying lengths, allowing the model to process them more efficiently.</p>
<div id="3088df0b" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNN(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_size, num_class, expansion_factor<span class="op">=</span><span class="dv">2</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.EmbeddingBag(vocab_size, embed_size, sparse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dim <span class="op">=</span> embed_size <span class="op">*</span> expansion_factor</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer1 <span class="op">=</span> nn.Linear(embed_size, <span class="va">self</span>.hidden_dim)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer2 <span class="op">=</span> nn.Linear(<span class="va">self</span>.hidden_dim, embed_size)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer3 <span class="op">=</span> nn.Linear(embed_size, num_class)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.layer1(x))</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.layer2(x))</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layer3(x)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="hyperparameters-and-training" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="hyperparameters-and-training">Hyperparameters and Training</h3>
<p>Before training, I set the hyperparameters for the neural network. The vocabulary size and number of classes are sourced from the <code>ResumeDataset</code> class. The embedding size is set to 60, the learning rate to 1e-3, and the model is trained for 40 epochs.</p>
<div id="3b57c0b7" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> dataset.vocab_size()</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>num_class <span class="op">=</span> dataset.num_class()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>embed_size <span class="op">=</span> <span class="dv">60</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>lr<span class="op">=</span><span class="fl">0.001</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">40</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I instantiated the model, assigned it to the available device, and defined the loss function and optimizer. The loss function used is Cross Entropy Loss, suitable for multi-class classification, and the optimizer is Adam, known for its adaptive learning rate capabilities. Additionally, I set up a learning rate scheduler to reduce the learning rate by a factor of 0.1 if the validation loss does not improve over a specified <code>patience</code> period, helping to prevent overfitting and enhance generalization. The model and hyperparameters were then passed to the <code>train_model</code> function<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;During fine-tuning, the model achieved better accuracy with a dropout rate of 0.4.</p></div></div><p>To visualize the training progress, I set the <code>visualize</code> parameter to ‘liveloss’, utilizing the <a href="https://p.migdal.pl/livelossplot/">PlotLosses library</a> for a dynamically updating plot that shows real-time training and validation loss and accuracy. This allows for effective monitoring and adjustment of hyperparameters as needed.</p>
<div id="885e13ac" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNN(vocab_size, embed_size, num_class, dropout<span class="op">=</span><span class="fl">0.4</span>).to(device)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> CrossEntropyLoss()</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> ReduceLROnPlateau(loss, patience<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler, visualize<span class="op">=</span><span class="st">'liveloss'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><a href="../assets/projects/resume-analyzer/nn_files/nn-plot.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="../assets/projects/resume-analyzer/nn_files/nn-plot.png" class="img-fluid"></a></p>
<pre><code>accuracy
  training           (min:    0.036, max:    0.712, cur:    0.707)
  validation         (min:    0.037, max:    0.694, cur:    0.685)
log loss
  training           (min:    0.873, max:    3.187, cur:    0.921)
  validation         (min:    1.274, max:    3.184, cur:    1.330)
------------------------------
Best model saved:
Val Loss: 1.3300 | Val Acc: 0.6852
✅ Training complete!</code></pre>
<p>The training and validation losses decreased steadily until the 30th epoch, indicating effective learning. After 30 epochs, training loss continued to decrease while validation loss plateaued. However, the final training loss was 0.92 and the validation loss was 1.33, with a minimal gap suggesting good generalization.</p>
<p>In terms of accuracy, both training and validation accuracies increased until they converged at 40 epochs. At convergence, the training accuracy was 71% and the validation accuracy was 69%. The small difference between these accuracies indicates minimal overfitting and good generalization to unseen data. The best model saved achieved a validation loss of 1.33 and a validation accuracy of 68.52%.</p>
</section>
<section id="evaluation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="evaluation">Evaluation</h3>
<p>After training the model, I evaluated its performance on the test set using the <code>test_model</code> function. This function takes the trained model, test data loader, and criterion as inputs, and returns the model’s accuracy on the test set.</p>
<div id="bbba3c89" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> test_model(model, test_loader, criterion)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Test Loss: 1.1444 | Test Acc: 0.7315
✅ Testing complete!</code></pre>
<p>The FNN achieved a test accuracy of <strong>73.15%</strong> and a loss of 1.1444. The test performance is consistent with the validation accuracy observed during training, indicating that the model generalizes well to new data and demonstrates robustness in its predictions.</p>
<p>While the FNN’s accuracy is lower than the baseline model, it is still impressive given the simplicity of the architecture. Performance could be enhanced by incorporating advanced regularization techniques, utilizing pre-trained word embeddings<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, or increasing model complexity. The next section explores a more advanced Transformer-based architecture that leverages self-attention mechanisms to capture long-range dependencies in the data.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Examples of pre-trained embeddings include: <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a>, <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, and <a href="https://fasttext.cc/">fastText</a>.</p></div></div><p>To conclude this section, I saved the performance metrics of the FNN model for future analysis.</p>
<div id="a519a9ca" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>save_performance(model_name<span class="op">=</span><span class="st">'Feedforward Neural Network'</span>,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                 architecture<span class="op">=</span><span class="st">'embed_layer-&gt;dropout-&gt;120-&gt;dropout-&gt;60-&gt;dropout-&gt;num_classes'</span>,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>                 embed_size<span class="op">=</span><span class="st">'60'</span>,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="st">'1e-3'</span>,</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="st">'50'</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'Adam'</span>,</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>                 criterion<span class="op">=</span><span class="st">'CrossEntropyLoss'</span>,</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>                 accuracy<span class="op">=</span><span class="fl">73.15</span>,</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="encoder-model" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="encoder-model">Encoder Model</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="../assets/projects/resume-analyzer/encoder_files/transformer_encoder-highlighted.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="../assets/projects/resume-analyzer/encoder_files/transformer_encoder-highlighted.png" class="img-fluid"></a></p>
</div></div><p>The next model utilizes the encoder component of the Transformer architecture for text classification. Unlike decoders, which generate output sequences from dense representations, encoders instead transform input sequences into dense representations. This capability is particularly valuable for tasks such as sentiment analysis, named entity recognition, and text classification.</p>
<p>The encoder model follows the Transformer architecture described in <em>Attention is All You Need</em><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> and is used as a baseline for transformer-based models. I construct the encoder architecture with an embedding layer, a stack of encoder layers, and a feed-forward neural network for classification. I then initialize the hyperparameters for training and train the model using the Adam optimizer and Cross Entropy Loss criterion. The model’s performance is evaluated on the test set using accuracy as the evaluation metric.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Vaswani, Ashish u.&nbsp;a. (2017): Attention Is All You Need. Advances in neural information processing systems 30.</p></div></div><p>The imported packages and the <code>DataLoader</code> setup mirror those used for the Feedforward Neural Network model (see <a href="https://marcocamilo.com/resume-analyzer#import-packages-and-data-1">packages</a> and <a href="https://marcocamilo.com/resume-analyzer#dataloader">data preparation</a>), so I will focus on the model architecture.</p>
<blockquote class="blockquote">
<h3 id="takeaways-2" class="anchored">Takeaways</h3>
<ul>
<li>The Transformer Encoder model achieved an accuracy of 75% on the test set.</li>
<li>This model provides a solid baseline for transformer-based approaches in text classification tasks.</li>
</ul>
</blockquote>
<section id="model-architecture-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="model-architecture-1">Model Architecture</h3>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/projects/resume-analyzer/encoder_files/encoder.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Modified image from Sebastian Raschka"><img src="../assets/projects/resume-analyzer/encoder_files/encoder.jpg" class="img-fluid figure-img" alt="Modified image from Sebastian Raschka"></a></p>
<figcaption>Modified image from <a href="https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder">Sebastian Raschka</a></figcaption>
</figure>
</div>
</div><div id="fn8"><p><sup>8</sup>&nbsp;The multi-head self-attention mechanism is a component of the transformer model that weights the importance of different elements in a sequence by computing attention scores multiple times in parallel across different linear projections of the input.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;Layer normalization is a normalization technique that uses the mean and variance statistics calculated across all features.</p></div></div><p>The Encoder model features an embedding layer, a stack of encoder layers, and a fully connected neural network for classification. The embedding layer converts input sequences into dense representations, which then pass through the encoder layers. Each encoder layer includes a multi-head self-attention mechanism<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, followed by a residual connection with layer normalization<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, a feed-forward neural network, and another residual connection with layer normalization. The final output is processed by a feed-forward neural network for classification.</p>
<p>I built the encoder model from scratch to gain a deeper understanding of its architecture and components. Using a modular approach, I created each component as a separate class and integrated them in the <code>TransformerEncoder</code> class.</p>
<blockquote class="blockquote">
<p><strong>Further Reading:</strong></p>
<p>The implementation draws inspiration from these resources:</p>
<ul>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://www.youtube.com/watch?v=ISNdQcPhsts">Coding a Transformer from Scratch on PyTorch (YouTube)</a></li>
<li><a href="https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae">Text Classification with Transformer Encoders</a></li>
</ul>
</blockquote>
<div id="7dfb8593" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EmbeddingLayer(nn.Module):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, d_model: <span class="bu">int</span>):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dimensions of embedding layer</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, d_model)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Embedding dimension</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.embedding(x) <span class="op">*</span> math.sqrt(<span class="va">self</span>.d_model)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEmbedding(nn.Module):</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, d_model: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize positional embedding matrix (vocab_size, d_model)</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(vocab_size, d_model)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Positional vector (vocab_size, 1)</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, vocab_size).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Frequency term</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> <span class="op">-</span>(math.log(<span class="dv">10000</span>) <span class="op">/</span> d_model))</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sinusoidal functions</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add batch dimension</span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save to class</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :]</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-6</span>):</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Learnable parameters</span></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(d_model))</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.ones(d_model))</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Numerical stability in case of 0 denominator</span></span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> x.std(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear combination of layer norm with parameters gamma and beta</span></span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> (x <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="va">self</span>.eps) <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResidualConnection(nn.Module):</span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer normalization for residual connection</span></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(d_model)</span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x1, x2):</span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(<span class="va">self</span>.norm(x1 <span class="op">+</span> x2))</span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(nn.Module):</span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, d_ff: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2048</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear layers and dropout</span></span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(d_model, d_ff)</span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(d_ff, d_model)</span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear2(<span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.linear1(x))))</span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span><span class="fl">0.1</span>, qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>, is_causal: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>,  <span class="st">"d_model is not divisible by num_heads"</span></span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb26-76"><a href="#cb26-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb26-77"><a href="#cb26-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> d_model <span class="op">//</span> num_heads</span>
<span id="cb26-78"><a href="#cb26-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> dropout</span>
<span id="cb26-79"><a href="#cb26-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.is_causal <span class="op">=</span> is_causal</span>
<span id="cb26-80"><a href="#cb26-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-81"><a href="#cb26-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qkv <span class="op">=</span> nn.Linear(d_model, <span class="dv">3</span> <span class="op">*</span> d_model, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb26-82"><a href="#cb26-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(num_heads <span class="op">*</span> <span class="va">self</span>.head_dim, d_model)</span>
<span id="cb26-83"><a href="#cb26-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_layer <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb26-84"><a href="#cb26-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-85"><a href="#cb26-85" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-86"><a href="#cb26-86" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_length <span class="op">=</span> x.shape[:<span class="dv">2</span>]</span>
<span id="cb26-87"><a href="#cb26-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-88"><a href="#cb26-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformation and split into query, key, and value</span></span>
<span id="cb26-89"><a href="#cb26-89" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv(x)  <span class="co"># (batch_size, seq_length, 3 * embed_dim)</span></span>
<span id="cb26-90"><a href="#cb26-90" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> qkv.view(batch_size, seq_length, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)  <span class="co"># (batch_size, seq_length, 3, num_heads, head_dim)</span></span>
<span id="cb26-91"><a href="#cb26-91" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> qkv.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>)  <span class="co"># (3, batch_size, num_heads, seq_length, head_dim)</span></span>
<span id="cb26-92"><a href="#cb26-92" aria-hidden="true" tabindex="-1"></a>        queries, keys, values <span class="op">=</span> qkv  <span class="co"># 3 * (batch_size, num_heads, seq_length, head_dim)</span></span>
<span id="cb26-93"><a href="#cb26-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-94"><a href="#cb26-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scaled Dot-Product Attention</span></span>
<span id="cb26-95"><a href="#cb26-95" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> F.scaled_dot_product_attention(queries, keys, values, attn_mask<span class="op">=</span>mask, dropout_p<span class="op">=</span><span class="va">self</span>.dropout, is_causal<span class="op">=</span><span class="va">self</span>.is_causal)</span>
<span id="cb26-96"><a href="#cb26-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-97"><a href="#cb26-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine heads, where self.d_model = self.num_heads * self.head_dim</span></span>
<span id="cb26-98"><a href="#cb26-98" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> context_vec.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, seq_length, <span class="va">self</span>.d_model)</span>
<span id="cb26-99"><a href="#cb26-99" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> <span class="va">self</span>.dropout_layer(<span class="va">self</span>.linear(context_vec))</span>
<span id="cb26-100"><a href="#cb26-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-101"><a href="#cb26-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vec</span>
<span id="cb26-102"><a href="#cb26-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-103"><a href="#cb26-103" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderLayer(nn.Module):</span>
<span id="cb26-104"><a href="#cb26-104" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, hidden_dim: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-105"><a href="#cb26-105" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-106"><a href="#cb26-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-head self-attention mechanism</span></span>
<span id="cb26-107"><a href="#cb26-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.multihead_attention <span class="op">=</span> MultiHeadAttention(d_model, num_heads, dropout)</span>
<span id="cb26-108"><a href="#cb26-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First residual connection and layer normalization</span></span>
<span id="cb26-109"><a href="#cb26-109" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual1 <span class="op">=</span> ResidualConnection(d_model, dropout)</span>
<span id="cb26-110"><a href="#cb26-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feed-forward neural network</span></span>
<span id="cb26-111"><a href="#cb26-111" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(d_model, hidden_dim, dropout)</span>
<span id="cb26-112"><a href="#cb26-112" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second residual connection and layer normalization</span></span>
<span id="cb26-113"><a href="#cb26-113" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual2 <span class="op">=</span> ResidualConnection(d_model, dropout)</span>
<span id="cb26-114"><a href="#cb26-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-115"><a href="#cb26-115" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-116"><a href="#cb26-116" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.residual1(x, <span class="va">self</span>.multihead_attention(x, mask))</span>
<span id="cb26-117"><a href="#cb26-117" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.residual2(x, <span class="va">self</span>.feed_forward(x))</span>
<span id="cb26-118"><a href="#cb26-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb26-119"><a href="#cb26-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-120"><a href="#cb26-120" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderStack(nn.Module):</span>
<span id="cb26-121"><a href="#cb26-121" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, hidden_dim: <span class="bu">int</span>, num_layers: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-122"><a href="#cb26-122" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-123"><a href="#cb26-123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stack of encoder layers</span></span>
<span id="cb26-124"><a href="#cb26-124" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([EncoderLayer(d_model, num_heads, hidden_dim, dropout) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)])</span>
<span id="cb26-125"><a href="#cb26-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-126"><a href="#cb26-126" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-127"><a href="#cb26-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb26-128"><a href="#cb26-128" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, mask)</span>
<span id="cb26-129"><a href="#cb26-129" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb26-130"><a href="#cb26-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-131"><a href="#cb26-131" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerClassifier(nn.Module):</span>
<span id="cb26-132"><a href="#cb26-132" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, d_model: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, hidden_dim: <span class="bu">int</span>, num_layers: <span class="bu">int</span>, out_features: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-133"><a href="#cb26-133" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-134"><a href="#cb26-134" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> EmbeddingLayer(vocab_size, d_model)</span>
<span id="cb26-135"><a href="#cb26-135" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> PositionalEmbedding(vocab_size, d_model, dropout)</span>
<span id="cb26-136"><a href="#cb26-136" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> EncoderStack(d_model, num_heads, hidden_dim, num_layers, dropout)</span>
<span id="cb26-137"><a href="#cb26-137" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(d_model, out_features)</span>
<span id="cb26-138"><a href="#cb26-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-139"><a href="#cb26-139" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-140"><a href="#cb26-140" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb26-141"><a href="#cb26-141" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.positional_embedding(x)</span>
<span id="cb26-142"><a href="#cb26-142" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.encoder(x, mask)</span>
<span id="cb26-143"><a href="#cb26-143" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb26-144"><a href="#cb26-144" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.classifier(x)</span>
<span id="cb26-145"><a href="#cb26-145" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="hyperparameters-and-training-1" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters-and-training-1">Hyperparameters and Training</h3>
<p>With the model constructed, I set the training hyperparameters. As with the previous model, I obtain the vocabulary size and number of output features from the <code>ResumeDataset</code> class. The embedding size is set to 80, the hidden dimension to 180, and the multi-head attention mechanism uses 4 heads with an encoder stack of 4 layers. I train the model for 20 epochs with a learning rate of 1e-3.</p>
<div id="4fe673b3" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> dataset.vocab_size()</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>hidden_dim <span class="op">=</span> <span class="dv">180</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>num_layers <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>out_features <span class="op">=</span> dataset.num_class()</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I instantiated the model with the specified hyperparameters and moved it to the appropriate device. The criterion and optimizer remained unchanged from the previous model, using the Adam optimizer and CrossEntropyLoss for multi-class classification. I also initialized a learning rate scheduler with a patience of 2 epochs to mitigate overfitting. The model was trained using the <code>train_model</code> function.</p>
<div id="91aca250" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TransformerClassifier(vocab_size, d_model, num_heads, </span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>                                hidden_dim, num_layers, out_features, dropout<span class="op">=</span><span class="dv">0</span>).to(device)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> CrossEntropyLoss()</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> ReduceLROnPlateau(loss, patience<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><a href="../assets/projects/resume-analyzer/encoder_files/encoder-plot.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="../assets/projects/resume-analyzer/encoder_files/encoder-plot.png" class="img-fluid"></a></p>
<pre><code>accuracy
  training           (min:    0.043, max:    1.000, cur:    1.000)
  validation         (min:    0.035, max:    0.794, cur:    0.785)
log loss
  training           (min:    0.027, max:    3.276, cur:    0.029)
  validation         (min:    1.023, max:    3.273, cur:    1.071)
------------------------------
Best model saved:
Val Loss: 1.0709 | Val Acc: 0.7847
✅ Training complete!</code></pre>
<p>Despite achieving better validation accuracy than the FNN, the Encoder model exhibited a significant gap between training and validation performance, indicating overfitting. During the first 14 epochs, both training and validation losses decreased rapidly, and accuracies increased together. However, over the next 6 epochs, the training loss continued to drop from 1.25 to near 0, while the validation loss plateaued around 1.07. Training accuracy surged to 100%, whereas validation accuracy remained at 78%. This discrepancy highlighted that the model was overfitting.</p>
<p>Nevertheless, the Encoder model achieved a validation accuracy of 78%, which was slightly higher than that of the Feedforward Neural Network. I proceeded to evaluate the model on the test set to determine its final accuracy.</p>
</section>
<section id="evaluation-1" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-1">Evaluation</h3>
<div id="96c17968" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> test_model(model, test_loader, criterion)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Test Loss: 1.2526 | Test Acc: 0.7454
✅ Testing complete!</code></pre>
<p>Despite incorporating a more advanced architecture with a multi-head self-attention mechanism, the Encoder model achieved an <strong>accuracy of 74.5%</strong>, similar to the FNN model. The large gap observed between training and validation performance suggests that this overfitting may be affecting the model’s generalization capabilities.</p>
<p>With improved training and validation performance, the model could potentially reach higher accuracy on the test set. Enhancing the model’s performance might involve techniques such as data augmentation, adjusting hyperparameters like embedding size, hidden dimensions, and the number of layers, or experimenting with different optimizers and learning rate schedulers.</p>
<p>As with the previous models, I saved the performance metrics for later analysis.</p>
<div id="e29e8ce6" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>save_performance(model_name<span class="op">=</span><span class="st">'Transformer'</span>,</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>                 architecture<span class="op">=</span><span class="st">'embed_layer-&gt;encoder-&gt;linear_layer'</span>,</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>                 embed_size<span class="op">=</span><span class="st">'64'</span>,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="st">'1e-3'</span>,</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="st">'20'</span>,</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'Adam'</span>,</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>                 criterion<span class="op">=</span><span class="st">'CrossEntropyLoss'</span>,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>                 accuracy<span class="op">=</span><span class="dv">80</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="bert" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bert">BERT</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="../assets/projects/resume-analyzer/bert_files/bert-classifier.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="../assets/projects/resume-analyzer/bert_files/bert-classifier.png" class="img-fluid"></a></p>
</div></div><p>The final model is Bidirectional Encoder Representations from Transformers (BERT), a pre-trained transformer-based model known for its bidirectional context understanding, meaning that it can take into account the context of a word by looking at both the left and right context. This enables BERT to capture a wider range of contextual information, which is particularly useful for tasks such as text classification.</p>
<p>I created an iterable dataset using the <code>Dataset</code> and <code>DataLoader</code> classes to tokenize resumes with the BERT tokenizer, pad sequences to uniform lengths, and batch the data for training. The model architecture includes the pre-trained BERT base model, a dropout layer, and a linear output layer for classification. Hyperparameters were initialized, and the model was trained using Cross Entropy Loss and the Adam optimizer. Performance was evaluated based on accuracy, consistent with other deep learning models.</p>
<blockquote class="blockquote">
<h3 id="takeaways-3" class="anchored">Takeaways</h3>
<ul>
<li>The BERT model achieved a notable <strong>accuracy of 91.67%</strong> on the test set, significantly outperforming all previous models.</li>
<li>The close alignment between the validation and test accuracies demonstrated the model’s <strong>strong generalization</strong> ability to new, unseen data.</li>
<li>The model <strong>significantly outperforms</strong> all previous models tested so far.</li>
</ul>
</blockquote>
<section id="import-packages" class="level3">
<h3 class="anchored" data-anchor-id="import-packages">Import Packages</h3>
<p>In addition to the standard deep learning packages, I imported three classes from the <code>transformers</code> package:</p>
<ul>
<li><code>BertModel</code>: Loads the pre-trained BERT model.</li>
<li><code>BertTokenizer</code>: Constructs a BERT tokenizer.</li>
<li><code>DataCollatorWithPadding</code>: Creates batches with dynamically padded sequences.</li>
</ul>
<p>Given BERT’s unique output format, I also imported custom functions <code>train_BERT</code> and <code>test_BERT</code>, designed specifically to handle BERT’s outputs, including input IDs and attention masks, for evaluating the model’s performance.</p>
<div id="9bb7adf8" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel, BertTokenizer, DataCollatorWithPadding</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.dl <span class="im">import</span> train_BERT, test_BERT</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="dataset-and-dataloader-1" class="level3">
<h3 class="anchored" data-anchor-id="dataset-and-dataloader-1">Dataset and DataLoader</h3>
<p>Before creating the dataset and <code>DataLoader</code>, I initialized the BERT tokenizer and define the pre-trained BERT model. I then constructed the <code>ResumeBertDataset</code>, which handles the tokenization and preparation of resumes for model input.</p>
<p>Unlike the previous models, I configured the tokenizer using the <code>.encode_plus</code> method. This method returns a dictionary containing the batch encodings, including tokenized input sequences and attention masks. I set <code>padding</code> to <code>False</code>, as dynamic padding will be managed by the data collator, and <code>truncation</code> to <code>True</code> to truncate sequences that exceed the maximum length. The method also adds the special tokens <code>[CLS]</code> and <code>[SEP]</code>, required by BERT. I use <code>return_tensors='pt'</code> to return PyTorch tensors. The function ultimately returns a dictionary with the processed input sequences, attention masks, and labels.</p>
<div id="dcf3e0a0" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResumeBertDataset(Dataset):</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, max_length, tokenizer<span class="op">=</span>tokenizer, device<span class="op">=</span>device):</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.texts <span class="op">=</span> data.iloc[:,<span class="dv">1</span>].values</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> torch.tensor(data.iloc[:,<span class="dv">0</span>])</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_length <span class="op">=</span> max_length</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> num_class(<span class="va">self</span>):</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels.unique())</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        resumes <span class="op">=</span> <span class="va">self</span>.texts[idx]</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> <span class="va">self</span>.labels[idx]</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        encoding <span class="op">=</span> <span class="va">self</span>.tokenizer.encode_plus(</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>            resumes,</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>            add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>            max_length<span class="op">=</span><span class="va">self</span>.max_length,</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>            truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>            return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>            return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        ).to(<span class="va">self</span>.device)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>        input_ids <span class="op">=</span> encoding[<span class="st">'input_ids'</span>].squeeze()</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>        attention_mask <span class="op">=</span> encoding[<span class="st">'attention_mask'</span>].squeeze()</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>            <span class="st">'input_ids'</span>: input_ids,</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>            <span class="st">'attention_mask'</span>: attention_mask,</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>            <span class="st">'labels'</span>: labels</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I initialized the dataset and set <code>max_length</code> to 512, the maximum token limit that BERT could handle. I then split the dataset into 70% training, 15% validation, and 15% test sets using the <code>random_split</code> function. To manage varying sequence lengths, I used the <code>DataCollatorWithPadding</code> class, which dynamically padded sequences to the maximum length within each batch. Finally, I created <code>DataLoader</code> instances for the training, validation, and test sets, setting the batch size to 16, enabling data shuffling, and applying the data collator for padding.</p>
<div id="ea3d04b8" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> ResumeBertDataset(data, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset, test_dataset <span class="op">=</span> random_split(dataset, [<span class="fl">0.7</span>, <span class="fl">0.15</span>, <span class="fl">0.15</span>])</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorWithPadding(tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">16</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>data_collator)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">16</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>data_collator)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">16</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>data_collator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="model-architecture-2" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture-2">Model Architecture</h3>
<p>The <code>BertResumeClassifier</code> model comprised the pre-trained BERT base model, a dropout layer, and a linear output layer for resume classification. I utilized the <code>bert-base-uncased</code> pre-trained model to generate contextual embeddings from the input sequences. I extracted these embeddings by indexing the <code>pooler_output</code> key from the output dictionary. The embeddings were then passed through a dropout layer to mitigate overfitting, and subsequently fed into a fully connected linear layer that mapped the embeddings to the desired number of output classes for classification.</p>
<div id="90d86a57" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BertResumeClassifier(nn.Module):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_classes: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.01</span>):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bert <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="va">self</span>.bert.config.hidden_size, n_classes)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, attention_mask):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        pooled_output <span class="op">=</span> <span class="va">self</span>.bert(</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>          input_ids<span class="op">=</span>input_ids,</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>          attention_mask<span class="op">=</span>attention_mask</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        )[<span class="st">'pooler_output'</span>]</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.dropout(pooled_output)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.out(output)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="hyperparameters-and-training-2" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters-and-training-2">Hyperparameters and Training</h3>
<p>Since BERT is a pre-trained model with a fixed input size of 512 tokens, there were fewer parameters to set for the model itself. The primary parameter to configure was the number of output classes, which, as before, was obtained from the <code>Dataset</code> class.</p>
<p>I initialized the model, loss function, optimizer, and set the number of epochs. I used the Cross Entropy Loss function and the Adam optimizer, adjusting the learning rate to 2e-5, which had shown better performance in preliminary tests. The model was trained for 10 epochs.</p>
<div id="b3d37349" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> dataset.num_class()</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertResumeClassifier(n_classes).to(device)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> CrossEntropyLoss()</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>train_BERT(model, train_loader, val_loader, epochs, criterion, optimizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><a href="../assets/projects/resume-analyzer/bert_files/bert-plot.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="../assets/projects/resume-analyzer/bert_files/bert-plot.png" class="img-fluid"></a></p>
<pre><code>accuracy
    training             (min:    0.050, max:    0.991, cur:    0.991)
    validation           (min:    0.120, max:    0.933, cur:    0.933)
log loss
    training             (min:    0.081, max:    3.189, cur:    0.081)
    validation           (min:    0.414, max:    3.002, cur:    0.428)
------------------------------
Best model saved:
Val Loss: 0.4143 | Val Acc: 0.9190
✅ Training complete!</code></pre>
<p>The BERT model demonstrated a significant and steady decrease in both training and validation losses over the 10 epochs, indicating effective learning of data patterns. By the end of training, the training loss decreased from 3.1394 to 0.0589, while the validation loss fell from 2.7347 to 0.5070. This consistent reduction highlighted the model’s ability to capture and generalize the data without overfitting, as evidenced by the small gap between the training and validation losses by the end of the 10th epoch.</p>
<p>In terms of accuracy, both training and validation accuracies showed a steady increase throughout the 10 epochs. Training accuracy rose from 0.0774 to 0.9950, while validation accuracy improved from 0.3472 to 0.9028. These results indicated that the model effectively learned the data patterns and generalized well to unseen data. The minimal difference between the final training and validation accuracies underscored the model’s robustness and its ability to avoid overfitting, ensuring reliable performance on new data. The best saved model achieved a validation loss of 0.5070 and an accuracy of 0.9028.</p>
</section>
<section id="evaluation-2" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-2">Evaluation</h3>
<p>As before, I evaluate the model using the <code>test_model</code> function using the best saved model.</p>
<div id="ecf4d3ab" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> test_BERT(model, test_loader, criterion)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Test Loss: 0.3984 | Test Acc: 0.9167
✅ Testing complete!</code></pre>
<p>The BERT model achieved impressive performance on the test set, reaching an <strong>accuracy of 91.67%</strong> with a test loss of 0.3984. This result significantly outperformed all previous models tested. As anticipated from the training and validation performances, the model demonstrated robustness and excellent generalization to unseen data. This was further confirmed by the close alignment between test and validation accuracies, both of which represented datasets not previously seen by the model. The similar accuracy between the validation data and the test data indicated that the model could generalize effectively to new resumes and accurately classify them into the correct categories.</p>
<p>As with the previous models, I saved its performance using the <code>save_performance</code> function.</p>
<div id="fdbe3f96" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>save_performance(model_name<span class="op">=</span><span class="st">'BERT'</span>,</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>                 architecture<span class="op">=</span><span class="st">'bert-base-uncased&gt;dropout-&gt;linear_layer'</span>,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                 embed_size<span class="op">=</span><span class="st">'768'</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="st">'2e-5'</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="st">'10'</span>,</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'Adam'</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                 criterion<span class="op">=</span><span class="st">'CrossEntropyLoss'</span>,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>                 accuracy<span class="op">=</span><span class="dv">90</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="results-and-discussion" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="results-and-discussion">Results and Discussion</h2>
<p>Below I discuss the performance of the classification models, the reasons behind their varying accuracies, and their implications in industry applications.</p>
<p>All four models achieved accuracies above 70%, in large part thanks to effective data preparation including, involving thorough text preprocessing, data balancing, and advanced vectorization techniques. The models can be categorized into two performance groups: Linear SVC and BERT, both achieving around 90% accuracy, and FNN and Transformer, with approximately 70% accuracy. Interestingly, the highest-performing models are at opposite ends of the complexity spectrum, with Linear SVC being the simplest and BERT the most complex, while the lower-performing models have higher complexity than the baseline. The reasons for these performance differences are discussed below.</p>
<div id="fdc32a19" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>evaluation_df <span class="op">=</span> pd.read_json(<span class="st">'./output/classifier_performance.json'</span>).sort_values(by<span class="op">=</span><span class="st">'accuracy'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.barplot(evaluation_df, x<span class="op">=</span><span class="st">'model'</span>, y<span class="op">=</span><span class="st">'accuracy'</span>, hue<span class="op">=</span><span class="st">'model'</span>, palette<span class="op">=</span><span class="st">'hls'</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>[ax.bar_label(container, fmt<span class="op">=</span><span class="st">"</span><span class="sc">%0.2f%%</span><span class="st">"</span>) <span class="cf">for</span> container <span class="kw">in</span> ax.containers]</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><a href="../assets/projects/resume-analyzer/eval_files/model-performances.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="../assets/projects/resume-analyzer/eval_files/model-performances.png" class="img-fluid"></a></p>
<ol type="1">
<li><p><strong>BERT and Linear SVC: High Accuracy and Robust Performance</strong></p>
<ul>
<li><strong>Linear SVC</strong>: Achieved 84% accuracy, offering competitive performance due to its simplicity and effective feature representation<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. using TF-IDF vectors. This simplicity reduces the risk of overfitting and the straightforward feature representation contributes to the model’s high accuracy and fast training times. This model is a reliable option for businesses that prioritize efficiency and simplicity without sacrificing too much accuracy.</li>
<li><strong>BERT</strong>: With an accuracy of 91.67%, BERT demonstrated the highest performance among the models. Its success is attributed to its pre-trained nature and high-quality embeddings. The model was pre-trained on a large corpus and thus has the ability to generate embeddings that encode deep semantic information<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. Moreover, its bidirectional nature captures a wide range of contextual information across a long-range dependencies. Businesses with the resources to deploy complex models would benefit from BERT’s superior accuracy.</li>
</ul></li>
<li><p><strong>FNN and Encoder Model: Moderate Performance with Complex Architectures</strong></p>
<ul>
<li><strong>Feedforward Neural Network</strong>: Achieved around 73.15% accuracy. While more advanced than linear models, the FNN lacks the capability to capture sequential dependencies and contextual nuances, which limits its effectiveness in tasks requiring a deeper understanding of text.</li>
<li><strong>Encoder Model</strong>: Achieved 74.54% accuracy. Despite its sophisticated architecture, the Transformer underperformed relative to expectations. This is attributed to its lack of pre-training on a large corpus and possibly insufficient fine-tuning. While Transformers have significant potential, their performance is heavily influenced by pre-training and proper hyperparameter tuning.</li>
</ul></li>
<li><p><strong>Industry Recommendations</strong>:</p>
<ul>
<li><strong>Opt for BERT for Maximum Accuracy</strong>: For businesses prioritizing high accuracy and capable of supporting complex models, BERT is the best choice. Its high performance in classification tasks makes it ideal for scenarios where precision is critical.</li>
<li><strong>Consider Linear SVC for Simplicity and Efficiency</strong>: Linear SVC offers a balance of good performance and computational efficiency. It is suitable for applications needing quick deployment and easy interpretability.</li>
<li><strong>Invest in Transformer Models with Pre-Training</strong>: For those interested in advanced models, investing in pre-training or additional fine-tuning for Transformers could enhance their performance. Transformers show promise but require adequate preparation to reach their full potential.</li>
</ul></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;To read more on the efficiency of linear classifiers in text classification, see <a href="https://doi.org/10.18653/v1/2023.acl-short.160">Lin, Y.-C. et al.&nbsp;(2023) ‘Linear Classifier: An Often-Forgotten Baseline for Text Classification’</a>.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;See <a href="https://doi.org/10.48550/ARXIV.1810.04805">Devlin, J. et al.&nbsp;(2018) ‘BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding’</a>.</p></div></div></section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this project, I tackled the challenge of resume classification using a range of machine learning and deep learning models. The process began with preprocessing the resume data, including tokenizing the resumes and preparing them for model input. I implemented and evaluated four different models: Linear Support Vector Classifier (SVC), Feedforward Neural Network (FNN), Transformer model, and BERT. Each model was trained on the resume dataset and assessed based on accuracy metrics.</p>
<p>The evaluation provided valuable insights into the impact of model complexity and feature representation on performance. BERT achieved the highest accuracy of 91.67%, followed by the Linear SVC model at 87.15%. The Feedforward Neural Network and Transformer models recorded accuracies of 73.15% and 74.54%, respectively. BERT’s outstanding performance is attributed to its pre-trained architecture, which effectively captures deep semantic relationships within text. On the other hand, Linear SVC’s strong performance is due to its simplicity and efficiency with high-dimensional data, utilizing interpretable features like TF-IDF vectors.</p>
<p>Two important observations arise from these results:</p>
<blockquote class="blockquote">
<ol type="1">
<li><strong>State-of-the-art transformer</strong> models, such as BERT, combined with transfer learning, deliver superior performance.</li>
<li><strong>Simple models</strong> with high-quality, interpretable feature representations such as Linear SVC with TF-IDF vectors, can also achieve impressive.</li>
</ol>
</blockquote>
<p>These findings reveal that model complexity does not automatically translate to better performance. This is further supported by the fact that the two models with the lowest accuracies have more complex architectures than the baseline.</p>
<blockquote class="blockquote">
<p><strong>Quality feature representations and capturing contextual information are more important factors in achieving high performing text classification models.</strong></p>
</blockquote>
<p>Ultimately, the choice of model depends on the task requirements and available resources. For applications where high performance is critical and ample resources are available, leveraging advanced transformers like BERT with transfer learning is recommended. For scenarios prioritizing simplicity and interpretability, Linear SVC with TF-IDF vectors provides a high-performing, resource-efficient solution.</p>


</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/marcocamilo\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Marco-Andrés Camilo-Pietri</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"descPosition":"bottom","loop":false,"closeEffect":"zoom","selector":".lightbox","openEffect":"zoom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>