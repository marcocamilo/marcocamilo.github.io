<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.45">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Explores the performance of advanced NLP models and vectorization techniques on topic modeling, text classification and document similarity.">

<title>BERT, Encoders and Linear Models for Resume Text Classification – marcocamilo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/css/styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">marcocamilo</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../portfolio.html"> 
<span class="menu-text">Portfolio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a></li>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#linear-svc" id="toc-linear-svc" class="nav-link" data-scroll-target="#linear-svc">Linear SVC</a>
  <ul>
  <li><a href="#import-packages-and-data" id="toc-import-packages-and-data" class="nav-link" data-scroll-target="#import-packages-and-data">Import Packages and Data</a></li>
  <li><a href="#baseline-linearsvc" id="toc-baseline-linearsvc" class="nav-link" data-scroll-target="#baseline-linearsvc">Baseline LinearSVC</a></li>
  <li><a href="#linearsvc-with-truncated-svd" id="toc-linearsvc-with-truncated-svd" class="nav-link" data-scroll-target="#linearsvc-with-truncated-svd">LinearSVC with Truncated SVD</a></li>
  </ul></li>
  <li><a href="#feedforward-neural-network" id="toc-feedforward-neural-network" class="nav-link" data-scroll-target="#feedforward-neural-network">Feedforward Neural Network</a>
  <ul>
  <li><a href="#import-packages-and-data-1" id="toc-import-packages-and-data-1" class="nav-link" data-scroll-target="#import-packages-and-data-1">Import Packages and Data</a></li>
  <li><a href="#dataset-and-dataloader" id="toc-dataset-and-dataloader" class="nav-link" data-scroll-target="#dataset-and-dataloader">Dataset and DataLoader</a></li>
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture">Model Architecture</a></li>
  <li><a href="#hyperparameters-and-training" id="toc-hyperparameters-and-training" class="nav-link" data-scroll-target="#hyperparameters-and-training">Hyperparameters and Training</a></li>
  <li><a href="#evaluation" id="toc-evaluation" class="nav-link" data-scroll-target="#evaluation">Evaluation</a></li>
  </ul></li>
  <li><a href="#encoder-model" id="toc-encoder-model" class="nav-link" data-scroll-target="#encoder-model">Encoder Model</a>
  <ul>
  <li><a href="#model-architecture-1" id="toc-model-architecture-1" class="nav-link" data-scroll-target="#model-architecture-1">Model Architecture</a></li>
  <li><a href="#hyperparameters-and-training-1" id="toc-hyperparameters-and-training-1" class="nav-link" data-scroll-target="#hyperparameters-and-training-1">Hyperparameters and Training</a></li>
  <li><a href="#evaluation-1" id="toc-evaluation-1" class="nav-link" data-scroll-target="#evaluation-1">Evaluation</a></li>
  </ul></li>
  <li><a href="#bert" id="toc-bert" class="nav-link" data-scroll-target="#bert">BERT</a>
  <ul>
  <li><a href="#import-packages" id="toc-import-packages" class="nav-link" data-scroll-target="#import-packages">Import Packages</a></li>
  <li><a href="#dataset-and-dataloader-1" id="toc-dataset-and-dataloader-1" class="nav-link" data-scroll-target="#dataset-and-dataloader-1">Dataset and DataLoader</a></li>
  <li><a href="#model-architecture-2" id="toc-model-architecture-2" class="nav-link" data-scroll-target="#model-architecture-2">Model Architecture</a></li>
  <li><a href="#hyperparameters-and-training-2" id="toc-hyperparameters-and-training-2" class="nav-link" data-scroll-target="#hyperparameters-and-training-2">Hyperparameters and Training</a></li>
  <li><a href="#evaluation-2" id="toc-evaluation-2" class="nav-link" data-scroll-target="#evaluation-2">Evaluation</a></li>
  </ul></li>
  <li><a href="#results-and-discussion" id="toc-results-and-discussion" class="nav-link" data-scroll-target="#results-and-discussion">Results and Discussion</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">BERT, Encoders and Linear Models for Resume Text Classification</h1>
<p class="subtitle lead">Exploring Performance of Advanced NLP Algorithms for Text Classification</p>
</div>

<div>
  <div class="description">
    Explores the performance of advanced NLP models and vectorization techniques on topic modeling, text classification and document similarity.
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><img src="../assets/projects/resume-analyzer/resume-analyzer.jpg" class="img-fluid"></p>
<section id="executive-summary" class="level2">
<h2 class="anchored" data-anchor-id="executive-summary">Executive Summary</h2>
<p>The project explores the performance of advanced NLP models and vectorization techniques specifically for text classification using a dataset of resumes from various job categories. The algorithms tested include Linear SVC, Feedforward Neural Networks (FNN), Encoder models, and BERT, implemented using Scikit-Learn and PyTorch. The project aims to compare the performance of these algorithms on text classification and evaluate their effectiveness in analyzing resume data.</p>
<blockquote class="blockquote">
<h3 id="key-findings" class="anchored">Key Findings</h3>
<ul>
<li><strong>Key Factors in Model Performance</strong>: quality of the feature representations and the ability to capture contextual information across dependencies.</li>
<li><strong>Model performance</strong>
<ul>
<li><strong>BERT</strong>: Best performing model with an accuracy of <strong>91.67%</strong>. Showcases the effectiveness of pre-trained models and transfer learning.</li>
<li><strong>Linear SVC</strong>: Achieved an accuracy of <strong>87.15%</strong> with TF-IDF vectors. Attributed to the model’s simplicity and use of effective feature representation.</li>
<li><strong>Encoder Model</strong>: Achieved an accuracy of <strong>74.54%</strong>. Suggests the need for further fine-tunning, give the difference with its cousin transformer model.</li>
<li><strong>Feedforward Neural Network</strong>: achieved an accuracy of <strong>73.15%</strong>, the lowest of the four models. Indicates struggle with sequential dependencies and contextual nuances.</li>
</ul></li>
<li><strong>Preprocessing and Vectorization</strong>: effective text preprocessing and robust vectorization techniques significantly enhanced overall model performance (&gt; 70%).</li>
</ul>
</blockquote>
</section>
<section id="dataset" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<p>The project primarily makes use of the <a href="https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset">Resume Dataset</a> sourced from LiveCareer, available at Kaggle. The dataset consists of a collection of more than 2400 resumes in both string and HTML format, along with their respective labeled categories. The dataset consists of the following variables:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/projects/resume-analyzer/resume-dataset.png" class="img-fluid"></p>
</div></div><ul>
<li><strong><code>ID</code></strong>: Unique identifier for each resume</li>
<li><strong><code>Resume_str</code></strong>: Text content of the resume</li>
<li><strong><code>Resume_html</code></strong>: HTML content of the resume</li>
<li><strong><code>Category</code></strong>: Categorized field of the resume (e.g.&nbsp;Information-Technology, Teacher, Advocate, Business-Development, Healthcare)</li>
</ul>
</section>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<p>Data preprocessing primarily involved text preprocessing, as well as data rebalancing for the categories in the resume dataset.</p>
<p>For text cleaning, I used a custom&nbsp;<code>preprocessing</code>&nbsp;function that streamlines multiple text cleaning and preprocessing operations. The function is highly flexible, offering tunable parameters to adapt the preprocessing pipeline to include key operations such as lowercase conversion, HTML decoding, email and URL removal, special character removal, contraction expansion, custom regex cleaning, as well as tokenization, stemming, lemmatization, and stopwords removal.</p>
<p>In particular, I enhanced the text preprocessing by removing noise from the text data, such as non-existen words and frequent, document-specific stopwords like months, section headings and other common words in resumes.</p>
<div id="7f72b3be" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bs4 <span class="im">import</span> BeautifulSoup</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> unidecode <span class="im">import</span> unidecode</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> contractions</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords, words</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem.porter <span class="im">import</span> PorterStemmer</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocessing(</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    text, </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    stem<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    lem<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    html<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    exist<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    remove_emails<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    remove_urls<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    remove_digits<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    remove_punct<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    expand_contractions<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    remove_special_chars<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    remove_stopwords<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    lst_stopwords<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    lst_regex<span class="op">=</span><span class="va">None</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">str</span> <span class="op">|</span> <span class="bu">list</span>[<span class="bu">str</span>]:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lowercase conversion</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> text.lower()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># HTML decoding</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> html:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        soup <span class="op">=</span> BeautifulSoup(cleaned_text, <span class="st">"html.parser"</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> soup.get_text()</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove Emails</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_emails:</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"([a-z0-9+._-]+@[a-z0-9+._-]+\.[a-z0-9+_-]+)"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># URL removal</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_urls:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"(http|https|ftp|ssh)://[\w_-]+(?:\.[\w_-]+)+[\w.,@?^=%&amp;:/~+#-]*[\w@?^=%&amp;/~+#-]?"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove escape sequences and special characters</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_special_chars:</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"[^\x00-\x7f]"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> unidecode(cleaned_text)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove multiple characters</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"(.)\1{3,}"</span>, <span class="vs">r"\1"</span>, cleaned_text)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Expand contractions</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> expand_contractions:</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> contractions.fix(cleaned_text)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="st">"'(?=[Ss])"</span>, <span class="st">""</span>, cleaned_text)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove digits</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_digits:</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"\d"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Punctuation removal</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_punct:</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        cleaned_text <span class="op">=</span> re.sub(<span class="st">"[!</span><span class="ch">\"</span><span class="st">#$%&amp;</span><span class="ch">\\</span><span class="st">'()*+\,-./:;&lt;=&gt;?@\[\]\^_`{|}~]"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Line break and tab removal</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"[\n\t]"</span>, <span class="st">" "</span>, cleaned_text)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Excessive spacing removal</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"\s+"</span>, <span class="st">" "</span>, cleaned_text).strip()</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regex (in case, before cleaning)</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> lst_regex: </span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> regex <span class="kw">in</span> lst_regex:</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>            compiled_regex <span class="op">=</span> re.<span class="bu">compile</span>(regex)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> re.sub(compiled_regex, <span class="st">''</span>, cleaned_text)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenization (if tokenization, stemming, lemmatization or custom stopwords is required)</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> stem <span class="kw">or</span> lem <span class="kw">or</span> remove_stopwords <span class="kw">or</span> tokenize:</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(cleaned_text, <span class="bu">str</span>):</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> cleaned_text.split()</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove stopwords</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> remove_stopwords:</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> lst_stopwords <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>                lst_stopwords <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'english'</span>))</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> cleaned_text <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> lst_stopwords]</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove non-existent words</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> exist:</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>            english_words <span class="op">=</span> <span class="bu">set</span>(words.words())</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> cleaned_text <span class="cf">if</span> word <span class="kw">in</span> english_words]</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stemming</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> stem:</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>            stemmer <span class="op">=</span> PorterStemmer()</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> [stemmer.stem(word) <span class="cf">for</span> word <span class="kw">in</span> cleaned_text]</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Lemmatization</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> lem:</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>            lemmatizer <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> [lemmatizer.lemmatize(word) <span class="cf">for</span> word <span class="kw">in</span> cleaned_text]</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> tokenize:</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>            cleaned_text <span class="op">=</span> <span class="st">' '</span>.join(cleaned_text)</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cleaned_text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Next, I prepared the <code>Category</code> variable in the resumes dataset by converting it to numerical representation using <code>LabelEncoder</code> from Scikit-Learn, as these algorithms require numerical feature representations. After cleaning, I saved the text separately for topic modeling and document similarity tasks, where dataset balancing is unnecessary.</p>
<p>In particular for text classification, I addressed the imbalance in underrepresented categories by employing random resampling with Scikit-Learn’s <code>resample</code> method. This approach ensures that the model receives equitable representation across all categories, leading to enhanced accuracy and reduced bias, thereby improving overall performance.</p>
</section>
<section id="linear-svc" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="linear-svc">Linear SVC</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/projects/resume-analyzer/svc_files/svm.png" class="img-fluid"></p>
</div><div id="fn1"><p><sup>1</sup>&nbsp;Linear Support Vector Classifier (SVC) is a classification algorithm that seeks to find the <em>maximum-margin hyperplane</em>, that is, the hyperplane that most clearly classifies observations</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Truncated Singular Value Decomposition (SVD) is a dimensionality reduction technique that decomposes a matrix into three smaller matrices, retaining only the most significant features of the original matrix.</p></div></div><p>For this first model, I train a baseline Linear SVC<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> using the TF-IDF vectors. I then performance Latent Semantic Analysis (LSA) by applying Truncated Singular Value Decomposition (SVD)<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> to the TF-IDF matrix, to reduce the matrix size to a lower dimensional space. I evaluate the performance of both models, which will serve as the baseline to compare with more complex models in the upcoming sections.</p>
<blockquote class="blockquote">
<h3 id="takeaways" class="anchored">Takeaways</h3>
<ul>
<li>Linear SVC achieved an accuracy of 84% on the test set.</li>
<li>Linear SVC with Truncated SVD achieved an accuracy of 81% on the test set with only 5% of the original number of features.</li>
<li>The baseline model achieved a high level of accuracy and is more cost-effective when reducing the dimensionality of the feature matrix.</li>
</ul>
</blockquote>
<section id="import-packages-and-data" class="level3">
<h3 class="anchored" data-anchor-id="import-packages-and-data">Import Packages and Data</h3>
<p>Aside from standard libraries, I import two custom functions: <code>classifier_report</code> to generate a classification report and confusion matrix, and <code>save_performance</code>, which saves the performance metrics of the model to a JSON file for later analysis. I also load the pre-trained Label Encoder to label the encoded categories in upcoming plots.</p>
<div id="dee1e7d2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.ml <span class="im">import</span> classifier_report</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.utils <span class="im">import</span> save_performance</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the data with engineered features</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_parquet(<span class="st">'./data/3-processed/resume-features.parquet'</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Label model to label categories</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>le <span class="op">=</span> joblib.load(<span class="st">'./models/le-resumes.gz'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="baseline-linearsvc" class="level3">
<h3 class="anchored" data-anchor-id="baseline-linearsvc">Baseline LinearSVC</h3>
<p>I split the dataset into 70% training and 30% testing sets. I use the <code>tfidf_vectors</code> column as the feature matrix and stack the vectors into a single matrix using <code>np.vstack</code>. For the target variable I set the encoded <code>Category</code> column. I extract the variables using the <code>.values</code> method, which extracts the variables into a NumPy array, improving performance. To verify the split, I print the shape of the training and testing sets.</p>
<div id="f862fa64" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack(df[<span class="st">'tfidf_vectors'</span>].values)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'Category'</span>].values</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                                                    y, </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>                                                    test_size<span class="op">=</span><span class="fl">0.30</span>, </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>                                                    random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing set: </span><span class="sc">{</span>X_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Training set: (2016, 11618), (2016,)
Testing set: (864, 11618), (864,)</code></pre>
<p>Then, I train a Linear SVC model with default hyperparameters and evaluate the model’s performance using the <code>classifier_report</code> function, which generates a classification report and confusion matrix.</p>
<div id="cb801111" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>svc <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> classifier_report([X_train, X_test, y_train, y_test], svc, le.classes_, <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>SVC accruacy score 87.15%</code></pre>
<p><img src="../assets/projects/resume-analyzer/svc_files/figure-markdown_strict/cell-4-output-2.png" class="img-fluid"></p>
<p>The model achieves an <strong>87% accuracy</strong> on the test set, which is already very good for a baseline model. However, TF-IDF vectors often result in sparse, high-dimensional representations with a low information ratio. To address this issue, I recur to Truncated Singular Value Decomposition (SVD).</p>
</section>
<section id="linearsvc-with-truncated-svd" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="linearsvc-with-truncated-svd">LinearSVC with Truncated SVD</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/projects/resume-analyzer/svc_files/truncated-svd.png" class="img-fluid"></p>
</div><div id="fn3"><p><sup>3</sup>&nbsp;For an in depth explanation, see Manning, C.D., Raghavan, P. and Schütze, H. (2008) <a href="https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">‘Matrix decompositions and latent semantic indexing’</a>, Introduction to information retrieval, 1.</p></div></div><p>Transforming TF-IDF matrices by means of Truncated SVD is known as Latent Semantic Analysis (LSA). It takes the <span class="math inline">\(n\)</span> largest eigenvalues and transforms the original matrix to capture the most significant semantic relationships between terms and documents, while discarding noise and low-information features<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>I use <code>TruncatedSVD</code> to reduce the number of components to 500, which surprisingly is less than 5% of the original number of features. Below I apply the transformation and split the transformed feature matrix into training and testing sets before training the new model.</p>
<div id="704049e9" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>t_svd <span class="op">=</span> TruncatedSVD(n_components<span class="op">=</span><span class="dv">500</span>, algorithm<span class="op">=</span><span class="st">'arpack'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>X_svd <span class="op">=</span> t_svd.fit_transform(X)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X_train_svd, X_test_svd, y_train_svd, y_test_svd <span class="op">=</span> train_test_split(X_svd, </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                                                                    y, </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                                                                    test_size<span class="op">=</span><span class="fl">0.30</span>, </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                                                                    random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: </span><span class="sc">{</span>X_train_svd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y_train_svd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing set: </span><span class="sc">{</span>X_test_svd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y_test_svd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Training set: (2016, 500), (2016,)
Testing set: (864, 500), (864,)</code></pre>
<p>I now train a new Linear SVC model using the SVD-transformed feature matrix and generate a classification report and confusion matrix.</p>
<div id="613d7876" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>svc_svd <span class="op">=</span> LinearSVC(dual<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> classifier_report([X_train_svd, X_test_svd, y_train_svd, y_test_svd], svc_svd, le.classes_, <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>SVC accruacy score 84.94%</code></pre>
<p><img src="../assets/projects/resume-analyzer/svc_files/figure-markdown_strict/cell-6-output-2.png" class="img-fluid"></p>
<p>The model achieved an <strong>84% accuracy</strong> on the test set, which is slightly lower than the baseline model. However, these results come from a model <strong>trained on less than 5%</strong> of the original number of features while still retaining a high level of accuracy. This demonstrates the sparcity of the TF-IDF vectors, while also showing that the model can still achieve an excellent level of accuracy with a substantially reduced feature matrix.</p>
<p>Before moving onto the next model, I save the performance metrics of the baseline model for later comparison.</p>
<div id="12bb9b54" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>save_performance(model_name<span class="op">=</span><span class="st">'LinearSVC'</span>,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>                 architecture<span class="op">=</span><span class="st">'default'</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                 embed_size<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>                 criterion<span class="op">=</span><span class="st">'n/a'</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>                 accuracy<span class="op">=</span><span class="fl">87.15</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="feedforward-neural-network" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="feedforward-neural-network">Feedforward Neural Network</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/projects/resume-analyzer/nn_files/fnn.png" class="img-fluid"></p>
</div><div id="fn4"><p><sup>4</sup>&nbsp;Feedforward Neural Networks (FNN) are a type of neural networks where information moves in only one direction—forward—from the input nodes, through hidden layers, and to the output nodes</p></div></div><p>The next model is a Feedforward Neural Network (FNN)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> built using PyTorch. I create an iterator for the dataset using the <code>DataLoader</code> class, which tokenizes and numericalized the resumes, dynamically pads the sequences, and batches the data for training, saving memory and computation time. Then, I construct a simple neural network architecture with an embedding layer, followed by three fully connected layers with ReLU activation functions. The model is trained using the Adam optimizer and CrossEntropyLoss criterion, and its performance is evaluated on the test set using accuracy as the evaluation metric.</p>
<blockquote class="blockquote">
<h3 id="takeaways-1" class="anchored">Takeaways</h3>
<ul>
<li>The Feedforward Neural Network achieved an accuracy of 73.15% with a loss of 1.1444 on the test set.</li>
<li>Performance suggests minimal overfitting, given the small gap between training and validation.</li>
<li>Model demonstrates robust generalization, with test accuracy aligning closely with validation accuracy.</li>
</ul>
</blockquote>
<section id="import-packages-and-data-1" class="level3">
<h3 class="anchored" data-anchor-id="import-packages-and-data-1">Import Packages and Data</h3>
<p>Aside from the standard PyTorch and pandas imports, I also import three custom functions:</p>
<ul>
<li><code>train_model</code>: trains the model based on the hyperparameters and data provided, prints the training and validation loss and accuracy in real-time, and saves the best model based on the iteration with the lowest validation loss. It also provides an option to visualize the training progress using the <code>PlotLosses</code> library or <code>matplotlib</code>.</li>
<li><code>test_model</code>: evaluates the model on the test set using the best model saved during training and returns the testing accuracy.</li>
<li><code>save_performance</code>: saves the performance metrics of the model to a json file for future analysis.</li>
</ul>
<div id="9e290238" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> CrossEntropyLoss</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.utils.rnn <span class="im">import</span> pad_sequence</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> ReduceLROnPlateau</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Dataset, random_split</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data.utils <span class="im">import</span> get_tokenizer</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> build_vocab_from_iterator</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> livelossplot <span class="im">import</span> PlotLosses</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.dl <span class="im">import</span> train_model, test_model</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.utils <span class="im">import</span> save_performance</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_parquet(<span class="st">'./data/3-processed/resume-features.parquet'</span>, columns<span class="op">=</span>[<span class="st">'Category'</span>, <span class="st">'cleaned_resumes'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>When training deep learning models, I always code the option to use a GPU if available and set the <code>device</code> variable accordingly. This not only allows the model to leverage the parallel computing if available, but also makes the code reproducible across different device setups. The snippet below checks if a GPU is available and sets the device variable accordingly, which is later used by the <code>DataLoader</code> and model.</p>
<div id="64019b0b" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Using </span><span class="sc">{}</span><span class="st">."</span>.<span class="bu">format</span>(device))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Using cpu.</code></pre>
</section>
<section id="dataset-and-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="dataset-and-dataloader">Dataset and DataLoader</h3>
<p>Before constructing the <code>Dataset</code> class, I define a <code>tokenization</code> function that instantiates the tokenizer, tokenizes the text data, and builds a vocabulary using PyTorch’s <code>get_tokenizer</code> and <code>build_vocab_from_iterator</code> functions. The function returns the tokenized texts to be indexed by the <code>DataLoader</code> during training, and the vocabulary, which will be used to determine the vocabulary size.</p>
<div id="0b5ddc51" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenization(texts, tokenizer_type<span class="op">=</span><span class="st">'basic_english'</span>, specials<span class="op">=</span>[<span class="st">'&lt;unk&gt;'</span>], device<span class="op">=</span>device):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Instantiate tokenizer</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> get_tokenizer(tokenizer_type)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize text data</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> [tokenizer(text) <span class="cf">for</span> text <span class="kw">in</span> texts]</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build vocabulary</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> build_vocab_from_iterator(tokens, specials<span class="op">=</span>specials)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set default index for unknown tokens</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    vocab.set_default_index(vocab[<span class="st">'&lt;unk&gt;'</span>])</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tokenized texts to a tensor</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    tokenized_texts <span class="op">=</span> [torch.tensor([vocab[token] <span class="cf">for</span> token <span class="kw">in</span> text], dtype<span class="op">=</span>torch.int64, device<span class="op">=</span>device) <span class="cf">for</span> text <span class="kw">in</span> tokens]</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized_texts, vocab</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Next. I construct the <code>ResumeDataset</code> iterator, which preprocesses the text data using the <code>tokenization</code> function and indexes samples for the <code>DataLoader</code> during training. The <code>__len__</code> method returns the length of the dataset, the <code>vocab_size</code> method returns the size of the vocabulary, the <code>num_class</code> method returns the number of unique classes in the dataset, and the <code>__getitem__</code> method returns a sample of text and label from the dataset.</p>
<div id="a759e257" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResumeDataset(Dataset):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dataset initialization and preprocessing</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize dataset attributes</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text <span class="op">=</span> data.iloc[:,<span class="dv">1</span>]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> data.iloc[:,<span class="dv">0</span>]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenized_texts, <span class="va">self</span>.vocab <span class="op">=</span> tokenization(<span class="va">self</span>.text)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get length of dataset</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get vocabulary size</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> vocab_size(<span class="va">self</span>):</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.vocab)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get number of classes</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> num_class(<span class="va">self</span>):</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels.unique())</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get item from dataset</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        sequence <span class="op">=</span> <span class="va">self</span>.tokenized_texts[idx]</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="va">self</span>.labels[idx]</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sequence, label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I also define a <code>collate_fn</code> function to use dynamic padding when batching the data. Dynamic padding is a technique used to pad sequences to the length of the longest sequence in a batch, as opposed to the longest sequence in the entire dataset. Because the model expects uniform dimensions to perform operations, sequences need to be padded to ensure each one has the same length. Dynamic padding allows the model to process sequences of varying lengths more efficiently, saving memory and computation time.</p>
<div id="f597942f" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collate_fn(batch):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    sequences, labels <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>batch)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pad sequences to the longest sequence in the batch</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    sequences_padded <span class="op">=</span> pad_sequence(sequences, batch_first<span class="op">=</span><span class="va">True</span>, padding_value<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert labels to tensor</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.tensor(labels, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sequences_padded, labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Finally, I instantiate the <code>ResumeDataset</code> class and split the dataset into 70% training, 15% validation, and 15% test sets using the <code>random_split</code> function. I create <code>DataLoader</code> iterators for each set, using the <code>collate_fn</code> function to apply dynamic padding to the sequences.</p>
<div id="1b2c3e92" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> ResumeDataset(data)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset, test_dataset <span class="op">=</span> random_split(dataset, [<span class="fl">0.7</span>, <span class="fl">0.15</span>, <span class="fl">0.15</span>])</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_fn)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_fn)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="model-architecture" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="model-architecture">Model Architecture</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/projects/resume-analyzer/nn_files/fnn-architecture.jpg" class="img-fluid"></p>
</div></div><p>The model is a simple Feedforward Neural Network with an embedding layer, followed by two fully connected layers with ReLU activation functions, and a final fully connected layer with the number of classes as the output size. The model architecture is defined in the <code>SimpleNN</code> class, which takes the vocabulary size, embedding size, number of classes as parameters. The <code>expansion_factor</code> is defined to determine the hidden dimension size, here set to 2.</p>
<p>The <code>EmbeddingBag</code> function efficiently computes the embeddings by performing a two-step operation: first, it creates embeddings for the input indices, adn then reduces the embedding output using the mean across the sequence dimension. This is useful for sequences of varying lengths, as it allows the model to process them more efficiently.</p>
<div id="41cb9935" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNN(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_size, num_class, expansion_factor<span class="op">=</span><span class="dv">2</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.EmbeddingBag(vocab_size, embed_size, sparse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dim <span class="op">=</span> embed_size <span class="op">*</span> expansion_factor</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer1 <span class="op">=</span> nn.Linear(embed_size, <span class="va">self</span>.hidden_dim)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer2 <span class="op">=</span> nn.Linear(<span class="va">self</span>.hidden_dim, embed_size)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer3 <span class="op">=</span> nn.Linear(embed_size, num_class)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.layer1(x))</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.layer2(x))</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layer3(x)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="hyperparameters-and-training" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="hyperparameters-and-training">Hyperparameters and Training</h3>
<p>Before training, I set the hyperparameters for training the neural network. The vocabulary size and number of classes are obtained from the <code>ResumeDataset</code> class.The embedding size is set to 60 and the learning rate is set to 1e-3. The model is trained for 40 epochs.</p>
<div id="051882a3" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> dataset.vocab_size()</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>num_class <span class="op">=</span> dataset.num_class()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>embed_size <span class="op">=</span> <span class="dv">60</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>lr<span class="op">=</span><span class="fl">0.001</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">40</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I then instantiate the model, sending it to the available device, and define the loss function and optimizer. The loss function is set to CrossEntropyLoss, which is suitable for multi-class classification tasks. The optimizer is Adam, an adaptive learning rate optimization algorithm well-suited for training deep neural networks. In addition, I define a learning rate scheduler that reduces the learning rate by a factor of 0.1 if the validation loss does not improve for <code>patience</code> number of epochs. This prevents the model from overfitting and improves generalization. The model and hyperparameters are then passed for training using the <code>train_model</code> function<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;During fine-tunning, I found the model converged with better accuracy when using a dropout rate of 0.4.</p></div></div><p>To visualize the training progress, I set the <code>visualize</code> parameter to ‘liveloss’, which uses the <a href="https://p.migdal.pl/livelossplot/">PlotLosses library</a> to create a dynamicallly updating plot that visulalized the training and validation loss and accuracy in real-time. This allows me to monitor the model’s performance and make adjustments to the hyperparameters if necessary.</p>
<div id="cb9edefb" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNN(vocab_size, embed_size, num_class, dropout<span class="op">=</span><span class="fl">0.4</span>).to(device)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> CrossEntropyLoss()</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> ReduceLROnPlateau(loss, patience<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler, visualize<span class="op">=</span><span class="st">'liveloss'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><img src="../assets/projects/resume-analyzer/nn_files/nn-plot.png" class="img-fluid"></p>
<pre><code>accuracy
  training           (min:    0.036, max:    0.712, cur:    0.707)
  validation         (min:    0.037, max:    0.694, cur:    0.685)
log loss
  training           (min:    0.873, max:    3.187, cur:    0.921)
  validation         (min:    1.274, max:    3.184, cur:    1.330)
------------------------------
Best model saved:
Val Loss: 1.3300 | Val Acc: 0.6852
✅ Training complete!</code></pre>
<p>Starting with the loss, the training and validation losses decrease steadily until 30 epochs, showing that the model is effectively learning the patterns in the data. After this, the training loss continues to decrease, while the validation loss plateaus. However, the model finishes with only a small gap between the training and validation loss, with the training loss at 0.92 and the validation loss at 1.33. The small size of the gap indicates that the model is not overfitting the training data and generalizes well to unseen data.</p>
<p>Regarding accuracy, the training and validation accuracy increase steadily until 40 epochs, after which they converge. At convergence, the training accuracy is slightly higher than the validation accuracy, with the model achieving a training accuracy of 71% and a validation accuracy of 69%. This small difference between the training and validation accuracy indicates that the model does not overfit the training data and generalizes well to unseen data. The best saved model has a validation loss of 1.33 and a validation accuracy of 68.52%.</p>
</section>
<section id="evaluation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="evaluation">Evaluation</h3>
<p>After training the model, I evaluate its performance on the test set using the <code>test_model</code> function. The function takes the trained model, test data loader, and criterion as input and returns the accuracy of the model on the test set.</p>
<div id="8e9a2b87" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> test_model(model, test_loader, criterion)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Test Loss: 1.1444 | Test Acc: 0.7315
✅ Testing complete!</code></pre>
<p>The Feedforward Neural Network model achieves an accuracy of <strong>73.15%</strong> and a loss of 1.1444 on the test set. As expected from the training and validation plots, the model performs reasonably well on unseen data, with the test accuracy aligning closely with the validation accuracy observed during training. The consistency between validation and test accuracies suggests that the model successfully generalizes to new data and demonstrates robustness in its predictions.</p>
<p>Compared to the baseline model, the Feedforward Neural Network model achieved a lower accuracy on the test set. However, the model’s performance is still quite impressive considering the simplicity of the architecture. The model’s performance can likely be improved by introducing more advanced regularization techniques, using pre-trained word embeddings<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, or increasing the complexity of the model architecture. This last point is what I explore in the next section, where I implement a more advanced model architecture using a Transformer-based neural network that leverages self-attention mechanisms to capture long-range dependencies in the data.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Examples of pre-trained embeddings include: <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a>, <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, and <a href="https://fasttext.cc/">fastText</a>.</p></div></div><p>To conclude this section, I save the performance metrics of the Feedforward Neural Network model for later analysis.</p>
<div id="29b02759" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>save_performance(model_name<span class="op">=</span><span class="st">'Feedforward Neural Network'</span>,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                 architecture<span class="op">=</span><span class="st">'embed_layer-&gt;dropout-&gt;120-&gt;dropout-&gt;60-&gt;dropout-&gt;num_classes'</span>,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>                 embed_size<span class="op">=</span><span class="st">'60'</span>,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="st">'1e-3'</span>,</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="st">'50'</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'Adam'</span>,</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>                 criterion<span class="op">=</span><span class="st">'CrossEntropyLoss'</span>,</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>                 accuracy<span class="op">=</span><span class="fl">73.15</span>,</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="encoder-model" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="encoder-model">Encoder Model</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/projects/resume-analyzer/encoder_files/transformer_encoder-highlighted.png" class="img-fluid"></p>
</div></div><p>The following model utilizes the encoder component of the Transformer architecture for text classification. Unlike decoders, which convert dense representations into output sequences, encoders instead transform input sequences into dense representations. Their ability to extract sequence information and convert them to a dense representation makes them particularly useful for tasks such as sentiment analysis, named entity recognition, and text classification.</p>
<p>The encoder model follows the Transformer architecture described in <em>Attention is All You Need</em><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> and is used as a baseline for transformer-based models. I construct the encoder architecture with an embedding layer, a stack of encoder layers, and a feed-forward neural network for classification. I then initialize the hyperparameters for training and train the model using the Adam optimizer and CrossEntropyLoss criterion. The model’s performance is evaluated on the test set using accuracy as the evaluation metric.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Attention Is All You Need</p></div></div><p>The imported packages as well as the step in building the <code>DataLoader</code> are the same as those for the Feedforward Neural Network model (see <a href="https://marcocamilo.com/resume-analyzer#import-packages-and-data-1">packages</a> and <a href="https://marcocamilo.com/resume-analyzer#dataloader">data preparation</a>. Therefore, I skip directly to the model architecture.</p>
<blockquote class="blockquote">
<h3 id="takeaways-2" class="anchored">Takeaways</h3>
<ul>
<li>The Transformer Encoder model achieves an accuracy of 75% on the test set.</li>
<li>This model serves as a robust baseline for transformer-based models in text classification tasks.</li>
</ul>
</blockquote>
<section id="model-architecture-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="model-architecture-1">Model Architecture</h3>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/projects/resume-analyzer/encoder_files/encoder.jpg" class="img-fluid figure-img"></p>
<figcaption>Modified image from <a href="https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder">Sebastian Raschka</a></figcaption>
</figure>
</div>
</div><div id="fn8"><p><sup>8</sup>&nbsp;The multi-head self-attention mechanism is a component of the transformer model that weights the importance of different elements in a sequence by computing attention scores multiple times in parallel across different linear projections of the input.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;Layer normalization is a normalization technique that uses the mean and variance statistics calculated across all features.</p></div></div><p>The Encoder model consists of an embedding layer, a stack of encoder layers, and a fully connected neural network for classifying the outputs. The embedding layer converts the input sequences into dense representations, which are then passed through the encoder layers. Each encoder layer consists of a multi-head self-attention mechanism<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> followed by a residual connection with layer normalization<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> and a feed-forward neural network, followed by another residual connection with layer normalization. The output is finally passed through a feed-forward neural network for classification.</p>
<p>I decide to build the encoder model from scratch, as it allows me to better understand the architecture and the components of the model. I opt for a modular approach, where I construct each component of the model as a separate class and then combine them in the <code>TransformerEncoder</code> class.</p>
<blockquote class="blockquote">
<p><strong>Further Reading:</strong></p>
<p>The implementation of this model was in great part inspired by the following resources: - <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> - <a href="https://www.youtube.com/watch?v=ISNdQcPhsts">Coding a Transformer from Scratch on PyTorch (YouTube)</a> - <a href="https://towardsdatascience.com/text-classification-with-transformer-encoders-1dcaa50dabae">Text Classification with Transformer Encoders</a></p>
</blockquote>
<div id="199992b7" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EmbeddingLayer(nn.Module):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, d_model: <span class="bu">int</span>):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dimensions of embedding layer</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, d_model)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Embedding dimension</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.embedding(x) <span class="op">*</span> math.sqrt(<span class="va">self</span>.d_model)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEmbedding(nn.Module):</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, d_model: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize positional embedding matrix (vocab_size, d_model)</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(vocab_size, d_model)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Positional vector (vocab_size, 1)</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, vocab_size).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Frequency term</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> <span class="op">-</span>(math.log(<span class="dv">10000</span>) <span class="op">/</span> d_model))</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sinusoidal functions</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add batch dimension</span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save to class</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :]</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-6</span>):</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Learnable parameters</span></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(d_model))</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.ones(d_model))</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Numerical stability in case of 0 denominator</span></span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> x.std(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear combination of layer norm with parameters gamma and beta</span></span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> (x <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="va">self</span>.eps) <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResidualConnection(nn.Module):</span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer normalization for residual connection</span></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(d_model)</span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x1, x2):</span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(<span class="va">self</span>.norm(x1 <span class="op">+</span> x2))</span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(nn.Module):</span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, d_ff: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2048</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear layers and dropout</span></span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(d_model, d_ff)</span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(d_ff, d_model)</span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear2(<span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.linear1(x))))</span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span><span class="fl">0.1</span>, qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>, is_causal: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>,  <span class="st">"d_model is not divisible by num_heads"</span></span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb26-76"><a href="#cb26-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb26-77"><a href="#cb26-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> d_model <span class="op">//</span> num_heads</span>
<span id="cb26-78"><a href="#cb26-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> dropout</span>
<span id="cb26-79"><a href="#cb26-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.is_causal <span class="op">=</span> is_causal</span>
<span id="cb26-80"><a href="#cb26-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-81"><a href="#cb26-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qkv <span class="op">=</span> nn.Linear(d_model, <span class="dv">3</span> <span class="op">*</span> d_model, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb26-82"><a href="#cb26-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(num_heads <span class="op">*</span> <span class="va">self</span>.head_dim, d_model)</span>
<span id="cb26-83"><a href="#cb26-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_layer <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb26-84"><a href="#cb26-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-85"><a href="#cb26-85" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-86"><a href="#cb26-86" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_length <span class="op">=</span> x.shape[:<span class="dv">2</span>]</span>
<span id="cb26-87"><a href="#cb26-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-88"><a href="#cb26-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformation and split into query, key, and value</span></span>
<span id="cb26-89"><a href="#cb26-89" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv(x)  <span class="co"># (batch_size, seq_length, 3 * embed_dim)</span></span>
<span id="cb26-90"><a href="#cb26-90" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> qkv.view(batch_size, seq_length, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)  <span class="co"># (batch_size, seq_length, 3, num_heads, head_dim)</span></span>
<span id="cb26-91"><a href="#cb26-91" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> qkv.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>)  <span class="co"># (3, batch_size, num_heads, seq_length, head_dim)</span></span>
<span id="cb26-92"><a href="#cb26-92" aria-hidden="true" tabindex="-1"></a>        queries, keys, values <span class="op">=</span> qkv  <span class="co"># 3 * (batch_size, num_heads, seq_length, head_dim)</span></span>
<span id="cb26-93"><a href="#cb26-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-94"><a href="#cb26-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scaled Dot-Product Attention</span></span>
<span id="cb26-95"><a href="#cb26-95" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> F.scaled_dot_product_attention(queries, keys, values, attn_mask<span class="op">=</span>mask, dropout_p<span class="op">=</span><span class="va">self</span>.dropout, is_causal<span class="op">=</span><span class="va">self</span>.is_causal)</span>
<span id="cb26-96"><a href="#cb26-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-97"><a href="#cb26-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine heads, where self.d_model = self.num_heads * self.head_dim</span></span>
<span id="cb26-98"><a href="#cb26-98" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> context_vec.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, seq_length, <span class="va">self</span>.d_model)</span>
<span id="cb26-99"><a href="#cb26-99" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> <span class="va">self</span>.dropout_layer(<span class="va">self</span>.linear(context_vec))</span>
<span id="cb26-100"><a href="#cb26-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-101"><a href="#cb26-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vec</span>
<span id="cb26-102"><a href="#cb26-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-103"><a href="#cb26-103" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderLayer(nn.Module):</span>
<span id="cb26-104"><a href="#cb26-104" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, hidden_dim: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-105"><a href="#cb26-105" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-106"><a href="#cb26-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-head self-attention mechanism</span></span>
<span id="cb26-107"><a href="#cb26-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.multihead_attention <span class="op">=</span> MultiHeadAttention(d_model, num_heads, dropout)</span>
<span id="cb26-108"><a href="#cb26-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First residual connection and layer normalization</span></span>
<span id="cb26-109"><a href="#cb26-109" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual1 <span class="op">=</span> ResidualConnection(d_model, dropout)</span>
<span id="cb26-110"><a href="#cb26-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feed-forward neural network</span></span>
<span id="cb26-111"><a href="#cb26-111" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(d_model, hidden_dim, dropout)</span>
<span id="cb26-112"><a href="#cb26-112" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second residual connection and layer normalization</span></span>
<span id="cb26-113"><a href="#cb26-113" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual2 <span class="op">=</span> ResidualConnection(d_model, dropout)</span>
<span id="cb26-114"><a href="#cb26-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-115"><a href="#cb26-115" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-116"><a href="#cb26-116" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.residual1(x, <span class="va">self</span>.multihead_attention(x, mask))</span>
<span id="cb26-117"><a href="#cb26-117" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.residual2(x, <span class="va">self</span>.feed_forward(x))</span>
<span id="cb26-118"><a href="#cb26-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb26-119"><a href="#cb26-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-120"><a href="#cb26-120" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderStack(nn.Module):</span>
<span id="cb26-121"><a href="#cb26-121" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, hidden_dim: <span class="bu">int</span>, num_layers: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-122"><a href="#cb26-122" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-123"><a href="#cb26-123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stack of encoder layers</span></span>
<span id="cb26-124"><a href="#cb26-124" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([EncoderLayer(d_model, num_heads, hidden_dim, dropout) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)])</span>
<span id="cb26-125"><a href="#cb26-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-126"><a href="#cb26-126" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-127"><a href="#cb26-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb26-128"><a href="#cb26-128" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, mask)</span>
<span id="cb26-129"><a href="#cb26-129" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb26-130"><a href="#cb26-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-131"><a href="#cb26-131" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerClassifier(nn.Module):</span>
<span id="cb26-132"><a href="#cb26-132" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, d_model: <span class="bu">int</span>, num_heads: <span class="bu">int</span>, hidden_dim: <span class="bu">int</span>, num_layers: <span class="bu">int</span>, out_features: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb26-133"><a href="#cb26-133" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-134"><a href="#cb26-134" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> EmbeddingLayer(vocab_size, d_model)</span>
<span id="cb26-135"><a href="#cb26-135" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> PositionalEmbedding(vocab_size, d_model, dropout)</span>
<span id="cb26-136"><a href="#cb26-136" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> EncoderStack(d_model, num_heads, hidden_dim, num_layers, dropout)</span>
<span id="cb26-137"><a href="#cb26-137" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(d_model, out_features)</span>
<span id="cb26-138"><a href="#cb26-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-139"><a href="#cb26-139" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-140"><a href="#cb26-140" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb26-141"><a href="#cb26-141" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.positional_embedding(x)</span>
<span id="cb26-142"><a href="#cb26-142" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.encoder(x, mask)</span>
<span id="cb26-143"><a href="#cb26-143" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb26-144"><a href="#cb26-144" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.classifier(x)</span>
<span id="cb26-145"><a href="#cb26-145" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="hyperparameters-and-training-1" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters-and-training-1">Hyperparameters and Training</h3>
<p>With the model constructed, I initialize the hyperparameters for training. As with the previous model, I obtain the vocabulary size and number of output features from the <code>ResumeDataset</code> class. The embedding size is fixed at 80, while the hidden dimension is set to 180. For the multi-head attention mechanism, I use 4 heads, with an the encoder stack comprising of 4 layers. I train the model for 20 epochs at a learning rate of 1e-3.</p>
<div id="d43f0d89" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> dataset.vocab_size()</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>hidden_dim <span class="op">=</span> <span class="dv">180</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>num_layers <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>out_features <span class="op">=</span> dataset.num_class()</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I instantiate the model with the hyperparameters and move it to the device. The criterion and optimizer are left unchanged from the previous model, with the optimizer set to the Adam optimizer and the criterion set to the CrossEntropyLoss, suitable for multi-class classification tasks. I also initialize the learning rate scheduler with a patience of 2, to prevent the model from overfitting. The model is then trained using the <code>train_model</code> function.</p>
<div id="b1e4dc5a" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TransformerClassifier(vocab_size, d_model, num_heads, </span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>                                hidden_dim, num_layers, out_features, dropout<span class="op">=</span><span class="dv">0</span>).to(device)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> CrossEntropyLoss()</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> ReduceLROnPlateau(loss, patience<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><img src="../assets/projects/resume-analyzer/encoder_files/encoder-plot.png" class="img-fluid"></p>
<pre><code>accuracy
  training           (min:    0.043, max:    1.000, cur:    1.000)
  validation         (min:    0.035, max:    0.794, cur:    0.785)
log loss
  training           (min:    0.027, max:    3.276, cur:    0.029)
  validation         (min:    1.023, max:    3.273, cur:    1.071)
------------------------------
Best model saved:
Val Loss: 1.0709 | Val Acc: 0.7847
✅ Training complete!</code></pre>
<p>According to the performance plots, despite the encoder model achieving a better validation accuracy than the Feedforward Neural Network, the model exhibits a large gap between the training and validation performance, indicating that the model is overfitting. The model rapidly decreases the training and validation losses, and rapidly incrases their accuracies during the first 14 epochs. For the following 6 epochs, however, the training loss continues to rapidly decrease from 1.25 to near 0, while the validation loss stagnates at around 1.07. Similarly, the training accuracy rapidly increases to 100%, while the validation accuracy remains at 78%. The model converges 20 epochs in with a significant gap between training and validation models, indicating that the model is overfitting.</p>
<p>Nevertheless, the model achieves a validation accuracy of 78%, which is slightly higher than the Feedforward Neural Network model. I decide to evaluate the model on the test set to obtain the final accuracy.</p>
</section>
<section id="evaluation-1" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-1">Evaluation</h3>
<div id="4e813e81" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> test_model(model, test_loader, criterion)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Test Loss: 1.2526 | Test Acc: 0.7454
✅ Testing complete!</code></pre>
<p>Despite having implemented a more advanced model architecture with the addition of the multi-head self-attention mechanism, the encoder model achieves an accuracy of 74.5%, similar to the Feedforward Neural Network model. As discussed earlier, the model exhibits a large gap between the training and validation performance, which could be hindering the model’s generalization capabilities.</p>
<p>Given a better training and validation performance, the model could potentially achieve a higher accuracy on the test set. The model’s performance could likely be improved by means of data augmentation, modifying hyperparameters such as embedding size, hidden dimension, and number of layers, or by using a different optimizer or learning rate scheduler.</p>
<p>As with the previous models, I save the performance metrics for later analysis.</p>
<div id="07240753" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>save_performance(model_name<span class="op">=</span><span class="st">'Transformer'</span>,</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>                 architecture<span class="op">=</span><span class="st">'embed_layer-&gt;encoder-&gt;linear_layer'</span>,</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>                 embed_size<span class="op">=</span><span class="st">'64'</span>,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="st">'1e-3'</span>,</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="st">'20'</span>,</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'Adam'</span>,</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>                 criterion<span class="op">=</span><span class="st">'CrossEntropyLoss'</span>,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>                 accuracy<span class="op">=</span><span class="dv">80</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="bert" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bert">BERT</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/projects/resume-analyzer/bert_files/bert-classifier.png" class="img-fluid"></p>
</div></div><p>The last model is also an encoder-based model called the Bidirectional Encoder Representations from Transformers (BERT). BERT is a pre-trained transformer-based model that can be fine-tuned for a wide range of NLP tasks by adding task-specific output layers. The model is bidirectional, meaning that it can take into account the context of a word by looking at both the left and right context. This allows the model to capture a wider range of contextual information, which is particularly useful for tasks such as text classification. This allows the model to capture rich semantic relationships and dependencies within text sequences, which is particularly useful for tasks such as text classification.</p>
<p>As with previous PyTorch models, I create an iterable dataset using the <code>Dataset</code> and <code>DataLoader</code> classes, to tokenize the resumes using the BERT tokenizer, pad sequences to equal lengths, split the data and batch the data for training. I then construct the model architecture, consisting of the pre-trained BERT base model, an added dropout layer and a linear output layer for classification. I initialize the hyperparameters and train the model using Cross Entropy Loss along with the Adam optimizer. The model is evaluated as with other deep learning models using accuracy.</p>
<blockquote class="blockquote">
<h2 id="takeaways-3" class="anchored">### Takeaways</h2>
<ul>
<li></li>
<li></li>
</ul>
</blockquote>
<section id="import-packages" class="level3">
<h3 class="anchored" data-anchor-id="import-packages">Import Packages</h3>
<p>In addition to the standard deep learning packages used so far, I import three classes from the <code>transformers</code> package:</p>
<ul>
<li><code>BertModel</code>: loads the pre-trained BERT model.</li>
<li><code>BertTokenizer</code>: constructs a BERT tokenizer.</li>
<li><code>DataCollatorWithPadding</code>: builds a batch with dynamically padded sequences.</li>
</ul>
<p>Because of BERT’s output format, I also import a custom <code>train_BERT</code> and <code>test_BERT</code> function, which are specifically tailored to return the model’s training and test performances using BERT’s output, including input IDs and attention masks.</p>
<div id="ac13e930" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel, BertTokenizer, DataCollatorWithPadding</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.modules.dl <span class="im">import</span> train_BERT, test_BERT</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="dataset-and-dataloader-1" class="level3">
<h3 class="anchored" data-anchor-id="dataset-and-dataloader-1">Dataset and DataLoader</h3>
<p>Before creating the dataset and dataloader, I initizalize the tokenizer and define the pre-trained BERT model. I then create the <code>ResumeBertDataset</code>, which tokenizes resumes and prepares them for model input.</p>
<p>In contrast to the previous model, I configure the tokenizer using the <code>.encode_plus</code> method, which returns a dictionary of the batch encodings, including tokenized input sequences and attention masks. I set the <code>padding</code> parameter to <code>False</code> to avoid padding, as this will be handled dynamically by the data collator. Additionally, I set <code>truncation</code> to <code>True</code> to truncate sequences that exceed the maximum length. The method also adds the special tokens <code>[CLS]</code> and <code>[SEP]</code> to the input sequences, required by BERT. I set the <code>return_tensors</code> parameter to <code>'pt'</code> to return PyTorch tensors. Finally, I return a dictionary with the processed input sequences, attention masks, and labels.</p>
<div id="4e7c3bce" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResumeBertDataset(Dataset):</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, max_length, tokenizer<span class="op">=</span>tokenizer, device<span class="op">=</span>device):</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.texts <span class="op">=</span> data.iloc[:,<span class="dv">1</span>].values</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> torch.tensor(data.iloc[:,<span class="dv">0</span>])</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_length <span class="op">=</span> max_length</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> num_class(<span class="va">self</span>):</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels.unique())</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        resumes <span class="op">=</span> <span class="va">self</span>.texts[idx]</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> <span class="va">self</span>.labels[idx]</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        encoding <span class="op">=</span> <span class="va">self</span>.tokenizer.encode_plus(</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>            resumes,</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>            add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>            max_length<span class="op">=</span><span class="va">self</span>.max_length,</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>            truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>            return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>            return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        ).to(<span class="va">self</span>.device)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>        input_ids <span class="op">=</span> encoding[<span class="st">'input_ids'</span>].squeeze()</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>        attention_mask <span class="op">=</span> encoding[<span class="st">'attention_mask'</span>].squeeze()</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>            <span class="st">'input_ids'</span>: input_ids,</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>            <span class="st">'attention_mask'</span>: attention_mask,</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>            <span class="st">'labels'</span>: labels</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I initialize the dataset and set <code>max_length</code> to 512, which is the maximum number of tokens that BERT can process. I then split the dataset into 70% training, 15% validation, and 15% test sets using the <code>random_split</code> function. I use the <code>DataCollatorWithPadding</code> class to dynamically pad sequences to the maximum length in each batch. Finally, I create dataloaders for the training, validation, and test sets using the <code>DataLoader</code> class, setting the batch size to 16, shuffling the data, and assigning the data collator.</p>
<div id="10958d26" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> ResumeBertDataset(data, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset, test_dataset <span class="op">=</span> random_split(dataset, [<span class="fl">0.7</span>, <span class="fl">0.15</span>, <span class="fl">0.15</span>])</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorWithPadding(tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">16</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>data_collator)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">16</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>data_collator)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">16</span>, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>data_collator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="model-architecture-2" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture-2">Model Architecture</h3>
<p>The <code>BertResumeClassifier</code> model consists of the pre-trained BERT base model, a dropout layer, and a linear output layer to classify the resumes. The BERT model uses the <code>bert-base-uncased</code> pre-trained model to generate contextual embeddings from the input sequences. I extract the embeddings by indexing the <code>pooler_output</code> key from the output dictionary. These embeddings are then passed through a dropout layer to prevent overfitting, and subsequently fed into a fully connected linear layer which maps the embeddings to the desired number of output classes for classification.</p>
<div id="00a3020f" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BertResumeClassifier(nn.Module):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_classes: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.01</span>):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bert <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="va">self</span>.bert.config.hidden_size, n_classes)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, attention_mask):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        pooled_output <span class="op">=</span> <span class="va">self</span>.bert(</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>          input_ids<span class="op">=</span>input_ids,</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>          attention_mask<span class="op">=</span>attention_mask</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        )[<span class="st">'pooler_output'</span>]</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.dropout(pooled_output)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.out(output)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="hyperparameters-and-training-2" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters-and-training-2">Hyperparameters and Training</h3>
<p>Since BERT is a pre-trained model and it accepts a fixed input size of 512 tokens, the are less parameters to set from the model itself. The only parameter that needs to be set is the number of classes—which, as before, is obtained from the <code>Dataset</code> class.</p>
<p>I initialize the model, loss function, optimizer, and number of epochs. I use as before the Cross Entropy Loss function and the Adam optimizer, although this time with a learning rate of 2e-5, since it seemed to result in better performance. I train the model for only 10 epochs.</p>
<div id="8fdb228a" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> dataset.num_class()</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertResumeClassifier(n_classes).to(device)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> CrossEntropyLoss()</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>train_BERT(model, train_loader, val_loader, epochs, criterion, optimizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><img src="../assets/projects/resume-analyzer/bert_files/bert-plot.png" class="img-fluid"></p>
<pre><code>accuracy
    training             (min:    0.050, max:    0.991, cur:    0.991)
    validation           (min:    0.120, max:    0.933, cur:    0.933)
log loss
    training             (min:    0.081, max:    3.189, cur:    0.081)
    validation           (min:    0.414, max:    3.002, cur:    0.428)
------------------------------
Best model saved:
Val Loss: 0.4143 | Val Acc: 0.9190
✅ Training complete!</code></pre>
<p>The BERT model shows a significant and steady decrease in both training and validation losses over the 10 epochs, indicating effective learning of data patterns. By the end of training, the model’s train loss decreased from 3.1394 to 0.0589, while the validation loss decreased from 2.7347 to 0.5070. This consistent reduction highlights the model’s ability to capture and generalize the data without overfitting, as demonstrated by the small gap between the training and validation losses by the end of the 10th epoch.</p>
<p>Regarding accuracy, the training and validation accuracies also show a steady increase over the 10 epochs. The training accuracy increased from 0.0774 to 0.9950, with validation accuracy improving from 0.3472 to 0.9028. This indicates that the model effectively learned the patterns in the data and generalizes well to unseen data. The small difference between the final training and validation accuracies demonstrates the model’s robustness and ability to avoid overfitting, ensuring reliable performance on new data. The best saved model has a validation loss of 0.5070 and an accuracy of 0.9028.</p>
</section>
<section id="evaluation-2" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-2">Evaluation</h3>
<p>As before, I evaluate the model using the <code>test_model</code> function using the best saved model.</p>
<div id="5bc49064" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> test_BERT(model, test_loader, criterion)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>Test Loss: 0.3984 | Test Acc: 0.9167
✅ Testing complete!</code></pre>
<p>The BERT model achieved an impressive performance on the test set, reaching an accuracy of 91.67% with a test loss of 0.3984. This significantly outperforms all previous models tested so far. As expected from the training and validation performances, the model is robust and generalizes very well to unseen data. This is further confirmed by the very close alignement between test and validation accuracies, both of which represent datasets not previously seen by the model. The close accuracy between the validation data and the test data shows the model is capable to generalize to new resumes and effectively classify them into the correct categories.</p>
<p>As with previous models, I save its performance using the <code>save_performance</code> function.</p>
<div id="c2f4cd67" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>save_performance(model_name<span class="op">=</span><span class="st">'BERT'</span>,</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>                 architecture<span class="op">=</span><span class="st">'bert-base-uncased&gt;dropout-&gt;linear_layer'</span>,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                 embed_size<span class="op">=</span><span class="st">'768'</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                 learning_rate<span class="op">=</span><span class="st">'2e-5'</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="st">'10'</span>,</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'Adam'</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                 criterion<span class="op">=</span><span class="st">'CrossEntropyLoss'</span>,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>                 accuracy<span class="op">=</span><span class="dv">90</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="results-and-discussion" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="results-and-discussion">Results and Discussion</h2>
<p>In this section of the project, I explored four different models for resume classification: Linear SVC, FNN, Transformer, and BERT. I evaluated the performance of each model using the accuracy metric and collected the results after each deployment. Below I plot the accuracy of each model and discuss the results.</p>
<div id="bf5b513d" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>View Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>evaluation_df <span class="op">=</span> pd.read_json(<span class="st">'./output/classifier_performance.json'</span>).sort_values(by<span class="op">=</span><span class="st">'accuracy'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.barplot(evaluation_df, x<span class="op">=</span><span class="st">'model'</span>, y<span class="op">=</span><span class="st">'accuracy'</span>, hue<span class="op">=</span><span class="st">'model'</span>, palette<span class="op">=</span><span class="st">'hls'</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>[ax.bar_label(container, fmt<span class="op">=</span><span class="st">"</span><span class="sc">%0.2f%%</span><span class="st">"</span>) <span class="cf">for</span> container <span class="kw">in</span> ax.containers]</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/projects/resume-analyzer/bert_files/bert-plot.png" class="img-fluid"></p>
</div></div><p>All four models performed well in classifying resumes, achieving accuracies above 70%. This success can be largely attributed to the effectiveness of the data preparation process, including text preprocessing, data balancing, and robust vectorization techniques. These preprocessing steps provided the models with high-quality input features that significantly enhanced their performance.</p>
<p>Upon closer evaluation, the models can be grouped into two categories based on their performance. The first group, achieving around 90% accuracy, includes the Linear SVC and BERT models. The second group, with accuracies around 70%, includes the FNN and Transformer models. Interestingly, the two best-performing models feature both the simplest and most complex architectures respectively, while the models with the lowest performance have more complex architectures than the baseline model. I discuss the reasons behind these results below.</p>
<p>Linear SVC’s high performance can be attributed its simplicity and the effective feature representation<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. The model is a classical machine learning algorithm that uses a linear kernel and no deep learning, which yields a simple architecture that is easy to train. Additionally, the model was trained on TF-IDF vectors, which result in a matrix with simple, but interpretable and informative features. This simplicity reduces the risk of overfitting and the straightforward feature representation contributes to the model’s high accuracy and fast training times.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;To read more on the efficiency of linear classifiers in text classification, see <a href="https://doi.org/10.18653/v1/2023.acl-short.160">Lin, Y.-C. et al.&nbsp;(2023) ‘Linear Classifier: An Often-Forgotten Baseline for Text Classification’</a>.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;See <a href="https://doi.org/10.48550/ARXIV.1810.04805">Devlin, J. et al.&nbsp;(2018) ‘BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding’</a>.</p></div></div><p>In contrast, BERT’s performance strems from its pre-trained nature and high-quality embeddings. The model was pre-trained on a large corpus and thus has the ability to generate embeddings that encode deep semantic information<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. Moreover, its bidirectional nature captures a wide range of contextual information across a long-range dependencies. Its suitability for Transfer Learning allows maximizing its pre-trained weights and easily fine-tuning on resume classification task, resulting in the high accuracy achieved.</p>
<p>The FNN and Transformer models, despite their increased complexity, achieved lower accuracies of around 70%. The Feedforward Neural Network, while more advanced than a linear model, lacks the ability to capture sequential dependencies and contextual nuances in the data, thus expected to perform worse than the transformer models.</p>
<p>However, the Transformer model should have been able to capture the sequential dependencies in the resume data, given its state-of-the-art architecture and use of multi-head self-attention. But contrary to BERT, the Transformer used in this project was not pre-trained on a large corpus. This limited its ability to generate high-quality dense representations of the texts. Additionally, insufficient fine-tuning may have prevented the Transformer from reaching its full potential. Given the results of a similar, more complex model such as BERT, additional hyperparameter tuning and training time could improve its performance.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this project, I explored the task of resume classification using machine learning and deep learning models. The process began with preprocessing the resume data, including tokenizing the resumes and preparing them for model input. I implemented four models: a Linear Support Vector Classifier, a Feedforward Neural Network, a Transformer model, and a BERT model. These models were trained and evaluated on the resume dataset, with performance compared based on accuracy.</p>
<p>The results were very insightful and provoked interesting observations on the role of model complexity and feature representation in achieving high performance. The BERT model achieved the highest accuracy of 91.67%, closely followed by the LinearSVC model at 87.15%. The Feedforward Neural Network and Transformer models achieved lower accuracies of 73.15% and 74.54%, respectively. BERT’s superior performance can be attributed to its pre-trained transformer architecture, which captures rich semantic relationships and dependencies within text sequences. The strong performance of the LinearSVC model can be attributed to its simplicity and efficiency in handling high-dimensional data, leveraging high-quality, interpretable feature representations such as TF-IDF vectors.</p>
<p>Two important observations arise from these results:</p>
<blockquote class="blockquote">
<ol type="1">
<li>State-of-the-art transformer models, combined with transfer learning from pre-trained models like BERT, yield the best performance.</li>
<li>Simple models with high-quality, interpretable feature representations such as TF-IDF vectors, can also achieve high performance.</li>
</ol>
</blockquote>
<p>These contrasting observations indicate that model complexity alone does not guarantee high performance. This is further supported by the fact that the two models with the lowest accuracies have more complex architectures than the baseline.</p>
<blockquote class="blockquote">
<p><strong>The quality of the feature representation and the ability to capture contextual information across dependencies are more important factors in achieving high performance.</strong></p>
</blockquote>
<p>Ultimately, the best model depends on the requirements of the task at hand and the resources available for development. For tasks where high performance is critical and ample resources are available, using state-of-the-art transformer models such as BERT with transfer learning are recommended. For tasks prioritizing simplicity and interpretability, Linear SVC with TF-IDF vectors still offers a high-performance, resource-efficient alternative.</p>


</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/marcocamilo\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Marco-Andrés Camilo-Pietri</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>