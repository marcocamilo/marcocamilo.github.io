---
title: Airline On-Time EDA
subtitle: Exploring Airline Performance from 2 Million Flights across 40 Years
description: Explore a data-driven journey into predicting and mitigating customer churn with this project on predictive analytics. Leveraging a Random Forest Classifier, discover insights that empower businesses to proactively retain customers and enhance overall satisfaction.
featured: 2
image: "![](https://i.dailymail.co.uk/i/pix/2013/05/02/article-2318173-19957F94000005DC-90_964x639.jpg)"
category: Exploratory Data Analysis
type: EDA
skills:
    - Pandas
    - NumPy
    - Matplotlib
    - Seaborn
    - Dashboarding
---

{{< meta image >}}

Embark on a data-driven exploration into predicting and mitigating customer churn with our project on predictive analytics. By harnessing the power of a Random Forest Classifier, uncover valuable insights that enable businesses to preemptively address customer churn and elevate overall satisfaction. Join us on this journey as we delve into the realm of customer retention and optimization.

## Importing Packages and Data

```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import seaborn as sns
import missingno as msno

import sys
sys.path.append("./src/modules/")
from variables_03 import *
from mac_functions import *

figsize = (14,8)

plt.rcParams['figure.figsize'] = figsize
sns.set(rc={'figure.figsize':figsize})
pd.set_option('display.max_columns', 200)
pd.options.display.float_format = '{:,.3f}'.format

df = pd.read_parquet('./data/interim/03-airline_2m.parquet')
```

## Time Variables

We decide to analyze the time variables (`CRSDepTime`, `DepTime`,
`CRSArrTime`, `ArrTime`) separate from the delay variables, as they
require different data cleaning tasks and provide deeper insights into
the distribution of the data. We will start by analyzing the
distribution of these variables and determine any potential issues that
may require further processing.

### Value Counts


``` python
time_cols = ['CRSDepTime', 'DepTime', 'CRSArrTime', 'ArrTime']
colors = ['red', 'green', 'yellow', 'blue']

time_hist(df, time_cols, colors, 75, 'Distribution of CRS and True Depature and Arrivals')
time_hist(df, time_cols, colors, 100, 'Distribution of CRS and True Depature and Arrivals: Zoomed into Values 0-100', limit=500)
```


![](/assets/projects/04-univariate_analysis_2_files/figure-html/cell-3-output-1.png)

![](/assets/projects/04-univariate_analysis_2_files/figure-html/cell-3-output-2.png)

The histograms above reveal several insights about the nature of time
variables in the dataset. Firstly, we see the distribution is not
continuous, but discrete with regular gaps throughout the range of
values. This is probably due to the HHMM format, which, representing
time values, excludes all values between 60 and 99. This results in a
discontinous range of values, which is not ideal for out analysis.

Moreover, there is a markedly lower frequency of values below 500,
showing that flights before 500 are less frequent. The second histogram
zooms into this range. The plot not only demosntrates the progression in
the number of *red eye* flights between midnight and 5am, but also shows
more clearly that the distribution of values is regularly discontinuous
at values between 60 and 99. This confirms that HHMM is not a suitable
format for time operations, and is instead must be replaced by a
`datetime` object. Moreover, late-night `ArrTime` values in particular
seem to be the most frequent among the time variables.

Finally, there is a markedly large number of 0s among time variables.
This could suggest that missing values are expressed by means of a 0
value. This might represent a two-fold issue: 0 is representing missing
values, while 2400 represents midnight in an invalid time format. This
results in midnight values and missing values are undistinguishable in
the dataset. This will require some processing before converting values
to a `datetime` object.

We will now address these issues in the following sections.

### Understanding 0s, 2400s and NaNs

We will start by exploring the value counts of 0s, 2400s and missing
values in the time variables. We will then decide how to handle these
values.


``` python
pd.DataFrame({
    '0': df[time_cols].eq(0).sum(),
    'na': df[time_cols].isna().sum(),
    '2400': df[time_cols].eq(2400).sum()
})
```


|           |    0 | na | 2400 |
|-----------|------|----|------|
| CRSDepTime| 6464 | 0  | 1    |
| DepTime   | 0    | 0  | 160  |
| CRSArrTime| 6464 | 0  | 37   |
| ArrTime   | 0    | 0  | 710  |

The value counts show some interesting patterns. Both CRS variables
`CRSDepTime` and `CRSArrTime` have the *exact* same number of 0s and do
not contain missing values. This makes it highly likely that these
values represent missing values. Additionally, the 2400 values occur
mostly in true time variables and tend more to be arrival times. They
are likely to represent midnight, but are not supported by the
`datetime` object.

Given the spike of 0 values shown in the histogram above, along with the
complete absence of NaN values in the CRS variables, we can conclude
that the 0s in the CRS variables represent missing values. We will deal
with how to fill these values in a later section.

### Feature Engineering: Datetime Conversion

As we stated above, the HHMM format is not a suitable format for time
operations, and is instead must be replaced by a `datetime` object. We
will start by converting the time variables to a `datetime` object. This
will allow us to perform date operations and visualizations.

However, in order to do this, we must also account for the 0s and 2400s
in the CRS variables, as well as the NaN values in the true time
variables. We will create a function that will convert the time
variables to a `datetime` object, while also accounting for these
values.


``` python
def convert_to_datetime(df, time_cols, date):
    matrix = df[time_cols].values
    matrix = matrix.astype('int').astype('str')
    matrix = np.char.zfill(matrix, 4)
    matrix[matrix == '0000'] = np.nan
    matrix[matrix == '2400'] = '0000'
    df[time_cols] = matrix
    for col in time_cols:
        df[col] = pd.to_datetime(date + ' ' + df[col], format="%Y-%m-%d %H%M", errors='coerce')
    return df

df = convert_to_datetime(df, time_cols, df['FlightDate'].astype(str))

df[time_cols]
```


<table>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998-01-02 16:40:00</td>
<td>1998-01-02 16:59:00</td>
<td>1998-01-02 18:36:00</td>
<td>1998-01-02 18:59:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>2009-05-28 12:04:00</td>
<td>2009-05-28 12:02:00</td>
<td>2009-05-28 15:41:00</td>
<td>2009-05-28 15:41:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2013-06-29 16:30:00</td>
<td>2013-06-29 16:44:00</td>
<td>2013-06-29 19:45:00</td>
<td>2013-06-29 19:42:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>2010-08-31 13:05:00</td>
<td>2010-08-31 13:05:00</td>
<td>2010-08-31 20:35:00</td>
<td>2010-08-31 20:15:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2006-01-15 18:20:00</td>
<td>2006-01-15 19:11:00</td>
<td>2006-01-15 20:26:00</td>
<td>2006-01-15 20:58:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1958943</td>
<td>2008-03-23 14:40:00</td>
<td>2008-03-23 14:44:00</td>
<td>2008-03-23 15:50:00</td>
<td>2008-03-23 15:43:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1958944</td>
<td>1999-01-05 09:45:00</td>
<td>1999-01-05 09:45:00</td>
<td>1999-01-05 12:42:00</td>
<td>1999-01-05 12:34:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1958945</td>
<td>2003-11-14 12:25:00</td>
<td>2003-11-14 12:19:00</td>
<td>2003-11-14 13:19:00</td>
<td>2003-11-14 13:08:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1958946</td>
<td>2012-05-15 18:30:00</td>
<td>2012-05-15 18:38:00</td>
<td>2012-05-15 19:50:00</td>
<td>2012-05-15 19:47:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1958947</td>
<td>2003-04-29 16:15:00</td>
<td>2003-04-29 16:10:00</td>
<td>2003-04-29 17:24:00</td>
<td>2003-04-29 17:10:00</td>
</tr>
</tbody>
</table>

<p>1958948 rows × 4 columns</p>
</div>

Now, all values are in a valid `datetime` format, and in a continuous
range of values. Moreover, all 0 values are now uniformely represented
by NaN values. Below we print the missing value counts for the CRS
variables, to confirm they have the same value count as 0s before this
transformation.


``` python
df[time_cols].isna().sum()
```


    CRSDepTime    6464
    DepTime          0
    CRSArrTime    6464
    ArrTime          0
    dtype: int64

### Data Cleaning: Filling Missing CRS Values

We will now consider how to best fill in the missing CRS values. Our
first option is to attempt to fill them using information from other
variables. We will start by exploring the rows with missing time values
to see if there is any pattern that could help us fill them.


``` python
df[df[time_cols].isna().any(axis=1)][dep_cols + arr_cols].sample(10)
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">DepDelay</th>
<th data-quarto-table-cell-role="th">DepDelayMinutes</th>
<th data-quarto-table-cell-role="th">DepDel15</th>
<th data-quarto-table-cell-role="th">DepartureDelayGroups</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
<th data-quarto-table-cell-role="th">ArrDelay</th>
<th data-quarto-table-cell-role="th">ArrDelayMinutes</th>
<th data-quarto-table-cell-role="th">ArrDel15</th>
<th data-quarto-table-cell-role="th">ArrivalDelayGroups</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">259064</td>
<td>NaT</td>
<td>1996-01-29 10:01:00</td>
<td>1.000</td>
<td>1.000</td>
<td>0.000</td>
<td>0.000</td>
<td>NaT</td>
<td>1996-01-29 12:54:00</td>
<td>-4.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1476653</td>
<td>NaT</td>
<td>1995-12-04 19:03:00</td>
<td>1.000</td>
<td>1.000</td>
<td>0.000</td>
<td>0.000</td>
<td>NaT</td>
<td>1995-12-04 21:48:00</td>
<td>-12.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">292410</td>
<td>NaT</td>
<td>1999-03-03 22:25:00</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>NaT</td>
<td>1999-03-03 00:50:00</td>
<td>6.000</td>
<td>6.000</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">450293</td>
<td>NaT</td>
<td>1996-01-17 08:10:00</td>
<td>-1.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
<td>NaT</td>
<td>1996-01-17 11:47:00</td>
<td>-4.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1389309</td>
<td>NaT</td>
<td>1995-11-05 10:09:00</td>
<td>8.000</td>
<td>8.000</td>
<td>0.000</td>
<td>0.000</td>
<td>NaT</td>
<td>1995-11-05 15:17:00</td>
<td>12.000</td>
<td>12.000</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">802408</td>
<td>NaT</td>
<td>1995-08-08 19:24:00</td>
<td>49.000</td>
<td>49.000</td>
<td>1.000</td>
<td>3.000</td>
<td>NaT</td>
<td>1995-08-08 20:40:00</td>
<td>49.000</td>
<td>49.000</td>
<td>1.000</td>
<td>3.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1477057</td>
<td>NaT</td>
<td>1998-01-13 07:18:00</td>
<td>18.000</td>
<td>18.000</td>
<td>1.000</td>
<td>1.000</td>
<td>NaT</td>
<td>1998-01-13 08:10:00</td>
<td>30.000</td>
<td>30.000</td>
<td>1.000</td>
<td>2.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1312808</td>
<td>NaT</td>
<td>1995-03-02 08:25:00</td>
<td>-3.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
<td>NaT</td>
<td>1995-03-02 10:40:00</td>
<td>-4.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1536069</td>
<td>NaT</td>
<td>1998-01-18 21:21:00</td>
<td>141.000</td>
<td>141.000</td>
<td>1.000</td>
<td>9.000</td>
<td>NaT</td>
<td>1998-01-18 23:38:00</td>
<td>93.000</td>
<td>93.000</td>
<td>1.000</td>
<td>6.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">866256</td>
<td>NaT</td>
<td>1999-06-19 15:42:00</td>
<td>2.000</td>
<td>2.000</td>
<td>0.000</td>
<td>0.000</td>
<td>NaT</td>
<td>1999-06-19 16:58:00</td>
<td>-12.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
</tr>
</tbody>
</table>

</div>

From the rows sample we can see that flights with CRS missing time
values all have the rest of their departure and arrival information.
Although we can see from the above value counts that delay variables
still have a small amount of missing values, we can consider calculating
the missing values from the true departure and arrival times, and the
delay variables. This will allow us to fill the missing values with the
sum of the true time and the delay.

Before we proceed, lets drop the rows with missing delay time values,
since their miniscule amount will not affect our analysis.


``` python
print(f"Before delay drop: {df.shape}")
df = df.dropna(subset=dep_cols[1:] + arr_cols[1:])
print(f"After delay drop: {df.shape}")
```


    Before delay drop: (1958948, 43)
    After delay drop: (1958862, 43)

Now, we will fill the missing CRS values by using the `.fillna()`
function and use the sum of the true time and the delay as the fill-in
value. We will do this for both the `DepTime` and `ArrTime` variables,
and print the missing value counts for the time variables to confirm the
changes.


``` python
df['CRSDepTime'] = df['CRSDepTime'].fillna(df['DepTime'] - pd.to_timedelta(df['DepDelay'], unit='m'))
df['CRSArrTime'] = df['CRSArrTime'].fillna(df['ArrTime'] - pd.to_timedelta(df['ArrDelay'], unit='m'))

df[time_cols].isna().sum()
```


    CRSDepTime    0
    DepTime       0
    CRSArrTime    0
    ArrTime       0
    dtype: int64

### Data Cleaning: Imputing Date and Time Zones for Datetime Objects

Before moving on, we must address an unfinished issue. Despite the
datatype conversion of time variables to `datetime`, all time variables
contain the same date, namely `FlightDate`. For many cases this is not
an issue, however, for flights that span midnight or cross time zones,
this could be a problem. Moreover, the times are in local time and do
not contain time zone information.

In order to accurately add the flight dates, we will need to add the
time zone for all time values in the dataset. We will import a processed
dataset with the time zone information for each state or territory, and
create a function that maps the departure and arrival times to the
corresponding time zone, based on their departure and arrival states. In
case of any ambiguous or non-existent times, we will set them to `NaT`
and drop them, if the number of such values is negligible.


``` python
state_tz_df = pd.read_parquet('data/processed/state_tz.parquet')
state_tz_dict = state_tz_df.set_index('state')['tz'].to_dict()

def time_zone(df, time_col, location_col):
    timezones = df[location_col].map(state_tz_dict)
    times = pd.to_datetime(df[time_col])
    
    unique_timezones = timezones.unique()
    for tz in unique_timezones:
        mask = timezones == tz
        df.loc[mask, time_col] = times[mask].dt.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
    
    if df[time_col].isna().mean() < 0.01:
        df = df.dropna(subset=[time_col])
    else:
        print(f"Check {time_col} for NaT")
    
    return df

for time_col in time_cols[:2]:
    df = time_zone(df, time_col, 'OriginState')
for time_col in time_cols[2:]:
    df = time_zone(df, time_col, 'DestState')

display(df[['OriginState'] + time_cols[:2] + ['DestState'] + time_cols[2:]].head())
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">OriginState</th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">DestState</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>MN</td>
<td>1998-01-02 16:40:00-06:00</td>
<td>1998-01-02 16:59:00-06:00</td>
<td>UT</td>
<td>1998-01-02 18:36:00-07:00</td>
<td>1998-01-02 18:59:00-07:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>WI</td>
<td>2009-05-28 12:04:00-05:00</td>
<td>2009-05-28 12:02:00-05:00</td>
<td>FL</td>
<td>2009-05-28 15:41:00-05:00</td>
<td>2009-05-28 15:41:00-05:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>CO</td>
<td>2013-06-29 16:30:00-06:00</td>
<td>2013-06-29 16:44:00-06:00</td>
<td>TX</td>
<td>2013-06-29 19:45:00-06:00</td>
<td>2013-06-29 19:42:00-06:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>CA</td>
<td>2010-08-31 13:05:00-07:00</td>
<td>2010-08-31 13:05:00-07:00</td>
<td>MI</td>
<td>2010-08-31 20:35:00-05:00</td>
<td>2010-08-31 20:15:00-05:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>NJ</td>
<td>2006-01-15 18:20:00-05:00</td>
<td>2006-01-15 19:11:00-05:00</td>
<td>NC</td>
<td>2006-01-15 20:26:00-05:00</td>
<td>2006-01-15 20:58:00-05:00</td>
</tr>
</tbody>
</table>

</div>

Now that all times have their corresponding time zones, we can
accurately calculate the dates for all time variables. We assume that
the `FlightDate` only applies to the scheduled flight departure.
Consequently, all other time variables will have to be calculated based
on the `FlightDate` and the time difference between the `CRSDepTime` and
the rest of the time variables. To achieve this, we will write a
function that takes two columns: the time variable and its corresponding
state variable (whether departure or arrival state). The function will
convert the time variable to UTC and calculate the time difference
between the variable and the `CRSDepTime`. Then, it will add this
difference to the original time variable, which will result in an
accurate date in UTC. Finally, the function will search the time zone of
the corresponding state and convert the datetime object back to its
original time zone.

``` python
def date_calculation(df, time_col, location_col, deptime=False):
    timezones = df[location_col].map(state_tz_dict)
    times = df[time_col]
    
    utc_crs_dep = pd.to_datetime(df['CRSDepTime'], utc=True)
    utc_times = pd.to_datetime(times, utc=True)

    diff = (utc_times - utc_crs_dep).dt.seconds
    if deptime:
        diff -= (utc_crs_dep.dt.time > utc_times.dt.time) * 86400
    new_times = utc_crs_dep + pd.to_timedelta(diff, 's')
    
    unique_timezones = timezones.unique()
    for tz in unique_timezones:
        mask = timezones == tz
        df.loc[mask, time_col] = new_times[mask].dt.tz_convert(tz)
    
    return df

df = date_calculation(df, 'CRSArrTime', 'DestState')
df = date_calculation(df, 'ArrTime', 'DestState')
df = date_calculation(df, 'DepTime', 'OriginState', True)
```

To make sure that the date calculations are accurate, we will check the
dataset for arrival variables that are earlier than their corresponding
departure variable.


``` python
negative_crsarr = df.query("CRSArrTime < CRSDepTime")
negative_arr = df.query("ArrTime < DepTime")

print(f"There are {negative_crsarr.shape[0]} negative CRSArrTime values: {negative_crsarr.shape[0]/df.shape[0]:.2%}")
display(negative_crsarr[['OriginState'] + time_cols[:2] + ['DestState'] + time_cols[2:]])

print(f"There are {negative_arr.shape[0]} negative ArrTime values: {negative_arr.shape[0]/df.shape[0]:.2%}")
display(negative_arr[['OriginState'] + time_cols[:2] + ['DestState'] + time_cols[2:]])
```


    There are 0 negative CRSArrTime values: 0.00%
    There are 10307 negative ArrTime values: 0.53%

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">OriginState</th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">DestState</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
</tr>
</thead>
<tbody>
</tbody>
</table>

</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">OriginState</th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">DestState</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">259</td>
<td>NJ</td>
<td>2006-09-10 20:05:00-04:00</td>
<td>2006-09-11 19:55:00-04:00</td>
<td>GA</td>
<td>2006-09-10 22:29:00-04:00</td>
<td>2006-09-10 22:27:00-04:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">362</td>
<td>MI</td>
<td>2004-07-14 21:05:00-05:00</td>
<td>2004-07-14 22:00:00-05:00</td>
<td>NY</td>
<td>2004-07-14 22:05:00-04:00</td>
<td>2004-07-14 22:58:00-04:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">370</td>
<td>FL</td>
<td>2010-06-23 20:24:00-05:00</td>
<td>2010-06-23 20:28:00-05:00</td>
<td>GA</td>
<td>2010-06-23 21:34:00-04:00</td>
<td>2010-06-23 21:26:00-04:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">440</td>
<td>CA</td>
<td>2002-10-28 16:00:00-08:00</td>
<td>2002-10-29 15:56:00-08:00</td>
<td>NV</td>
<td>2002-10-28 17:24:00-08:00</td>
<td>2002-10-28 17:09:00-08:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">456</td>
<td>IL</td>
<td>1996-10-18 19:00:00-05:00</td>
<td>1996-10-19 18:57:00-05:00</td>
<td>NY</td>
<td>1996-10-18 22:00:00-04:00</td>
<td>1996-10-18 22:02:00-04:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1958241</td>
<td>CA</td>
<td>2006-07-29 17:00:00-07:00</td>
<td>2006-07-30 16:59:00-07:00</td>
<td>CA</td>
<td>2006-07-29 17:48:00-07:00</td>
<td>2006-07-29 17:53:00-07:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1958542</td>
<td>TX</td>
<td>2006-06-27 18:05:00-06:00</td>
<td>2006-06-28 17:59:00-06:00</td>
<td>TX</td>
<td>2006-06-27 19:29:00-06:00</td>
<td>2006-06-27 19:44:00-06:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1958593</td>
<td>TN</td>
<td>2012-07-18 09:45:00-05:00</td>
<td>2012-07-18 10:22:00-05:00</td>
<td>GA</td>
<td>2012-07-19 10:38:00-04:00</td>
<td>2012-07-18 11:10:00-04:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1958920</td>
<td>VA</td>
<td>2014-02-23 19:00:00-05:00</td>
<td>2014-02-24 18:55:00-05:00</td>
<td>GA</td>
<td>2014-02-23 20:59:00-05:00</td>
<td>2014-02-23 20:55:00-05:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1958943</td>
<td>NV</td>
<td>2008-03-23 14:40:00-07:00</td>
<td>2008-03-23 14:44:00-07:00</td>
<td>AZ</td>
<td>2008-03-23 15:50:00-06:00</td>
<td>2008-03-23 15:43:00-06:00</td>
</tr>
</tbody>
</table>

<p>10307 rows × 6 columns</p>

</div>

![](https://www.nationsonline.org/maps/US-timezones-map.jpg)

There seem to be a few flights with arrival times earlier than their
departure times, even in UTC time. For some cases, it is likely due to
the fact that some US states have more than one time zone (as shown in
the image below), while our timezone dictionary only accounts for one
timezone per state. These are the cases for states such as Michigan,
Winsconsin, North and South Dakota, Indiana, Texas, Teneessee, Kentucky,
among others, which can be seen in the sample data above. For other
cases, it is likely due to the fact that some flights span across
midnight in UTC, and our function does not account for this. This can be
seen in the sample data above, where `DepTime` has an additional day
compared to `CRSDepTime`.

Since the number of such values is negligible, we will drop these rows
from the dataset.


``` python
print(f"Before drop of negative arrivals: {df.shape}")
df = df.drop(negative_arr.index)
print(f"After drop of negative arrivals: {df.shape}")
```


    Before drop of negative arrivals: (1958816, 43)
    After drop of negative arrivals: (1948509, 43)

### Feature Engineering: Binary Delay Variable

We will now create binary delay variables for both departure and arrival
delays. This will allow us to analyze the distribution of the delays in
the dataset and possibly be useful later on for our prediction model. We
will add two new binary columns to our DataFrame, `DepDelaryBinary` and
`ArrDelaryBinary`, which indicate whether the departure and arrival
delays, respectively, are greater than the specified tolerance level (10
minutes in this case). We then update our column lists and display the
head of the DataFrame for these selected columns.


``` python
tolerance = 15
df.insert(25, "DepDelayBinary", (df['DepDelayMinutes'] > tolerance).astype('int'))
df.insert(31, "ArrDelayBinary", (df['ArrDelayMinutes'] > tolerance).astype('int'))

dep_cols.insert(4, "DepDelayBinary")
arr_cols.insert(4, "ArrDelayBinary")

df[dep_cols + arr_cols].head()
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">DepDelay</th>
<th data-quarto-table-cell-role="th">DepDelayMinutes</th>
<th data-quarto-table-cell-role="th">DepDelayBinary</th>
<th data-quarto-table-cell-role="th">DepDel15</th>
<th data-quarto-table-cell-role="th">DepartureDelayGroups</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
<th data-quarto-table-cell-role="th">ArrDelay</th>
<th data-quarto-table-cell-role="th">ArrDelayMinutes</th>
<th data-quarto-table-cell-role="th">ArrDelayBinary</th>
<th data-quarto-table-cell-role="th">ArrDel15</th>
<th data-quarto-table-cell-role="th">ArrivalDelayGroups</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998-01-02 16:40:00-06:00</td>
<td>1998-01-02 16:59:00-06:00</td>
<td>19.000</td>
<td>19.000</td>
<td>1</td>
<td>1.000</td>
<td>1.000</td>
<td>1998-01-02 18:36:00-07:00</td>
<td>1998-01-02 18:59:00-07:00</td>
<td>23.000</td>
<td>23.000</td>
<td>1</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>2009-05-28 12:04:00-05:00</td>
<td>2009-05-28 12:02:00-05:00</td>
<td>-2.000</td>
<td>0.000</td>
<td>0</td>
<td>0.000</td>
<td>-1.000</td>
<td>2009-05-28 15:41:00-05:00</td>
<td>2009-05-28 15:41:00-05:00</td>
<td>0.000</td>
<td>0.000</td>
<td>0</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2013-06-29 16:30:00-06:00</td>
<td>2013-06-29 16:44:00-06:00</td>
<td>14.000</td>
<td>14.000</td>
<td>0</td>
<td>0.000</td>
<td>0.000</td>
<td>2013-06-29 19:45:00-06:00</td>
<td>2013-06-29 19:42:00-06:00</td>
<td>-3.000</td>
<td>0.000</td>
<td>0</td>
<td>0.000</td>
<td>-1.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>2010-08-31 13:05:00-07:00</td>
<td>2010-08-31 13:05:00-07:00</td>
<td>0.000</td>
<td>0.000</td>
<td>0</td>
<td>0.000</td>
<td>0.000</td>
<td>2010-08-31 20:35:00-05:00</td>
<td>2010-08-31 20:15:00-05:00</td>
<td>-20.000</td>
<td>0.000</td>
<td>0</td>
<td>0.000</td>
<td>-2.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2006-01-15 18:20:00-05:00</td>
<td>2006-01-14 19:11:00-05:00</td>
<td>51.000</td>
<td>51.000</td>
<td>1</td>
<td>1.000</td>
<td>3.000</td>
<td>2006-01-15 20:26:00-05:00</td>
<td>2006-01-15 20:58:00-05:00</td>
<td>32.000</td>
<td>32.000</td>
<td>1</td>
<td>1.000</td>
<td>2.000</td>
</tr>
</tbody>
</table>

</div>

### Data Cleaning: Changing Data Types

A minor detail we mentioned above is that the numerical variables
measuring duration and boolean variables would benefit from being saved
as integers. We will thus convert the delay variables to integers and
boolean variables.


``` python
df[dep_cols[2:] + arr_cols[2:]] = df[dep_cols[2:] + arr_cols[2:]].astype('int')
df[dep_cols[2:] + arr_cols[2:]].sample(5)
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">DepDelay</th>
<th data-quarto-table-cell-role="th">DepDelayMinutes</th>
<th data-quarto-table-cell-role="th">DepDelayBinary</th>
<th data-quarto-table-cell-role="th">DepDel15</th>
<th data-quarto-table-cell-role="th">DepartureDelayGroups</th>
<th data-quarto-table-cell-role="th">ArrDelay</th>
<th data-quarto-table-cell-role="th">ArrDelayMinutes</th>
<th data-quarto-table-cell-role="th">ArrDelayBinary</th>
<th data-quarto-table-cell-role="th">ArrDel15</th>
<th data-quarto-table-cell-role="th">ArrivalDelayGroups</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">663927</td>
<td>6</td>
<td>6</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>23</td>
<td>23</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">301250</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">573000</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1922332</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>-7</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>-1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">335044</td>
<td>74</td>
<td>74</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>64</td>
<td>64</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
</tbody>
</table>

</div>

## Departure and Arrival Variables (updated)

So far, we have explore missing value correlations among departure and
arrival variables, dropped unnecessary variables, converted the time
variables to `datetime` format, filled missing CRS values, and created
binary delay variables. As mentioned at the start of this section,
considering the need for data cleaning and the varying nature of missing
values, we can now analyze the distribution of the departure and arrival
variables in the dataset.

We will divide our analysis into two parts: the time variables and the
delay variables. Given our data has been cleaned, we can finally focus
on the statistical analysis of these variables.

### Time Variables

Since our time data is now clean and in a valid format, we will start by
analyzing the distribution of the time variables in the dataset. We will
use the `time_hist` function to create a histogram for the time
variables.


``` python
for col, color in zip(time_cols, colors):
    times = df[col].apply(lambda x : x.time())
    times = [t.hour * 60 + t.minute for t in times]
    plt.hist(times, alpha=0.5, bins=75, color=color)
    plt.axvline(np.mean(times), color=color, linestyle='dashed', linewidth=2)
plt.legend(time_cols)
plt.title('Distribution of CRS and True Depature and Arrivals')
plt.xticks(np.arange(0, 1441, 60), [f'{h}:00' for h in range(25)])
plt.show()
```


![](/assets/projects/04-univariate_analysis_2_files/figure-html/cell-16-output-1.png)

The histogram now shows a clear picture on the distribution of time
variables. At first glance, the plot shows departures are more prevalent
during morning hours, while arrivals are more predominant during the
evening, which is expected. This is further demonstrated by the 2-hour
difference in the depature and arrival means, which shows that
departures tend to occur earlier on average than arrivals.

Particularly noticeable is the progression of flight frequencies across
the day. The data shows that early morning flights, particularly at 3am,
are the least frequent for both depatures and arrivals. However, there
is a significant increase in the number of flights at 5am for departures
and 7am for arrivals. There is a noticeable of departure flights at 7am,
with over 55000 flights registered across the dataset. Afterwards,
overall peak hours, where *both* arrivals and departures are at their
highest, remain relatively consistent from 9pm. The number of departures
decreases first at 6pm, whereas the number of arrivals starts decreasing
from 9pm. This decrease continues past midnight, reaching its lowest
point at around 3am.

Moreover, the distribution of CRS and true time variable pairs for
departures and arrivals, respectively, are almost identical, as shown by
the close pairs of means. This already reveals that the size of flight
delays is small on average across the dataset.

Given this insight, let us now focus on the distribution of delay
variables in particular.

### Delay Variables


``` python
univariate_preview(df, dep_cols[2:] + arr_cols[2:])
```


    'Data Preview'

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">DepDelay</th>
<th data-quarto-table-cell-role="th">DepDelayMinutes</th>
<th data-quarto-table-cell-role="th">DepDelayBinary</th>
<th data-quarto-table-cell-role="th">DepDel15</th>
<th data-quarto-table-cell-role="th">DepartureDelayGroups</th>
<th data-quarto-table-cell-role="th">ArrDelay</th>
<th data-quarto-table-cell-role="th">ArrDelayMinutes</th>
<th data-quarto-table-cell-role="th">ArrDelayBinary</th>
<th data-quarto-table-cell-role="th">ArrDel15</th>
<th data-quarto-table-cell-role="th">ArrivalDelayGroups</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>19</td>
<td>19</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>23</td>
<td>23</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>-2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>-1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>14</td>
<td>14</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>-3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>-1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>-20</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>-2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>51</td>
<td>51</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>32</td>
<td>32</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
</tbody>
</table>

</div>

    'Value Counts'

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">columns</th>
<th data-quarto-table-cell-role="th">dtypes</th>
<th data-quarto-table-cell-role="th">nunique</th>
<th data-quarto-table-cell-role="th">top5</th>
<th data-quarto-table-cell-role="th">na%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">5</td>
<td>ArrDelay</td>
<td>int64</td>
<td>937</td>
<td>[0, -5, -3, -2, -4]</td>
<td>0.00%</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>DepDelay</td>
<td>int64</td>
<td>911</td>
<td>[0, -2, -1, -3, -4]</td>
<td>0.00%</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1</td>
<td>DepDelayMinutes</td>
<td>int64</td>
<td>855</td>
<td>[0, 1, 2, 3, 5]</td>
<td>0.00%</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">6</td>
<td>ArrDelayMinutes</td>
<td>int64</td>
<td>850</td>
<td>[0, 2, 1, 3, 5]</td>
<td>0.00%</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>DepartureDelayGroups</td>
<td>int64</td>
<td>15</td>
<td>[-1, 0, 1, 2, 3]</td>
<td>0.00%</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>ArrivalDelayGroups</td>
<td>int64</td>
<td>15</td>
<td>[-1, 0, -2, 1, 2]</td>
<td>0.00%</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>DepDelayBinary</td>
<td>int64</td>
<td>2</td>
<td>[0, 1]</td>
<td>0.00%</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>DepDel15</td>
<td>int64</td>
<td>2</td>
<td>[0, 1]</td>
<td>0.00%</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">7</td>
<td>ArrDelayBinary</td>
<td>int64</td>
<td>2</td>
<td>[0, 1]</td>
<td>0.00%</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">8</td>
<td>ArrDel15</td>
<td>int64</td>
<td>2</td>
<td>[0, 1]</td>
<td>0.00%</td>
</tr>
</tbody>
</table>

</div>

    'Summary Stats'

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">DepDelay</th>
<th data-quarto-table-cell-role="th">DepDelayMinutes</th>
<th data-quarto-table-cell-role="th">DepDelayBinary</th>
<th data-quarto-table-cell-role="th">DepDel15</th>
<th data-quarto-table-cell-role="th">DepartureDelayGroups</th>
<th data-quarto-table-cell-role="th">ArrDelay</th>
<th data-quarto-table-cell-role="th">ArrDelayMinutes</th>
<th data-quarto-table-cell-role="th">ArrDelayBinary</th>
<th data-quarto-table-cell-role="th">ArrDel15</th>
<th data-quarto-table-cell-role="th">ArrivalDelayGroups</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>8.495</td>
<td>10.400</td>
<td>0.160</td>
<td>0.169</td>
<td>0.062</td>
<td>6.197</td>
<td>11.785</td>
<td>0.189</td>
<td>0.198</td>
<td>-0.074</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>32.249</td>
<td>31.485</td>
<td>0.367</td>
<td>0.374</td>
<td>1.816</td>
<td>34.810</td>
<td>31.948</td>
<td>0.391</td>
<td>0.398</td>
<td>1.995</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>-990.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-2.000</td>
<td>-706.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-2.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>-3.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
<td>-10.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>-1.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>6.000</td>
<td>6.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
<td>10.000</td>
<td>10.000</td>
<td>0.000</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>1,878.000</td>
<td>1,878.000</td>
<td>1.000</td>
<td>1.000</td>
<td>12.000</td>
<td>1,898.000</td>
<td>1,898.000</td>
<td>1.000</td>
<td>1.000</td>
<td>12.000</td>
</tr>
</tbody>
</table>

</div>

The value counts above show several insights about the delay variables.
`DepDelay` and `ArrDelay` have the highest number of unique values,
since they are continuous variables that include negative values
(representing flights ahead of schedule). Interestingly, their top 5
values counts show that on-time and ahead of scheduled flights (by less
than 5 minutes) are the most frequent. The delay minutes variables have
a lower value count, since they only include positive values. According
to the value counts, delays of less than 5 minutes are the most common
for both departures and arrivals. The delay groups variables give a
deeper glimpse into the amount of delays, as they count the number of
15-minute intervals for flight delays. The most frequent delay groups
range from -15 to 30 minutes. The binary delay variables further confirm
that the majority of flights are on time.

The summary statistics on the other hand show some concerning facts
about the `DepDelay` and `ArrDelay` variables in particular. According
to the summary statistics, the minimum delay is -990 minutes (or -16.5
hours) for departures and -706 minutes (or -11.8 hours) for arrivals.
Conversely, the maximum delay for these, as well as for the delay
minutes variables, is 1878 minutes or (31.3 hours) for departures and
1898 minutes (or 31.64 hours) for arrivals. This is highly concerning,
as it is unlikely for flights to be delayed by such a large amount of
time. This clearly indicates the presence of outliers in the dataset.

Before exploring this issue, let us first visualize the distribution of
the categorical variables in this group to better understand what is
really happening with flight delays in the dataset. We will start by
creating histograms for the delay variables.


``` python
variables = dep_cols[4:] + arr_cols[4:]
ncols = 3
nvars = len(variables)
fig, axes = plt.subplots(nrows=int(nvars/ncols), ncols=ncols, figsize=(20, 10))

for col, ax in zip(dep_cols[4:] + arr_cols[4:], axes.flatten()):
    if df[col].nunique() > 2:
        ax.hist(df[col], bins=30, edgecolor='black')
        ax.set_title(col)
    else:
        sns.countplot(x=col, data=df, ax=ax, edgecolor='black')
        ax.set_xlabel(None)
        ax.set_title(col)
    plt.tight_layout()
plt.show()
```


![](/assets/projects/04-univariate_analysis_2_files/figure-html/cell-18-output-1.png)

The count plots for categorical delay variables give a better
understanding as to why the delay variables have such large outliers.
The delay binary variables show that around 80% of flights are on time,
with only around 20% exhibiting some kind of delay. Morevoer, the
15-minute delay variables have a similar distribution, indicating that
the majority of delayed flights are delayed by 15 minutes or more. This
amount is considered as the threshold for a delayed flight by the
airline industry.

The histograms for 15-minute delay groups offer a more detailed view of
the distribution of delays, as shown with normalized counts below.
Firstly, the distribution is right skewed, with more than 40% of flights
ahead of schedule by less than 15 minutes and around 30% of flights on
time. These two groups together represent around 70% of flights in the
dataset. This is consistent with the value counts above, which show that
the majority of flights exhibit no delays greater than 15 minutes
(represented in these histograms by values less than or equal to 0).

Beyond these values, we can better observe the outliers in the dataset.
The distribution has a long, right tail, with an increasingly smaller
number of flights experiencing delays greater than 15 minutes. A
uniformly low count of flights can be seen for delays greater than three
15-minute intervals (or 45 minutes). These are the flights that are
causing the large outliers in the delay variables. Finally, it is worth
noting that the `ArrivalDelayGroups` variable even shows a significant
amount of flights (around 13%) arriving ahead of schedule by more than
15 minutes.


``` python
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))

sns.histplot(df['DepartureDelayGroups'], bins=30, ax=axes[0], stat="percent", edgecolor='black')
sns.histplot(df['ArrivalDelayGroups'], bins=30, ax=axes[1], stat="percent", edgecolor='black')

plt.tight_layout()
plt.show()
```


![](/assets/projects/04-univariate_analysis_2_files/figure-html/cell-19-output-1.png)

We will leave the outlier exploration for these variables, as well as
any other, for our next notebook covering the bivariate analysis of this
dataset. For now, we will move on to the last sections, to analyze the
distribution of the remaining variables in the dataset.

## Flight Summary Variables

We will now analyze the distribution of the flight summary variables,
which include `FlightDate`, `UniqueCarrier`, `FlightNum`, `TailNum`,
`Distance`, `DistanceGroup`, and `DivAirportLandings`. We will start by
analyzing the distribution of these variables and determine any
potential issues that may require further processing.

### Value Counts


``` python
univariate_preview(df, sum_cols)
```


    'Data Preview'

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">CRSElapsedTime</th>
<th data-quarto-table-cell-role="th">ActualElapsedTime</th>
<th data-quarto-table-cell-role="th">AirTime</th>
<th data-quarto-table-cell-role="th">Distance</th>
<th data-quarto-table-cell-role="th">DistanceGroup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>176.000</td>
<td>180.000</td>
<td>153.000</td>
<td>991.000</td>
<td>4</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>157.000</td>
<td>159.000</td>
<td>141.000</td>
<td>1,066.000</td>
<td>5</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>135.000</td>
<td>118.000</td>
<td>103.000</td>
<td>773.000</td>
<td>4</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>270.000</td>
<td>250.000</td>
<td>220.000</td>
<td>1,979.000</td>
<td>8</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>126.000</td>
<td>107.000</td>
<td>80.000</td>
<td>529.000</td>
<td>3</td>
</tr>
</tbody>
</table>

</div>

    'Value Counts'

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">columns</th>
<th data-quarto-table-cell-role="th">dtypes</th>
<th data-quarto-table-cell-role="th">nunique</th>
<th data-quarto-table-cell-role="th">top5</th>
<th data-quarto-table-cell-role="th">na%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">3</td>
<td>Distance</td>
<td>float64</td>
<td>1902</td>
<td>[337.0, 370.0, 236.0, 328.0, 224.0]</td>
<td>0.00%</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2</td>
<td>AirTime</td>
<td>float64</td>
<td>666</td>
<td>[50.0, 45.0, 55.0, 60.0, 53.0]</td>
<td>19.32%</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1</td>
<td>ActualElapsedTime</td>
<td>float64</td>
<td>664</td>
<td>[70.0, 65.0, 75.0, 60.0, 80.0]</td>
<td>0.00%</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>CRSElapsedTime</td>
<td>float64</td>
<td>630</td>
<td>[70.0, 65.0, 75.0, 80.0, 60.0]</td>
<td>0.00%</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>DistanceGroup</td>
<td>int64</td>
<td>11</td>
<td>[2, 3, 1, 4, 5]</td>
<td>0.00%</td>
</tr>
</tbody>
</table>

</div>

    'Summary Stats'

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">CRSElapsedTime</th>
<th data-quarto-table-cell-role="th">ActualElapsedTime</th>
<th data-quarto-table-cell-role="th">AirTime</th>
<th data-quarto-table-cell-role="th">Distance</th>
<th data-quarto-table-cell-role="th">DistanceGroup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
<td>1,572,116.000</td>
<td>1,948,509.000</td>
<td>1,948,509.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>127.445</td>
<td>125.167</td>
<td>106.121</td>
<td>736.464</td>
<td>3.421</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>70.524</td>
<td>70.390</td>
<td>68.602</td>
<td>569.531</td>
<td>2.246</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>0.000</td>
<td>3.000</td>
<td>-703.000</td>
<td>11.000</td>
<td>1.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>75.000</td>
<td>74.000</td>
<td>56.000</td>
<td>325.000</td>
<td>2.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>109.000</td>
<td>107.000</td>
<td>87.000</td>
<td>583.000</td>
<td>3.000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>159.000</td>
<td>156.000</td>
<td>136.000</td>
<td>972.000</td>
<td>4.000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>705.000</td>
<td>975.000</td>
<td>965.000</td>
<td>5,095.000</td>
<td>11.000</td>
</tr>
</tbody>
</table>

</div>

The value counts shows us that most variables except for `DistanceGroup`
are numerical variables, thus exhibiting a high number of unique values.
The values are mostly saved as floats, which, as with the departure and
arrival variables in previous sections, might be best to save as
integers. Regarding top values, we see that the most frequent values for
the `Distance` column range from the low 200s to the high 300s,
`AirTime` values cluster around 50 minutes, and `CRSElapsedTime` and
`ActualElapsedTime` at 60 minutes. The `DistanceGroup` variable is a
categorical variable that counts the flight distance in 250-mile
intervals. All top values are less than 5, indicating that top flights
are less than 1250 miles long.

The summary statistics show a more detailed understanding of the
numerical distribution of the variables, which we visualize below.


``` python
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 10))
(ax1, ax2, ax3, ax4, ax5, ax6) = axes.flatten()

sns.histplot(df['Distance'], bins=30, ax=ax1, kde=True, edgecolor='black')
sns.countplot(x='DistanceGroup', data=df, ax=ax2, edgecolor='black', stat='percent')
sns.histplot(df['CRSElapsedTime'], bins=30, ax=ax3, kde=True, edgecolor='black')
sns.histplot(df['ActualElapsedTime'], bins=30, ax=ax4, kde=True, edgecolor='black')
sns.histplot(df['AirTime'], bins=30, ax=ax5, kde=True, edgecolor='black')

plt.tight_layout()
plt.show()
```


![](/assets/projects/04-univariate_analysis_2_files/figure-html/cell-21-output-1.png)

Starting with the `Distance` variable, the histogram shows the
distribution is right skewed, with the majority of flights being less
than 1000 miles long. The normalized count plot for `DistanceGroup`,
which groups flights into distance groups 250 Miles, shows a similar
distribution. According to the summary statistics for these variables,
the mean distance for flights in the dataset is 735 miles, with a
standard deviation of 569 miles, and 75% of flights being less than 969
miles long or within the first 4 distance group. Fewer flights are
longer, which may be the case for flights to Hawaii, Alaska, as well as
to U.S. territories and possessions and cross-country flights.

The summary statistics and histograms for `CRSElapsedTime`,
`ActualElapsedTime`, and `AirTime` show a different story. The
distribution for these variables is also right skewed, with 75% of
flights lasting under 160 minutes or 2 hours and 40 minutes from
departure to arrival. However, the `ActualElapsedTime` and, in
particular, `AirTime` variables exhibit negative values, which is not
possible for flight times. Two possible explanations for this is that
time zone differences, as well as flights spanning across midnight,
might have caused the arrival time to be earlier than the departure
time. This will require further exploration to confirm, but, if true,
would simple require time zone adjustments and simple matrix operations
to fix. Finally, the `AirTime` variable has a large amount of missing
values, which will require further processing.

We will explore these issues in the following sections, starting with
the question of outlying values in the distance variables. Then we will
move on to the issue of negative values in the time variables. But
first, we will convert the numerical variables to integers.

### Data Cleaning: Changing Data Types

We will convert the numerical variables to integers, as they are all
counts or durations, and thus do not require decimal points. To allow
while keeping the missing values, we will convert the variables to
`Int64` type and ignore missing values using the `errors='coerce'`
argument from `pd.to_numeric`.


``` python
df[sum_cols[:-1]] = df[sum_cols[:-1]].apply(pd.to_numeric, errors='coerce').astype('Int64')
display(df[sum_cols].dtypes)
```


    CRSElapsedTime       Int64
    ActualElapsedTime    Int64
    AirTime              Int64
    Distance             Int64
    DistanceGroup        int64
    dtype: object

### Exploring Distance Outliers

We will start by exploring flights with outlying distances. To achieve
this, we will filter the dataset for flights with distances greater than
the 3rd quartile and preview the unique `Origin` and `Dest` value
combinations for these flights.


``` python
long_flights = df.query("Distance > 969").sort_values(by='Distance', ascending=False)
print(f"Number of flights greater than 3Q (969): {long_flights.shape[0]} or {long_flights.shape[0]/df.shape[0]:.2%}")

display(df.groupby(['Origin', 'Dest'])['Distance'].max().reset_index().sort_values(by='Distance', ascending=False).head(10))
```


    Number of flights greater than 3Q (969): 488141 or 25.05%

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Origin</th>
<th data-quarto-table-cell-role="th">Dest</th>
<th data-quarto-table-cell-role="th">Distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">3415</td>
<td>HNL</td>
<td>BOS</td>
<td>5095</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">934</td>
<td>BOS</td>
<td>HNL</td>
<td>5095</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3426</td>
<td>HNL</td>
<td>JFK</td>
<td>4983</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">4144</td>
<td>JFK</td>
<td>HNL</td>
<td>4983</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2854</td>
<td>EWR</td>
<td>HNL</td>
<td>4963</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3421</td>
<td>HNL</td>
<td>EWR</td>
<td>4963</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3423</td>
<td>HNL</td>
<td>IAD</td>
<td>4817</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3620</td>
<td>IAD</td>
<td>HNL</td>
<td>4817</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3416</td>
<td>HNL</td>
<td>CLT</td>
<td>4678</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1528</td>
<td>CLT</td>
<td>HNL</td>
<td>4678</td>
</tr>
</tbody>
</table>

</div>

As suspected, long distances are mostly flights to or from Hawaii. The
aggregation above shows that the top 10 longest flights are all to or
from Hawaii, with the longest flight being between Honolulu (HNL) and
Boston (BOS). According to the histograms from the previous section,
these flights are not only outliers, but are also among the least
frequent in the dataset.

Below we show which are the top 10 most frequent flights not involving
Hawaii.


``` python
long_flights_not_hawaii = long_flights.query("~OriginStateName.str.contains('Hawaii') & ~DestStateName.str.contains('Hawaii')")
display(long_flights_not_hawaii.groupby(['Origin', 'Dest'])['Distance'].max().drop_duplicates().reset_index().sort_values(by='Distance', ascending=False).head(10))
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Origin</th>
<th data-quarto-table-cell-role="th">Dest</th>
<th data-quarto-table-cell-role="th">Distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">33</td>
<td>ANC</td>
<td>ATL</td>
<td>3417</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">638</td>
<td>LAX</td>
<td>SJU</td>
<td>3386</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">756</td>
<td>PHL</td>
<td>ANC</td>
<td>3379</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">38</td>
<td>ANC</td>
<td>EWR</td>
<td>3370</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">39</td>
<td>ANC</td>
<td>IAH</td>
<td>3266</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">331</td>
<td>CVG</td>
<td>ANC</td>
<td>3110</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">36</td>
<td>ANC</td>
<td>DFW</td>
<td>3043</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">37</td>
<td>ANC</td>
<td>DTW</td>
<td>2986</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">50</td>
<td>ANC</td>
<td>STL</td>
<td>2936</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">44</td>
<td>ANC</td>
<td>ORD</td>
<td>2846</td>
</tr>
</tbody>
</table>

</div>

As we can see, the longest flights not involving Hawaii are mostly
flights betwen Alaska (ANC) and east cost cities like Atlanta (ATL),
Philadelphia (PHL), Newark (EWR) and Chicago (IAH). Interestingly, the
second longest flight in this groups does not involve either Hawaii or
Alaska, but a U.S. territory. This is a flight between San Juan (SJU)
and Los Angeles (LAX).

Lastly, we leave below the top 10 longest flights not involving Hawaii
or Alaska, which, as expected, mostly involve transcontinental flights.


``` python
long_flights_not_hawaii_alaska = long_flights.query("~OriginStateName.isin(['Hawaii', 'Alaska']) & ~DestStateName.isin(['Hawaii', 'Alaska'])")
display(long_flights_not_hawaii_alaska.groupby(['Origin', 'Dest'])['Distance'].max().drop_duplicates().reset_index().sort_values(by='Distance', ascending=False).head(10))
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Origin</th>
<th data-quarto-table-cell-role="th">Dest</th>
<th data-quarto-table-cell-role="th">Distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">618</td>
<td>LAX</td>
<td>SJU</td>
<td>3386</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">667</td>
<td>MIA</td>
<td>SEA</td>
<td>2724</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">481</td>
<td>FLL</td>
<td>SEA</td>
<td>2717</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">199</td>
<td>BOS</td>
<td>SFO</td>
<td>2704</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">191</td>
<td>BOS</td>
<td>OAK</td>
<td>2694</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">200</td>
<td>BOS</td>
<td>SJC</td>
<td>2689</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">203</td>
<td>BOS</td>
<td>SMF</td>
<td>2636</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">138</td>
<td>BDL</td>
<td>SFO</td>
<td>2625</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">184</td>
<td>BOS</td>
<td>LAX</td>
<td>2611</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">185</td>
<td>BOS</td>
<td>LGB</td>
<td>2602</td>
</tr>
</tbody>
</table>

</div>

### Data Cleaning: Negative Time Values

In this section, we will explore the issue of negative time values in
the `ActualElapsedTime` and `AirTime` variables. We will start by
filtering the dataset for flights with negative `ActualElapsedTime` and
`AirTime` values and preview the departure and arrival times (both CRS
and real) for these flights, along with their origin and destination
states to account for time differences.


``` python
negative_times = df.query("ActualElapsedTime < 0 | AirTime < 0")[['OriginState', 'DestState', 'CRSDepTime', 'CRSArrTime', 'DepTime', 'ArrTime', 'ActualElapsedTime', 'AirTime']]

print(f"Number of negative values in `ActualElapsedTime`: {negative_times.query('ActualElapsedTime < 0').shape[0]}")
print(f"Number of negative values in `AirTime`: {negative_times.query('AirTime < 0').shape[0]}")

display(negative_times.query("ActualElapsedTime < 0"))
display(negative_times.query("AirTime < 0").sample(5))
```


    Number of negative values in `ActualElapsedTime`: 0
    Number of negative values in `AirTime`: 27

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">OriginState</th>
<th data-quarto-table-cell-role="th">DestState</th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
<th data-quarto-table-cell-role="th">ActualElapsedTime</th>
<th data-quarto-table-cell-role="th">AirTime</th>
</tr>
</thead>
<tbody>
</tbody>
</table>

</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">OriginState</th>
<th data-quarto-table-cell-role="th">DestState</th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
<th data-quarto-table-cell-role="th">ActualElapsedTime</th>
<th data-quarto-table-cell-role="th">AirTime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1123316</td>
<td>UT</td>
<td>MT</td>
<td>2004-05-16 22:40:00-06:00</td>
<td>2004-05-17 00:05:00-06:00</td>
<td>2004-05-16 22:35:00-06:00</td>
<td>2004-05-16 23:50:00-06:00</td>
<td>75</td>
<td>-60</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1179139</td>
<td>KY</td>
<td>OK</td>
<td>2004-12-19 09:45:00-05:00</td>
<td>2004-12-19 10:57:00-06:00</td>
<td>2004-12-19 10:15:00-05:00</td>
<td>2004-12-19 11:34:00-06:00</td>
<td>139</td>
<td>-594</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1153657</td>
<td>GA</td>
<td>FL</td>
<td>2004-05-22 09:50:00-04:00</td>
<td>2004-05-22 11:18:00-05:00</td>
<td>2004-05-22 11:05:00-04:00</td>
<td>2004-05-22 12:18:00-05:00</td>
<td>73</td>
<td>-685</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1439783</td>
<td>TN</td>
<td>KY</td>
<td>2004-02-23 10:10:00-06:00</td>
<td>2004-02-23 11:15:00-05:00</td>
<td>2004-02-23 10:10:00-06:00</td>
<td>2004-02-23 11:11:00-05:00</td>
<td>61</td>
<td>-626</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">742574</td>
<td>NY</td>
<td>KY</td>
<td>2004-11-10 16:59:00-05:00</td>
<td>2004-11-10 19:00:00-05:00</td>
<td>2004-11-10 17:10:00-05:00</td>
<td>2004-11-10 19:03:00-05:00</td>
<td>113</td>
<td>-63</td>
</tr>
</tbody>
</table>

</div>

The filtered data shows that the number of negative values in these
columns is extremely small (\<0.01% of the dataset). It is not clear
from the sample data how are the values for `AirTime` calculated, since
even with the deleted taxi variables the `AirTime` values are still not
consistent with the difference between `WheelsOff` and `WheelsOn`. We
decide therefore to drop the `AirTime` variable, as it is not clear how
it is calculated and it is not consistent with the rest of the dataset.

The sample data for `ActualElapsedTime` confirms our hypothesis that the
negative values are due to flights spanning across midnight and time
zone differences. This leads to an even bigger realization: given the
time variables do not contain time zone information, but are set in
local times, both the `CRSElapsedTime` and `ActualElapsedTime` values do
not represent the actual elapsed time for the flights. This is a major
issue, since accurate time measurements for flight duraction might be
very useful variable for feature engineeering and for our model. We will
therefore also drop these features and replace them with an accurate
calculation of the elapsed times, accounting for time zone differences.


``` python
print(f"Columns before drops: {df.shape[1]}")
df.drop(columns='AirTime', inplace=True)
df.drop(columns='CRSElapsedTime', inplace=True)
df.drop(columns='ActualElapsedTime', inplace=True)
sum_cols.remove('AirTime')
sum_cols.remove('CRSElapsedTime')
sum_cols.remove('ActualElapsedTime')
print(f"Columns after drops: {df.shape[1]}")
```


    Columns before drops: 45
    Columns after drops: 42

### Feature Engineering: Elapsed Time Calculation

With time zones imputed for all time variables, we can accurately
calculate the actual elapsed time for flights by converting the time
variables to UTC and calculating the time difference between the
`ArrTime_UTC` and `DepTime_UTC` variables. We will first convert the
time variables to UTC.


``` python
for time_col in time_cols:
    df[time_col + '_UTC'] = pd.to_datetime(df[time_col], utc=True)

display(df[time_cols + ['OriginState', 'DestState', 'CRSDepTime_UTC', 'DepTime_UTC', 'CRSArrTime_UTC', 'ArrTime_UTC']].head())
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">CRSDepTime</th>
<th data-quarto-table-cell-role="th">DepTime</th>
<th data-quarto-table-cell-role="th">CRSArrTime</th>
<th data-quarto-table-cell-role="th">ArrTime</th>
<th data-quarto-table-cell-role="th">OriginState</th>
<th data-quarto-table-cell-role="th">DestState</th>
<th data-quarto-table-cell-role="th">CRSDepTime_UTC</th>
<th data-quarto-table-cell-role="th">DepTime_UTC</th>
<th data-quarto-table-cell-role="th">CRSArrTime_UTC</th>
<th data-quarto-table-cell-role="th">ArrTime_UTC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998-01-02 16:40:00-06:00</td>
<td>1998-01-02 16:59:00-06:00</td>
<td>1998-01-02 18:36:00-07:00</td>
<td>1998-01-02 18:59:00-07:00</td>
<td>MN</td>
<td>UT</td>
<td>1998-01-02 22:40:00+00:00</td>
<td>1998-01-02 22:59:00+00:00</td>
<td>1998-01-03 01:36:00+00:00</td>
<td>1998-01-03 01:59:00+00:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>2009-05-28 12:04:00-05:00</td>
<td>2009-05-28 12:02:00-05:00</td>
<td>2009-05-28 15:41:00-05:00</td>
<td>2009-05-28 15:41:00-05:00</td>
<td>WI</td>
<td>FL</td>
<td>2009-05-28 17:04:00+00:00</td>
<td>2009-05-28 17:02:00+00:00</td>
<td>2009-05-28 20:41:00+00:00</td>
<td>2009-05-28 20:41:00+00:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2013-06-29 16:30:00-06:00</td>
<td>2013-06-29 16:44:00-06:00</td>
<td>2013-06-29 19:45:00-06:00</td>
<td>2013-06-29 19:42:00-06:00</td>
<td>CO</td>
<td>TX</td>
<td>2013-06-29 22:30:00+00:00</td>
<td>2013-06-29 22:44:00+00:00</td>
<td>2013-06-30 01:45:00+00:00</td>
<td>2013-06-30 01:42:00+00:00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>2010-08-31 13:05:00-07:00</td>
<td>2010-08-31 13:05:00-07:00</td>
<td>2010-08-31 20:35:00-05:00</td>
<td>2010-08-31 20:15:00-05:00</td>
<td>CA</td>
<td>MI</td>
<td>2010-08-31 20:05:00+00:00</td>
<td>2010-08-31 20:05:00+00:00</td>
<td>2010-09-01 01:35:00+00:00</td>
<td>2010-09-01 01:15:00+00:00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2006-01-15 18:20:00-05:00</td>
<td>2006-01-14 19:11:00-05:00</td>
<td>2006-01-15 20:26:00-05:00</td>
<td>2006-01-15 20:58:00-05:00</td>
<td>NJ</td>
<td>NC</td>
<td>2006-01-15 23:20:00+00:00</td>
<td>2006-01-15 00:11:00+00:00</td>
<td>2006-01-16 01:26:00+00:00</td>
<td>2006-01-16 01:58:00+00:00</td>
</tr>
</tbody>
</table>

</div>

Now with the time variables in UTC, we can accurately calculate the
actual elapsed time for the flights. We will recreate the
`CRSElapsedTime` and `ActualElapsedTime` with the difference between the
`ArrTime_UTC` and `DepTime_UTC` variables.


``` python
df['CRSElapsedTime'] = ((df['CRSArrTime_UTC'] - df['CRSDepTime_UTC']).dt.total_seconds() / 60).astype('int')
df['ActualElapsedTime'] = ((df['ArrTime_UTC'] - df['DepTime_UTC']).dt.total_seconds() / 60).astype('int')

sum_cols.append('CRSElapsedTime')
sum_cols.append('ActualElapsedTime')

display(df[sum_cols].head())
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Distance</th>
<th data-quarto-table-cell-role="th">DistanceGroup</th>
<th data-quarto-table-cell-role="th">CRSElapsedTime</th>
<th data-quarto-table-cell-role="th">ActualElapsedTime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>991</td>
<td>4</td>
<td>176</td>
<td>180</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1066</td>
<td>5</td>
<td>217</td>
<td>219</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>773</td>
<td>4</td>
<td>195</td>
<td>178</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1979</td>
<td>8</td>
<td>330</td>
<td>310</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>529</td>
<td>3</td>
<td>126</td>
<td>1547</td>
</tr>
</tbody>
</table>

</div>

### Value Counts (updated)

Now that we have cleaned the flight summary variables and recalculated
our elapsed time variables, we can now analyze the distribution of these
variables. Below we plot the elapsed time variables.


``` python
fig, axs = plt.subplots(2, 2, gridspec_kw={"height_ratios": (.2, .8)}, figsize=figsize)
axs = axs.ravel()

for i, col in enumerate(['CRSElapsedTime', 'ActualElapsedTime']):
    sns.boxplot(x=df[col], ax=axs[i]).set_xlabel('')
    sns.histplot(df[col], ax=axs[i+2])

x_range = df[['CRSElapsedTime', 'ActualElapsedTime']].values.flatten()
x_min, x_max = x_range.min(), x_range.max()
for ax in axs:
    ax.set_xlim(x_min - 100, x_max + 100)

plt.tight_layout()
plt.show()
```


![](/assets/projects/04-univariate_analysis_2_files/figure-html/cell-30-output-1.png)

The distributions for the `CRSElapsedTime` and `ActualElapsedTime`
variables are right skewed, with the majority of flights lasting less
than

## Conclusion

### Takeaways

### Exporting Dataset and Variables

``` python
df.to_parquet('./data/interim/04-airline_2m.parquet', index=False)
```

``` python
variables = {name: value for name, value in locals().items() if name.endswith('_cols')}

with open('./src/modules/variables_04.py', 'w') as f:
    for name, value in variables.items():
        f.write(f"{name} = {value}\n")
```


