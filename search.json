[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "NLP\n\n\n\n\n\nBERT, Encoders and Linear Models for Resume Text Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis project evaluates the performance of advanced NLP models and vectorization techniques for text classifcation using a resume dataset. Implementing Linear SVC, FNN, Encoder models, and BERT, the project achieved an accuracy of 91.67% with BERT. The project demonstrates how to build efficient preprocessing pipelines, optimize feature representation to enhance resource usage, and develop high-performing text classification models using Scikit-Learn and PyTorch.\n\n\n\n\n\nSentiment-Optimized Stock Price Forecasting Using Modern RNNs\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis project focuses on optimizing the predictive capabilities of modern RNN models for stock price movements of TSLA, AAPL, and GOOG. The goal is to enhance forecasting accuracy by utilizing historical stock data and news sentiment data. The analysis evaluates the performance of LSTM, GRU, and Attention-CNN-LSTM models, tested with and without sentiment data, to determine their effectiveness in stock price prediction.\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\nAirline On-Time Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe project explores on-time performance trends from 34 years of US domestic flight data, focusing on variations across carriers, routes, airports, and time. The exploratory data analysis (EDA) resulted in a comprehensive report with 35+ data insights and 25+ visualizations, converted into an interactive Streamlit dashboard. The analysis demonstrates how to extract critical performance trends from historical data, enabling stakeholders to make informed decisions and significantly boost operational efficiency in the aviation industry.\n\n\n\n\n\nBiodiversity, Endangerement and Conversation in Data from National Parks Service\n\n\n\n\n\n\n\n\n\n\n\n\nEmbark on a captivating exploration of biodiversity with this data science project, delving into the conservation statuses of endangered species across national parks. Through meticulous analysis, uncover profound insights into the distribution of endangered species, their likelihood of endangerment, and the most frequently spotted species in each park, illuminating the intricate dynamics of wildlife preservation and ecological sustainability.\n\n\n\n\nMachine Learning\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Engineer &\nData Scientist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  },
  {
    "objectID": "portfolio/airline-performance.html#dataset",
    "href": "portfolio/airline-performance.html#dataset",
    "title": "Airline On-Time Performance",
    "section": "Dataset",
    "text": "Dataset\nThe Airline Reporting Carrier On-Time Performance Dataset, provided by the U.S. Department of Transportation’s Bureau of Transportation Statistics, contains scheduled and actual departure and arrival times reported by U.S. air carriers between 1987 and 2020. For this project, I used a 2 million-flight sample from the dataset, which represents less than 1% of the full dataset available through IBM Developer1.\n1 The dataset is available in three sizes: the original dataset of 194,385,636 flights, a 2 million sample version and a 2 thousand sample of flights from LAX to JFK airport. All three versions are available as gzip compressed tar or csv files.Key features include scheduled and actual flight times, dates, carrier information, origin and destination details, and cancellation or diversion statuses. The dataset also provides summary statistics such as elapsed time, distance, and delay causes. For a comprehensive list of variables, refer to the full here. Below is a summary of the main variable groupings:\n\n\nFeature glossary\n\n\nTemporal variables: Year, Quarter, Month, DayofMonth, DayOfWeek, FlightDate\nFlight variables: Reporting_Airline, Tail_Number, Flight_Number_Reporting_Airline\nOrigin/Destination variables:\nOriginAirportID, Origin, OriginCityName, OriginState, OriginStateName,\n\nDestAirportID, Dest, DestCityName, DestState, DestStateName\n\nDeparture/Arrival time variables:\nCRSDepTime, DepTime, DepDelay, DepDelayMinutes, DepDel15, DepartureDelayGroups\n\nCRSArrTime, ArrTime, ArrDelay, ArrDelayMinutes, ArrDel15, ArrivalDelayGroups\n\nTaxi variables: TaxiOut, WheelsOff, WheelsOn, TaxiIn\nCancellation variables: Cancelled, CancellationCode, Diverted\nFlight summary variables: CRSElapsedTime, ActualElapsedTime, AirTime, Flights, Distance, DistanceGroup\nCause of Delay (Data starts 6/2003): CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay"
  },
  {
    "objectID": "portfolio/airline-performance.html#preprocessing-and-feature-engineering",
    "href": "portfolio/airline-performance.html#preprocessing-and-feature-engineering",
    "title": "Airline On-Time Performance",
    "section": "Preprocessing and Feature Engineering",
    "text": "Preprocessing and Feature Engineering\nThe strategy for preprocessing interleaved univariate analysis with data cleaning and feature engineering for each variable grouping, using univariate insights to guide the preprocessing steps. This approach encompasses three key areas: conducting univariate analysis, preprocessing flight-related variables, and (re)engineering time-related features.\n\nUnivariate Analysis\nI performed univariate analysis on each variable group to assess distributions, variability, and missing values. This involved:\n\n\ndf_overview: Provides a snapshot of the dataset, including its shape, sample head and tail, and non-null counts.\n\ndef df_overview(df):\n    print(f\"Shape: {df.shape}\\n\")\n    print(f\"Head and tail preview:\")\n    display(df)\n    print(f\"Df info:\")\n    print(df.info(verbose=True), \"\\n\")\n    print(\"-\"*70)\n\n\n\nunivariate_preview: Generates a compact report with data types, unique values, top values, null value percentages, and summary statistics for selected columns.\n\ndef univariate_preview(df, cols, describe=True):\n    display(\"Data Preview\")\n    display(df[cols].head())\n    \n    display(\"Value Counts\")\n    list = []\n    for col in cols:\n        list.append(\n            [col,\n            df[col].dtypes,\n            df[col].nunique(),\n            df[col].value_counts().iloc[:5].index.tolist(),\n            \"{:.2f}%\".format(df[col].isna().mean()*100)]\n            )\n    display(pd.DataFrame(list, \n                         columns = ['columns', 'dtypes', 'nunique', 'top5', 'na%']\n                         ).sort_values('nunique', ascending=False))\n    \n    if describe:\n        display(\"Summary Stats\")\n        display(pd.concat([\n            df[cols].describe(),\n            df[cols].skew().to_frame('skewness').T,\n            df[cols].kurtosis().to_frame('kurtosis').T,\n        ]))\n\nI also visualized missing values using the missingno package2, and analyzed distributions and value counts with matplotlib and seaborn.\n2 The missingno package offers tools for visualizing missing data patterns through heatmaps, bar charts, and dendrograms.\n\nPreprocessing Flight Info Variables\nThe initial preprocessing phase focused on flight information variables, such as dates, flight numbers, and origin/destination details. Based on the insights from each univariate analysis, I performed the following preprocessing steps:\n\nData Cleaning\n\nConverted FlightDate to datetime64[ns]: Enabled efficient date operations, aggregations, and visualizations.\nImputed Missing State and State Names: Addressed missing data to improve dataset completeness.\nStandardized City Names: Unified city names to ensure consistency and eliminate outliers.\nRemoved Taxi, Cancelled, and Diverted Variables: Dropped these variables due to high missing data rates.\n\nFeature Engineering\n\nMapped Airline Codes: Used an airline dataset to convert codes to names for better readability and insight.\nCreated Unique Flight Identifier: Combined carrier codes and flight numbers to reduce ambiguity and uniquely identify flights.\n\n\n\n\n(Re)Engineering Time Variables\nThe second phase of preprocessing addressed a critical issue with time-related variables, especially concerning flight delays. The delay variables were calculated from the raw timestamps, without accounting for time zone discrepancies, daylight savings time, and cross-midnight flights. To address these, I developed a method to impute time zone information, correct timestamps, and accurately recompute delays. The steps are the following:\n\nImputed Time Zone Information: Assigned time zones to timestamps using a dictionary of airport codes and time zones.\nReverse Engineered Arrival Dates: Standardized times to UTC, recalculated timestamps based on flight departure dates and time differences, and converted back to local time zones.\nRemoved Remaining Negative Delays: Filtered out negative delay values to maintain accurate delay calculations.\nRecalculated Delays: Adjusted delays based on scheduled versus actual times, accounting for time zone differences and daylight savings time.\n\nAdditional cleaning and engineering included:\n\nStandardized Time Representations: Converted 0s to NaN and 2400s to 0s for consistency.\nImputed Missing CRS Values: Filled in missing scheduled times using differences between actual times and delays.\nOptimized Data Types: Improved computational efficiency by adjusting data types.\nDatetime Recalculations: Created UTC versions of time variables and recalculated delays and elapsed times more accurately than the original dataset."
  },
  {
    "objectID": "portfolio/airline-performance.html#exploratory-data-analysis",
    "href": "portfolio/airline-performance.html#exploratory-data-analysis",
    "title": "Airline On-Time Performance",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe Exploratory Data Analysis (EDA) for this project employed two approaches: a systematic univariate analysis during preprocessing and a targeted bivariate and multivariate analysis to address each research question3. The investigation was organized around four main themes:\n3 While the bivariate analysis adopts a longitudinal perspective to compare delay frequencies over time, a cross-sectional approach could further enhance insights by examining delays at specific points in time. This advanced analysis is planned for future work.\nLongitudinal: Examining how delays vary over time.\nCorrelational: Investigating the relationship between departure and arrival delays.\nCorporate: Analyzing delay patterns across different carriers.\nGeographical: Assessing how delays vary by airport.\n\nBefore answering these questions, I will first summarize the findings from the univariate analysis\n\nUnivariate Analysis\nThe univariate analysis revealed several key insights about the dataset:\n\n\nDate Columns: The flights span 34 years, with the distribution mean slightly shifted left (2005), suggesting industry growth specially in the past 15 years. Other temporal variables such as months, days, and weekdays are evenly distributed.\n\n\n\n\n\nFlight Information: Among the 33 unique carriers, Southwest Airlines (WN), Delta Air Lines (DL), and American Airlines (AA) are the most frequently represented.\n\n\n\n\n\nOrigin/Destination: Flights are distributed across all 50 U.S. states and five overseas territories, including Puerto Rico, the U.S. Virgin Islands, and Guam. The top destinations include California (CA), Texas (TX), Florida (FL), Illinois (IL), Georgia (GA), and New York (NY).\n\n\n\n\n\nTime Variables: Departures peak in the morning, while arrivals are most frequent in the evening. The mean CRS (Scheduled) and actual times for departures and arrivals, respectively, are nearly identical, indicating that average flight delays are small\n\n\n\n\n\nDelay Variables: About 80% of flights are on time, with 20% experiencing delays exceeding 15 minutes. Delays are right-skewed, with over 40% of flights arriving early by less than 15 minutes and about 30% arriving exactly on time. Delayed flights typically fall within the 15 to 40-minute range.\n\n\n\n\n\nFlight Summary: Most flights cover distances under 1000 miles and have durations of less than 160 minutes (2 hours and 40 minutes), as indicated by the right-skewed distribution of elapsed time variables.\n\n\n\n\n\nBivariate Analysis\n\nHow do delays vary across time?\nThis question involves a longitudinal analysis comparing the number of departure and arrival delays across time.\nTo answer this question, I aggregated the data by month and year, which provides a balance between granularity, interpretability and computational efficiency. Rather than using raw delay sums, I normalized the counts by the total number of flights each month to provide a more accurate depiction of delay frequency, accounting for variations in monthly flight volumes.\nI then visualized the monthly evolution of delays over the years, as illustrated in the figure below.\n\nThe normalized plots above reveal several insights:\n\nCross-sectional observations:\n\nThe line plot shows periodic fluctuations in both departure and arrival delays, reflecting seasonal throughout each year.\nA sharp drop in delays between 2001 and 2003 suggests possible external influences, such as the reduction in flights post-9/11 impacting delays4.\n\nLongitudinal observations:\n\nThe regression plot indicates a narrowing gap between departure and arrival delays over time, converging to approximately 20% by 2020.\nThe line plot shows a reduction in the volatility of delays over the years, suggesting a slight stabilization in the frequency of delays.\n\n\n4 One potential cause for this temporary drop in delays is the contraction in the airline industry that followed 9/11, reducing the amount of delays together with the number of flights (see this article)Although the narrowing gap and reduced volatility might suggest improved on-time performance, the apparent improvement is due to a 5% increase in departure delays rather than a reduction in arrival delays, with arrival delays decreasing by only 1% over 30 years. This confirms that airlines have been unable to improve on-time performance at arrivals and have become less efficient in managing departure delays.\n\n\nAre departures delays correlated with arrival delays?\nThis question involves a correlation analysis to explore the relationship between departure delays and arrival delays, both of which are continuous variables.\nI calculated the Pearson correlation coefficient for flights delayed by more than 15 minutes5, excluding on-time flights to avoid skewing the results. The Pearson coefficient ranges from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no linear relationship. I used a custom jointplot function to visualize the correlation, which combines a scatter plot with a box plot to display the distribution of both variables and the Pearson coefficient.\n5 15 minutes is the industry standard for defining a delayed flight.Prior to plotting, I cleaned the data by removing extreme cases, such as negative delays caused by flight times crossing the midnight transition to daylight savings time, which were not corrected in the preprocessing. The resulting jointplot is shown below.\n\nThe Pearson correlation coefficient between departure and arrival delays is 0.93, indicating a strong positive linear relationship. Key observations from the jointplot include:\n\nOutliers and Extreme Cases: The scatter plot reveals a wide range of outliers beyond the interquartile range, but these outliers generally follow the strong linear relationship between departure and arrival delays.\nLinear Relationship: While the relationship is linear across the full range of data, it becomes more dispersed at lower delay values and more tightly correlated at higher values.\nInterquartile Range: The interquartile range for both kinds of delays is narrower compared to the full data range, with most delays concentrated within 60 minutes.\n\nTo gain a clearer understanding of the core relationship, I recalculated the Pearson correlation coefficient focusing on the interquartile range. This refined analysis is shown in the updated jointplot.\n\nWithin the interquartile range, the Pearson correlation coefficient is 0.67, reflecting a weaker yet still positive linear relationship. The jointplot reveals a broader dispersion of data points within this range, with the majority of delays falling between 15 and 40 minutes, as shown by the adjacent box plots. The plot’s empty square in the lower left represents flights that did not meet the industry delay threshold.\nThe filtered scatter plot has two distinct boundaries that define the relationship between departure and arrival delays. I calculated approximate regression lines for these boundaries to understand the relationship more clearly.\n\nTwo kinds of observations can be made about the correlation of departure and arrival delays for the interquartile range:\n\nShallow Bound: Indicates the minimum threshold to which arrival delays can be minimized.\n\nFollows a linear relationship: \\(y = 1.15x - 45\\), where \\(y\\) is the arrival delay and \\(x\\) is the departure delay.\nImplies that the arrival delay can only be reduced by up to 45 minutes relative to a departure delay.\n\nSteep Bound: Indicates how much arrival delays can increase relative to departure delays.\n\nFollows a much steeper linear trend: \\(y = 6.5x + 120\\), where \\(y\\) is the arrival delay and \\(x\\) is the departure delay.\nIndicates that arrival delays can significantly exceed the general trend, suggesting the influence of external factors such as diversions, weather, or congestion at the arrival airport.\nReflects that arrival delays can significantly exceed departure delays due to\nShows an upper limit of 250 minutes for on-time departures (see first scatterplot), then progressively aligning with the general trend as departure delays increase.\n\n\nIn summary, the historical correlation analysis within the interquartile range shows a moderately positive linear relationship between departure and arrival delays. The broad dispersion of data points reflects the impact of external factors that influence arrival delays more significantly than departure delays. Historically, this dispersion has been constrained by two limits: arrival delays have typically been minimized by up to 45 minutes relative to departure delays (lower bound) but can exceed them by up to 250 minutes (upper bound). Conversely, analysis of the full range of data has historically demonstrated a strong positive linear relationship between departure and arrival delays.\n\n\nHow do delays vary across carriers?\nAnalyzing how delays vary across carriers is one of the most frequently asked questions to measure individual airline performance and market competitiveness. It requires a bivariate analysis comparing the frequency of delays across carriers.\nTraditional approaches often focus on total delay counts, but this can be misleading due to:\n\nFlight Volume: Carriers with more flights naturally accumulate higher delay counts, which can skew the results.\nCarrier Age: Older carriers tend to have more historical data, potentially influencing delay counts.\n\nTo address these issues, this analysis employs a more precise metric: normalized delay frequency. This metric adjusts delay counts by the total number of flights per carrier6. This method provides a clearer picture of delay frequency by accounting for variations in flight volume and partially considering carrier age. The plots below illustrate the normalized delay frequencies across carriers.\n\n\n6 An alternative normalization approach could involve adjusting by carrier age, but such data was not available for this analysis.\nThe normalized delay counts offer a robust comparison of on-time performance across airlines, even for those with varying flight volumes or ages. Key insights from the analysis include:\n\nConsistent Rankings: JetBlue Airways, ExpressJet Airlines, and Frontier Airlines consistently rank among the top carriers for both departure and arrival delays.\nTop Departure Delays: JetBlue Airways, ExpressJet Airlines, Frontier Airlines, Allegiant Air, and Southwest Airlines have the highest proportion of delays at departure.\nTop Arrival Delays: Piedmont Airlines, JetBlue Airways, ExpressJet Airlines, Conair, and Frontier Airlines have the highest proportion of delays at arrival.\nLowest Delays: Carriers with the lowest delay frequencies include historical carriers with shorter operational histories in the dataset, such as Pan American World Airways, Midway Airlines, and Northwest Airlines, as well as regional carriers like Hawaiian Airlines, Aloha Air Cargo, Endeavor Air, and SkyWest Airlines.\nLargest Differences: Southwest Airlines shows the largest disparity between departure and arrival delays, ranking 5th in departure delays but 20th in arrival delays.\n\nFor evaluating carrier performance, the proportion of departure delays is particularly relevant. Departure delays are more directly controllable by carriers, influenced by factors such as boarding, fueling, and maintenance. Furthermore, as shown by the correlation analysis, departure delays are positively correlated with arrival delays, making them a key indicator of performance. Arrival delays, influenced by external factors like weather, air traffic control, and airport congestion, are less indicative of a carrier’s operational efficiency.\nConsidering this, carriers with the highest proportion of departure delays, also consistent in arrival delays, are JetBlue Airways, ExpressJet Airlines, and Frontier Airlines.\n\n\nHow do delays vary by airport?\nComparing delay frequencies across airports helps evaluate airport performance and its impact on the aviation sector. This analysis uses two metrics: total delays and normalized delay frequency. However, choosing a metric to answer this question proves challenging, as each metric reveals a completely different story:\n\nTotal delays are skewed towards airport size: Larger airports with more flights tend to show higher total delays, reflecting their size but potentially skewing results.\n\n\nAdvantage: Differentiates performance between airports.\nDisadvantage: Biased towards larger airports.\n\n\nNormalized delays show high kurtosis: This metric adjusts for airport size, but shows a distribution tightly clustered around the mean, which makes it harder to distinguish between airports.\n\n\nAdvantage: Provides a clearer view of delay frequency across airports.\nDisadvantage: Less effective in differentiating among airports with similar delay frequencies.\n\n\nGiven these factors, I chose to use total delay counts for arrival delays, as it provides a clearer picture of major airports’ performance and is more relevant for assessing well-known airports. Arrival delays, influenced by external factors like airport operations, offer a better metric for evaluating airport performance compared to departure delays, which are more affected by carrier operations7. Below are the plots of total arrival delays across airports.8. Below are the plots of the total delay counts across airports.\n7 According to the Airline On-Time Statistics and Delay Causes, air carrier delays accounted for nearly 25% of national delays from January 2010 to March 20208 Of course, as stated in the previous section, airport management is not the only cause for arrival delays. There is also a moderate correlation between departure and arrival delays which increases after as certain threshold of departure delays. However, when comparing airports, arrival delays abstract the role of the carrier and focus on the external factors, among which is airport management.\n\nArrival Delays by Airport\n\n\n\n\nThe bubble map visualizes arrival delays by airport, with bubble size representing the total number of delays. Key insights include:\n\nTop Airports: The top five airports with the highest total delay counts are Chicago O’Hare International Airport (ORD), Hartsfield-Jackson Atlanta International Airport (ATL), Dallas/Fort Worth International Airport (DFW), Los Angeles International Airport (LAX), and San Francisco International Airport (SFO).\nCostal Airports: Coastal airports such as Los Angeles (LAX), San Francisco (SFO), Newark (EWR), and Boston (BOS) also rank high among the top 15 airports.\nNew York Area Congestion: Newark (EWR) and LaGuardia (LGA) are among the top ten for delays, highlighting congestion and operational challenges in the New York area.\nGeographical Spread: The top 20 airports for delays are spread across the country, showing that delays are a widespread issue.\nJFK: John F. Kennedy (JFK) ranks 20th with 6,095 delays, significantly lower than other major airports in the New York Metropolitan area. The causes of this difference would be an interesting question for further analysis.\n\nOverall, this analysis highlights that larger airports, particularly those in coastal and high-traffic areas like New York, experience significant delays. The geographical spread of delays underscores that this issue affects airports nationwide. Overall, this analysis underscores the importance of addressing delays at both major hubs and regional airports to improve the efficiency of the U.S. aviation system."
  },
  {
    "objectID": "portfolio/airline-performance.html#conclusion",
    "href": "portfolio/airline-performance.html#conclusion",
    "title": "Airline On-Time Performance",
    "section": "Conclusion",
    "text": "Conclusion\nThis project analyzed 30 years of US domestic flight data to uncover trends in on-time performance across carriers, airports, and time. The analysis focused on four key questions: how delays vary over time, the correlation between departure and arrival delays, variations in delays among carriers, and delay distribution across airports. The findings are summarized below:\n\nLongitudinal Analysis\n\nOver 34 years, the gap between departure and arrival delays has narrowed due to an increase in departure delays, not a decrease in arrival delays.\nDelay volatility has slightly decreased, suggesting more stable seasonal delay patterns.\nArrival delays have only decreased by 1%, while departure delays have increased by over 5%, indicating limited progress in improving on-time performance.\n\nCorrelation Analysis\n\nDeparture and arrival delays show a strong positive linear relationship, with a Pearson correlation coefficient of 0.93.\nWithin the interquartile range, the correlation weakens to 0.67, and delays are bounded by two linear trends: arrival delays can be minimized by up to 45 minutes relative to departure delays but can exceed them by up to 250 minutes.\nThis disparity reflects that while departure delays are more under the carrier’s control, arrival delays are influenced by external factors like weather and airport congestion.\n\nCarrier Analysis\n\nJetBlue Airways, ExpressJet Airlines, and Frontier Airlines consistently have the highest frequencies of delays at both departure and arrival.\nSouthwest Airlines shows the largest difference between departure and arrival delays, ranking 5th in departure delays but 20th in arrival delays.\nDeparture delays are a more accurate indicator of carrier performance, as they are influenced by operational factors such as boarding and maintenance.\n\nAirport Analysis\n\nThe top airports with the highest total delay counts are Chicago O’Hare, Hartsfield-Jackson Atlanta, Dallas/Fort Worth, Los Angeles, and San Francisco.\nCoastal airports like Los Angeles, San Francisco, Newark, and Boston rank high, indicating congestion and operational challenges in both coasts.\nThe geographical spread of the top 20 airports shows that delays are a nationwide issue, spread across the Northeast, Southeast, Midwest, Central, and West regions of the US.\n\n\nThe analysis underscores persistent challenges in improving on-time performance, particularly with departure delays. The strong correlation between departure and arrival delays emphasizes the importance of addressing issues at the departure stage to enhance overall punctuality. The results suggest that tailored strategies are needed for different carriers and airports. The findings from this project have been effectively translated into an interactive visualization dashboard, providing valuable insights for stakeholders to inform decision-making and improve operational efficiency in the aviation industry."
  },
  {
    "objectID": "portfolio/resume-classifier.html#dataset",
    "href": "portfolio/resume-classifier.html#dataset",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Dataset",
    "text": "Dataset\nThe project utilizes the Resume Dataset from LiveCareer, available at Kaggle. The dataset comprises over 2400 resumes in both string and HTML format, each labeled with their respective labeled categories. The dataset includes of the following variables:\n\n\n\n\nID: A unique identifier for each resume\nResume_str: The textual content of the resume\nResume html: The HTML content of the resume\nCategory: The job field classification of each resume (e.g., Information Technology, Teaching, Advocacy, Business Development, Healthcare)"
  },
  {
    "objectID": "portfolio/resume-classifier.html#preprocessing",
    "href": "portfolio/resume-classifier.html#preprocessing",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Preprocessing",
    "text": "Preprocessing\nData preprocessing involved two tasks: text preprocessing and data rebalancing.\nFor text cleaning, I developed a custom preprocessing function that integrates several operations into a streamlined pipeline. This function is highly adaptable, with parameters to handle tasks such as converting text to lowercase, decoding HTML, removing emails and URLs, eliminating special characters, expanding contractions, applying custom regex cleaning, and performing tokenization, stemming, lemmatization, and stopword removal. In particular, I improved the text quality by eliminating noise like non-existent words and frequent, resume-specific stopwords such as months, function verbs, and section headings.\n\n\nView Code\nimport re\nfrom bs4 import BeautifulSoup\nfrom unidecode import unidecode\nimport contractions\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\ndef preprocessing(\n    text, \n    tokenize=False,\n    stem=False,\n    lem=False,\n    html=False,\n    exist=False,\n    remove_emails=True,\n    remove_urls=True,\n    remove_digits=True,\n    remove_punct=True,\n    expand_contractions=True,\n    remove_special_chars=True,\n    remove_stopwords=True,\n    lst_stopwords=None,\n    lst_regex=None\n) -&gt; str | list[str]:\n    \n    # Lowercase conversion\n    cleaned_text = text.lower()\n\n    # HTML decoding\n    if html:\n        soup = BeautifulSoup(cleaned_text, \"html.parser\")\n        cleaned_text = soup.get_text()\n    \n    # Remove Emails\n    if remove_emails:\n        cleaned_text = re.sub(r\"([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)\", \" \", cleaned_text)\n    \n    # URL removal\n    if remove_urls:\n        cleaned_text = re.sub(r\"(http|https|ftp|ssh)://[\\w_-]+(?:\\.[\\w_-]+)+[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-]?\", \" \", cleaned_text)\n    \n    # Remove escape sequences and special characters\n    if remove_special_chars:\n        cleaned_text = re.sub(r\"[^\\x00-\\x7f]\", \" \", cleaned_text)\n        cleaned_text = unidecode(cleaned_text)\n    \n    # Remove multiple characters\n    cleaned_text = re.sub(r\"(.)\\1{3,}\", r\"\\1\", cleaned_text)\n    \n    # Expand contractions\n    if expand_contractions:\n        cleaned_text = contractions.fix(cleaned_text)\n        cleaned_text = re.sub(\"'(?=[Ss])\", \"\", cleaned_text)\n    \n    # Remove digits\n    if remove_digits:\n        cleaned_text = re.sub(r\"\\d\", \" \", cleaned_text)\n    \n    # Punctuation removal\n    if remove_punct:\n        cleaned_text = re.sub(\"[!\\\"#$%&\\\\'()*+\\,-./:;&lt;=&gt;?@\\[\\]\\^_`{|}~]\", \" \", cleaned_text)\n    \n    # Line break and tab removal\n    cleaned_text = re.sub(r\"[\\n\\t]\", \" \", cleaned_text)\n    \n    # Excessive spacing removal\n    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n    \n    # Regex (in case, before cleaning)\n    if lst_regex: \n        for regex in lst_regex:\n            compiled_regex = re.compile(regex)\n            cleaned_text = re.sub(compiled_regex, '', cleaned_text)\n\n    # Tokenization (if tokenization, stemming, lemmatization or custom stopwords is required)\n    if stem or lem or remove_stopwords or tokenize:\n        if isinstance(cleaned_text, str):\n            cleaned_text = cleaned_text.split()\n        \n        # Remove stopwords\n        if remove_stopwords:\n            if lst_stopwords is None:\n                lst_stopwords = set(stopwords.words('english'))\n            cleaned_text = [word for word in cleaned_text if word not in lst_stopwords]\n\n        # Remove non-existent words\n        if exist:\n            english_words = set(words.words())\n            cleaned_text = [word for word in cleaned_text if word in english_words]\n\n        # Stemming\n        if stem:\n            stemmer = PorterStemmer()\n            cleaned_text = [stemmer.stem(word) for word in cleaned_text]\n\n        # Lemmatization\n        if lem:\n            lemmatizer = WordNetLemmatizer()\n            cleaned_text = [lemmatizer.lemmatize(word) for word in cleaned_text]\n        \n        if not tokenize:\n            cleaned_text = ' '.join(cleaned_text)\n\n    return cleaned_text\n\n\nNext, I converted the Category variable to a numerical format using Scikit-Learn’s LabelEncoder to meet the requirements of the algorithms. After cleaning, I saved the text separately for future use in tasks such as topic modeling and document similarity, where balancing is not required.\nTo address category imbalance for text classification, I employed random resampling using Scikit-Learn’s resample method. This approach ensures balanced representation across all categories, enhancing accuracy and reducing bias, thereby improving overall model performance."
  },
  {
    "objectID": "portfolio/resume-classifier.html#linear-svc",
    "href": "portfolio/resume-classifier.html#linear-svc",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Linear SVC",
    "text": "Linear SVC\n\n\n\n1 Linear Support Vector Classifier (SVC) is a classification algorithm that seeks to find the maximum-margin hyperplane, that is, the hyperplane that most clearly classifies observations2 Truncated Singular Value Decomposition (SVD) is a dimensionality reduction technique that decomposes a matrix into three smaller matrices, retaining only the most significant features of the original matrix.For this first model, I trained a baseline Linear SVC1 using the TF-IDF vectors. Then, I performed Latent Semantic Analysis (LSA) by applying Truncated Singular Value Decomposition (SVD)2 to reduce the TF-IDF matrix to a lower-dimensional space. The performance of both models provides a baseline for comparison with more advanced models in subsequent sections.\n\nTakeaways\n\nThe baseline model delivers robust accuracy while optimizing resource usage, particularly when dimensionality reduction is applied.\nLinear SVC achieved an accuracy of 84% on the test set.\nLinear SVC achieved 81% accuracy with Truncated SVD on the test set, utilizing only 5% of the original features.\n\n\n\nImport Packages and Data\nIn addition to standard libraries, I imported two custom functions: classifier_report to generate classification reports and confusion matrices, and save_performance to store model performance metrics in a JSON file for future analysis. I also loaded a pre-trained Label Encoder to label the encoded categories in subsequent visualizations.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import TruncatedSVD\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom src.modules.ml import classifier_report\nfrom src.modules.utils import save_performance\n\n# Import the data with engineered features\ndf = pd.read_parquet('./data/3-processed/resume-features.parquet')\n\n# Label model to label categories\nle = joblib.load('./models/le-resumes.gz')\n\n\n\nBaseline LinearSVC\nI split the dataset into 70% for training and 30% for testing. I used the tfidf_vectors column as the feature matrix, stacking the vectors into a single matrix with np.vstack. The encoded Category column served as the target variable, and I extracted the variables into a NumPy array using the .values method to improve performance. To verify the split, I printed the shapes of the training and testing sets.\n\n\nView Code\nX = np.vstack(df['tfidf_vectors'].values)\ny = df['Category'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n\nprint(f\"Training set: {X_train.shape}, {y_train.shape}\")\nprint(f\"Testing set: {X_test.shape}, {y_test.shape}\")\n\n\nTraining set: (2016, 11618), (2016,)\nTesting set: (864, 11618), (864,)\nNext, I trained a Linear SVC model with default hyperparameters and evaluated its performance using the classifier_report function, which generated a classification report and confusion matrix.\n\n\nView Code\nsvc = LinearSVC(dual=\"auto\")\naccuracy = classifier_report([X_train, X_test, y_train, y_test], svc, le.classes_, True)\n\n\nSVC accruacy score 87.15%\n\nThe model achieved an accuracy of 87% on the test set, which was a strong result for a baseline model. However, TF-IDF vectors often lead to sparse, high-dimensional representations with low information density. To address this, I employed Truncated Singular Value Decomposition (SVD).\n\n\nLinearSVC with Truncated SVD\n\n\n\n3 For an in-depth explanation, see Manning, C.D., Raghavan, P., & Schütze, H. (2008) ‘Matrix Decompositions and Latent Semantic Indexing’, Introduction to Information Retrieval, 1.Transforming TF-IDF matrices with Truncated SVD is known as Latent Semantic Analysis (LSA). This technique extracts the \\(n\\) largest eigenvalues to capture the most significant semantic relationships between terms and documents, while filtering out noise and low-information features3.\nI used TruncatedSVD to reduce the number of components to 500, which is less than 5% of the original number of features. I then applied this transformation, split the resulting feature matrix into training and testing sets, and proceeded to train the new model.\n\n\nView Code\nt_svd = TruncatedSVD(n_components=500, algorithm='arpack')\nX_svd = t_svd.fit_transform(X)\n\n\nX_train_svd, X_test_svd, y_train_svd, y_test_svd = train_test_split(X_svd, \n                                                                    y, \n                                                                    test_size=0.30, \n                                                                    random_state=42)\n\n\nprint(f\"Training set: {X_train_svd.shape}, {y_train_svd.shape}\")\nprint(f\"Testing set: {X_test_svd.shape}, {y_test_svd.shape}\")\n\n\nTraining set: (2016, 500), (2016,)\nTesting set: (864, 500), (864,)\nI trained a new Linear SVC model using the SVD-transformed feature matrix and generated a classification report and confusion matrix.\n\n\nView Code\nsvc_svd = LinearSVC(dual=\"auto\")\naccuracy = classifier_report([X_train_svd, X_test_svd, y_train_svd, y_test_svd], svc_svd, le.classes_, True)\n\n\nSVC accruacy score 84.94%\n\nThe model achieved an 84% accuracy on the test set, which is slightly lower than the baseline model. However, this performance comes from a model trained on less than 5% of the original features, demonstrating that despite the reduced dimensionality, the model retains a high level of accuracy. This highlights the sparsity of the TF-IDF vectors while showcasing that effective accuracy can be maintained with a significantly smaller feature set.\nBefore proceeding to the next model, I saved the performance metrics of the baseline model for future comparison.\n\n\nView Code\nsave_performance(model_name='LinearSVC',\n                 architecture='default',\n                 embed_size='n/a',\n                 learning_rate='n/a',\n                 epochs='n/a',\n                 optimizer='n/a',\n                 criterion='n/a',\n                 accuracy=87.15\n                 )"
  },
  {
    "objectID": "portfolio/resume-classifier.html#feedforward-neural-network",
    "href": "portfolio/resume-classifier.html#feedforward-neural-network",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Feedforward Neural Network",
    "text": "Feedforward Neural Network\n\n\n\n4 Feedforward Neural Networks (FNN) are a type of neural networks where information moves in only one direction—forward—from the input nodes, through hidden layers, and to the output nodes.The next model is a Feedforward Neural Network (FNN)4 built using PyTorch. I created an iterator for the dataset with the DataLoader class, which tokenized and numericalized the resumes, dynamically padded the sequences, and batched the data for training, thus saving memory and computation time. I then constructed a simple neural network architecture featuring an embedding layer, followed by three fully connected layers with ReLU activation functions. The model was trained using the Adam optimizer and Cross Entropy Loss criterion, and its performance was evaluated on the test set with accuracy as the metric.\n\nTakeaways\n\nThe Feedforward Neural Network achieved an accuracy of 73.15% with a loss of 1.1444 on the test set.\nPerformance suggests minimal overfitting, indicated by the small gap between training and validation accuracies.\nThe model demonstrates robust generalization, with test accuracy closely aligning with validation accuracy.\n\n\n\nImport Packages and Data\nIn addition to standard PyTorch and pandas imports, I also imported three custom functions:\n\ntrain_model: Trains the model using the provided hyperparameters and data, prints real-time training and validation loss and accuracy, and saves the best model based on the lowest validation loss. It also offers the option to visualize the training progress with the PlotLosses library or matplotlib.\ntest_model: Evaluates the model on the test set using the best model saved during training and returns the test accuracy.\nsave_performance: Saves the model’s performance metrics to a JSON file for future analysis.\n\n\n\nView Code\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom livelossplot import PlotLosses\nfrom tqdm import tqdm\n\nfrom src.modules.dl import train_model, test_model\nfrom src.modules.utils import save_performance\n\ndata = pd.read_parquet('./data/3-processed/resume-features.parquet', columns=['Category', 'cleaned_resumes'])\n\n\nWhen training deep learning models, I always include the option to use a GPU if available and set the device variable accordingly. This approach not only enables the model to utilize parallel computing resources if available, but also makes the code reproducible across different device setups. The snippet below checks for GPU availability and assigns the device variable, which is then used by the DataLoader and the model.\n\n\nView Code\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device('cpu')\nprint(\"Using {}.\".format(device))\n\n\nUsing cpu.\n\n\nDataset and DataLoader\nBefore constructing the Dataset class, I defined a tokenization function that initializes the tokenizer, tokenizes the text data, and builds a vocabulary using PyTorch’s get_tokenizer and build_vocab_from_iterator functions. This function returns the tokenized texts for indexing by the DataLoader during training and the vocabulary for determining vocabulary size.\n\n\nView Code\ndef tokenization(texts, tokenizer_type='basic_english', specials=['&lt;unk&gt;'], device=device):\n    # Instantiate tokenizer\n    tokenizer = get_tokenizer(tokenizer_type)\n    # Tokenize text data\n    tokens = [tokenizer(text) for text in texts]\n    # Build vocabulary\n    vocab = build_vocab_from_iterator(tokens, specials=specials)\n    # Set default index for unknown tokens\n    vocab.set_default_index(vocab['&lt;unk&gt;'])\n\n    # Convert tokenized texts to a tensor\n    tokenized_texts = [torch.tensor([vocab[token] for token in text], dtype=torch.int64, device=device) for text in tokens]\n\n    return tokenized_texts, vocab\n\n\nNext, I created the ResumeDataset iterator, which preprocesses the text data using the tokenization function and indexes samples for the DataLoader during training. The class includes: - __len__: Returns the length of the dataset. - vocab_size: Provides the size of the vocabulary. - num_class: Returns the number of unique classes in the dataset. - __getitem__: Returns a sample of text and its corresponding label from the dataset.\n\n\nView Code\nclass ResumeDataset(Dataset):\n    # Dataset initialization and preprocessing\n    def __init__(self, data):\n        # Initialize dataset attributes\n        super().__init__()\n        self.text = data.iloc[:,1]\n        self.labels = data.iloc[:,0]\n        \n        self.tokenized_texts, self.vocab = tokenization(self.text)\n\n    # Get length of dataset\n    def __len__(self):\n        return len(self.labels)\n\n    # Get vocabulary size\n    def vocab_size(self):\n        return len(self.vocab)\n\n    # Get number of classes\n    def num_class(self):\n        return len(self.labels.unique())\n\n    # Get item from dataset\n    def __getitem__(self, idx):\n        sequence = self.tokenized_texts[idx]\n        label = self.labels[idx]\n        return sequence, label\n\n\nI also defined a collate_fn function to implement dynamic padding when batching the data. Dynamic padding is a technique used to adjusts sequence lengths to match the longest sequence in each batch, rather than to the longest sequence in the entire dataset. Dynamic padding allows the model to process sequences of varying lengths more efficiently, saving memory and computation time.\n\n\nView Code\ndef collate_fn(batch):\n    sequences, labels = zip(*batch)\n    # Pad sequences to the longest sequence in the batch\n    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n    # Convert labels to tensor\n    labels = torch.tensor(labels, dtype=torch.long)\n    return sequences_padded, labels\n\n\nFinally, I instantiated the ResumeDataset class and split the dataset into 70% training, 15% validation, and 15% test sets using the random_split function. I created DataLoader iterators for each set, applying dynamic padding through the collate_fn function.\n\n\nView Code\ndataset = ResumeDataset(data)\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15])\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\n\n\n\nModel Architecture\n\n\n\nThe model is a simple FNN with an embedding layer, followed by two fully connected layers with ReLU activation functions, and a final fully connected layer with output size matching the number of classes. The architecture is defined in the SimpleNN class, which takes the vocabulary size, embedding size, number of classes, and an expansion_factor parameter (set to 2) to determine the hidden dimension size.\nThe EmbeddingBag function efficiently computes embeddings by first creating embeddings for the input indices and then averaging the output across the sequence dimension. This approach accommodates sequences of varying lengths, allowing the model to process them more efficiently.\n\n\nView Code\nclass SimpleNN(nn.Module):\n    def __init__(self, vocab_size, embed_size, num_class, expansion_factor=2, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_size, sparse=False)\n        self.hidden_dim = embed_size * expansion_factor\n        self.layer1 = nn.Linear(embed_size, self.hidden_dim)\n        self.layer2 = nn.Linear(self.hidden_dim, embed_size)\n        self.layer3 = nn.Linear(embed_size, num_class)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = F.relu(self.layer1(x))\n        x = self.dropout(x)\n        x = F.relu(self.layer2(x))\n        x = self.dropout(x)\n        x = self.layer3(x)\n        return x\n\n\n\n\nHyperparameters and Training\nBefore training, I set the hyperparameters for the neural network. The vocabulary size and number of classes are sourced from the ResumeDataset class. The embedding size is set to 60, the learning rate to 1e-3, and the model is trained for 40 epochs.\n\n\nView Code\nvocab_size = dataset.vocab_size()\nnum_class = dataset.num_class()\nembed_size = 60\nlr=0.001\nepochs = 40\n\n\nI instantiated the model, assigned it to the available device, and defined the loss function and optimizer. The loss function used is Cross Entropy Loss, suitable for multi-class classification, and the optimizer is Adam, known for its adaptive learning rate capabilities. Additionally, I set up a learning rate scheduler to reduce the learning rate by a factor of 0.1 if the validation loss does not improve over a specified patience period, helping to prevent overfitting and enhance generalization. The model and hyperparameters were then passed to the train_model function5.\n5 During fine-tuning, the model achieved better accuracy with a dropout rate of 0.4.To visualize the training progress, I set the visualize parameter to ‘liveloss’, utilizing the PlotLosses library for a dynamically updating plot that shows real-time training and validation loss and accuracy. This allows for effective monitoring and adjustment of hyperparameters as needed.\n\n\nView Code\nmodel = SimpleNN(vocab_size, embed_size, num_class, dropout=0.4).to(device)\ncriterion = CrossEntropyLoss()\nloss = Adam(model.parameters(), lr=lr)\nscheduler = ReduceLROnPlateau(loss, patience=2)\n\ntrain_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler, visualize='liveloss')\n\n\n\naccuracy\n  training           (min:    0.036, max:    0.712, cur:    0.707)\n  validation         (min:    0.037, max:    0.694, cur:    0.685)\nlog loss\n  training           (min:    0.873, max:    3.187, cur:    0.921)\n  validation         (min:    1.274, max:    3.184, cur:    1.330)\n------------------------------\nBest model saved:\nVal Loss: 1.3300 | Val Acc: 0.6852\n✅ Training complete!\nThe training and validation losses decreased steadily until the 30th epoch, indicating effective learning. After 30 epochs, training loss continued to decrease while validation loss plateaued. However, the final training loss was 0.92 and the validation loss was 1.33, with a minimal gap suggesting good generalization.\nIn terms of accuracy, both training and validation accuracies increased until they converged at 40 epochs. At convergence, the training accuracy was 71% and the validation accuracy was 69%. The small difference between these accuracies indicates minimal overfitting and good generalization to unseen data. The best model saved achieved a validation loss of 1.33 and a validation accuracy of 68.52%.\n\n\nEvaluation\nAfter training the model, I evaluated its performance on the test set using the test_model function. This function takes the trained model, test data loader, and criterion as inputs, and returns the model’s accuracy on the test set.\n\n\nView Code\naccuracy = test_model(model, test_loader, criterion)\n\n\nTest Loss: 1.1444 | Test Acc: 0.7315\n✅ Testing complete!\nThe FNN achieved a test accuracy of 73.15% and a loss of 1.1444. The test performance is consistent with the validation accuracy observed during training, indicating that the model generalizes well to new data and demonstrates robustness in its predictions.\nWhile the FNN’s accuracy is lower than the baseline model, it is still impressive given the simplicity of the architecture. Performance could be enhanced by incorporating advanced regularization techniques, utilizing pre-trained word embeddings6, or increasing model complexity. The next section explores a more advanced Transformer-based architecture that leverages self-attention mechanisms to capture long-range dependencies in the data.\n6 Examples of pre-trained embeddings include: Word2Vec, GloVe, and fastText.To conclude this section, I saved the performance metrics of the FNN model for future analysis.\n\n\nView Code\nsave_performance(model_name='Feedforward Neural Network',\n                 architecture='embed_layer-&gt;dropout-&gt;120-&gt;dropout-&gt;60-&gt;dropout-&gt;num_classes',\n                 embed_size='60',\n                 learning_rate='1e-3',\n                 epochs='50',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=73.15,\n                 )"
  },
  {
    "objectID": "portfolio/resume-classifier.html#encoder-model",
    "href": "portfolio/resume-classifier.html#encoder-model",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Encoder Model",
    "text": "Encoder Model\n\n\n\nThe next model utilizes the encoder component of the Transformer architecture for text classification. Unlike decoders, which generate output sequences from dense representations, encoders instead transform input sequences into dense representations. This capability is particularly valuable for tasks such as sentiment analysis, named entity recognition, and text classification.\nThe encoder model follows the Transformer architecture described in Attention is All You Need7 and is used as a baseline for transformer-based models. I construct the encoder architecture with an embedding layer, a stack of encoder layers, and a feed-forward neural network for classification. I then initialize the hyperparameters for training and train the model using the Adam optimizer and Cross Entropy Loss criterion. The model’s performance is evaluated on the test set using accuracy as the evaluation metric.\n7 Vaswani, Ashish u. a. (2017): Attention Is All You Need. Advances in neural information processing systems 30.The imported packages and the DataLoader setup mirror those used for the Feedforward Neural Network model (see packages and data preparation), so I will focus on the model architecture.\n\nTakeaways\n\nThe Transformer Encoder model achieved an accuracy of 75% on the test set.\nThis model provides a solid baseline for transformer-based approaches in text classification tasks.\n\n\n\nModel Architecture\n\n\n\n\n\nModified image from Sebastian Raschka\n\n\n8 The multi-head self-attention mechanism is a component of the transformer model that weights the importance of different elements in a sequence by computing attention scores multiple times in parallel across different linear projections of the input.9 Layer normalization is a normalization technique that uses the mean and variance statistics calculated across all features.The Encoder model features an embedding layer, a stack of encoder layers, and a fully connected neural network for classification. The embedding layer converts input sequences into dense representations, which then pass through the encoder layers. Each encoder layer includes a multi-head self-attention mechanism8, followed by a residual connection with layer normalization9, a feed-forward neural network, and another residual connection with layer normalization. The final output is processed by a feed-forward neural network for classification.\nI built the encoder model from scratch to gain a deeper understanding of its architecture and components. Using a modular approach, I created each component as a separate class and integrated them in the TransformerEncoder class.\n\nFurther Reading:\nThe implementation draws inspiration from these resources:\n\nThe Annotated Transformer\nCoding a Transformer from Scratch on PyTorch (YouTube)\nText Classification with Transformer Encoders\n\n\n\n\nView Code\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        # Dimensions of embedding layer\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        # Embedding dimension\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, dropout: float = 0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        # Initialize positional embedding matrix (vocab_size, d_model)\n        pe = torch.zeros(vocab_size, d_model)\n        # Positional vector (vocab_size, 1)\n        position = torch.arange(0, vocab_size).unsqueeze(1)\n        # Frequency term\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000) / d_model))\n        # Sinusoidal functions\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # Add batch dimension\n        pe = pe.unsqueeze(0)\n        # Save to class\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, d_model: int, eps: float = 1e-6):\n        super().__init__()\n        # Learnable parameters\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.ones(d_model))\n        # Numerical stability in case of 0 denominator\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        # Linear combination of layer norm with parameters gamma and beta\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1):\n        super().__init__()\n        # Layer normalization for residual connection\n        self.norm = LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x1, x2):\n        return self.dropout(self.norm(x1 + x2))\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1):\n        super().__init__()\n        # Linear layers and dropout\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float =0.1, qkv_bias: bool = False, is_causal: bool = False):\n        super().__init__()\n        assert d_model % num_heads == 0,  \"d_model is not divisible by num_heads\"\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.dropout = dropout\n        self.is_causal = is_causal\n\n        self.qkv = nn.Linear(d_model, 3 * d_model, bias=qkv_bias)\n        self.linear = nn.Linear(num_heads * self.head_dim, d_model)\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_length = x.shape[:2]\n\n        # Linear transformation and split into query, key, and value\n        qkv = self.qkv(x)  # (batch_size, seq_length, 3 * embed_dim)\n        qkv = qkv.view(batch_size, seq_length, 3, self.num_heads, self.head_dim)  # (batch_size, seq_length, 3, num_heads, head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_length, head_dim)\n        queries, keys, values = qkv  # 3 * (batch_size, num_heads, seq_length, head_dim)\n\n        # Scaled Dot-Product Attention\n        context_vec = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask, dropout_p=self.dropout, is_causal=self.is_causal)\n\n        # Combine heads, where self.d_model = self.num_heads * self.head_dim\n        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n        context_vec = self.dropout_layer(self.linear(context_vec))\n\n        return context_vec\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, hidden_dim: int, dropout: float = 0.1):\n        super().__init__()\n        # Multi-head self-attention mechanism\n        self.multihead_attention = MultiHeadAttention(d_model, num_heads, dropout)\n        # First residual connection and layer normalization\n        self.residual1 = ResidualConnection(d_model, dropout)\n        # Feed-forward neural network\n        self.feed_forward = FeedForward(d_model, hidden_dim, dropout)\n        # Second residual connection and layer normalization\n        self.residual2 = ResidualConnection(d_model, dropout)\n\n    def forward(self, x, mask=None):\n        x = self.residual1(x, self.multihead_attention(x, mask))\n        x = self.residual2(x, self.feed_forward(x))\n        return x\n\nclass EncoderStack(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, hidden_dim: int, num_layers: int, dropout: float = 0.1):\n        super().__init__()\n        # Stack of encoder layers\n        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, hidden_dim, dropout) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, num_heads: int, hidden_dim: int, num_layers: int, out_features: int, dropout: float = 0.1):\n        super().__init__()\n        self.embedding = EmbeddingLayer(vocab_size, d_model)\n        self.positional_embedding = PositionalEmbedding(vocab_size, d_model, dropout)\n        self.encoder = EncoderStack(d_model, num_heads, hidden_dim, num_layers, dropout)\n        self.classifier = nn.Linear(d_model, out_features)\n\n    def forward(self, x, mask=None):\n        x = self.embedding(x)\n        x = self.positional_embedding(x)\n        x = self.encoder(x, mask)\n        x = x.mean(dim=1)\n        x = self.classifier(x)\n        return x\n\n\n\n\nHyperparameters and Training\nWith the model constructed, I set the training hyperparameters. As with the previous model, I obtain the vocabulary size and number of output features from the ResumeDataset class. The embedding size is set to 80, the hidden dimension to 180, and the multi-head attention mechanism uses 4 heads with an encoder stack of 4 layers. I train the model for 20 epochs with a learning rate of 1e-3.\n\n\nView Code\nvocab_size = dataset.vocab_size()\nd_model = 80\nnum_heads = 4\nhidden_dim = 180\nnum_layers = 4\nout_features = dataset.num_class()\nlr = 0.001\nepochs = 20\n\n\nI instantiated the model with the specified hyperparameters and moved it to the appropriate device. The criterion and optimizer remained unchanged from the previous model, using the Adam optimizer and CrossEntropyLoss for multi-class classification. I also initialized a learning rate scheduler with a patience of 2 epochs to mitigate overfitting. The model was trained using the train_model function.\n\n\nView Code\nmodel = TransformerClassifier(vocab_size, d_model, num_heads, \n                                hidden_dim, num_layers, out_features, dropout=0).to(device)\ncriterion = CrossEntropyLoss()\nloss = Adam(model.parameters(), lr=lr)\nscheduler = ReduceLROnPlateau(loss, patience=2)\n\ntrain_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler)\n\n\n\naccuracy\n  training           (min:    0.043, max:    1.000, cur:    1.000)\n  validation         (min:    0.035, max:    0.794, cur:    0.785)\nlog loss\n  training           (min:    0.027, max:    3.276, cur:    0.029)\n  validation         (min:    1.023, max:    3.273, cur:    1.071)\n------------------------------\nBest model saved:\nVal Loss: 1.0709 | Val Acc: 0.7847\n✅ Training complete!\nDespite achieving better validation accuracy than the FNN, the Encoder model exhibited a significant gap between training and validation performance, indicating overfitting. During the first 14 epochs, both training and validation losses decreased rapidly, and accuracies increased together. However, over the next 6 epochs, the training loss continued to drop from 1.25 to near 0, while the validation loss plateaued around 1.07. Training accuracy surged to 100%, whereas validation accuracy remained at 78%. This discrepancy highlighted that the model was overfitting.\nNevertheless, the Encoder model achieved a validation accuracy of 78%, which was slightly higher than that of the Feedforward Neural Network. I proceeded to evaluate the model on the test set to determine its final accuracy.\n\n\nEvaluation\n\n\nView Code\naccuracy = test_model(model, test_loader, criterion)\n\n\nTest Loss: 1.2526 | Test Acc: 0.7454\n✅ Testing complete!\nDespite incorporating a more advanced architecture with a multi-head self-attention mechanism, the Encoder model achieved an accuracy of 74.5%, similar to the FNN model. The large gap observed between training and validation performance suggests that this overfitting may be affecting the model’s generalization capabilities.\nWith improved training and validation performance, the model could potentially reach higher accuracy on the test set. Enhancing the model’s performance might involve techniques such as data augmentation, adjusting hyperparameters like embedding size, hidden dimensions, and the number of layers, or experimenting with different optimizers and learning rate schedulers.\nAs with the previous models, I saved the performance metrics for later analysis.\n\n\nView Code\nsave_performance(model_name='Transformer',\n                 architecture='embed_layer-&gt;encoder-&gt;linear_layer',\n                 embed_size='64',\n                 learning_rate='1e-3',\n                 epochs='20',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=80\n                 )"
  },
  {
    "objectID": "portfolio/resume-classifier.html#bert",
    "href": "portfolio/resume-classifier.html#bert",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "BERT",
    "text": "BERT\n\n\n\nThe final model is Bidirectional Encoder Representations from Transformers (BERT), a pre-trained transformer-based model known for its bidirectional context understanding, meaning that it can take into account the context of a word by looking at both the left and right context. This enables BERT to capture a wider range of contextual information, which is particularly useful for tasks such as text classification.\nI created an iterable dataset using the Dataset and DataLoader classes to tokenize resumes with the BERT tokenizer, pad sequences to uniform lengths, and batch the data for training. The model architecture includes the pre-trained BERT base model, a dropout layer, and a linear output layer for classification. Hyperparameters were initialized, and the model was trained using Cross Entropy Loss and the Adam optimizer. Performance was evaluated based on accuracy, consistent with other deep learning models.\n\nTakeaways\n\nThe BERT model achieved a notable accuracy of 91.67% on the test set, significantly outperforming all previous models.\nThe close alignment between the validation and test accuracies demonstrated the model’s strong generalization ability to new, unseen data.\nThe model significantly outperforms all previous models tested so far.\n\n\n\nImport Packages\nIn addition to the standard deep learning packages, I imported three classes from the transformers package:\n\nBertModel: Loads the pre-trained BERT model.\nBertTokenizer: Constructs a BERT tokenizer.\nDataCollatorWithPadding: Creates batches with dynamically padded sequences.\n\nGiven BERT’s unique output format, I also imported custom functions train_BERT and test_BERT, designed specifically to handle BERT’s outputs, including input IDs and attention masks, for evaluating the model’s performance.\n\n\nView Code\nfrom transformers import BertModel, BertTokenizer, DataCollatorWithPadding\n\nfrom src.modules.dl import train_BERT, test_BERT\n\n\n\n\nDataset and DataLoader\nBefore creating the dataset and DataLoader, I initialized the BERT tokenizer and define the pre-trained BERT model. I then constructed the ResumeBertDataset, which handles the tokenization and preparation of resumes for model input.\nUnlike the previous models, I configured the tokenizer using the .encode_plus method. This method returns a dictionary containing the batch encodings, including tokenized input sequences and attention masks. I set padding to False, as dynamic padding will be managed by the data collator, and truncation to True to truncate sequences that exceed the maximum length. The method also adds the special tokens [CLS] and [SEP], required by BERT. I use return_tensors='pt' to return PyTorch tensors. The function ultimately returns a dictionary with the processed input sequences, attention masks, and labels.\n\n\nView Code\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nclass ResumeBertDataset(Dataset):\n    def __init__(self, data, max_length, tokenizer=tokenizer, device=device):\n        super().__init__()\n        self.texts = data.iloc[:,1].values\n        self.labels = torch.tensor(data.iloc[:,0])\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def __len__(self):\n        return len(self.labels)\n\n    def num_class(self):\n        return len(self.labels.unique())\n\n    def __getitem__(self, idx):\n        resumes = self.texts[idx]\n        labels = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            resumes,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            truncation=True,\n            padding=False,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ).to(self.device)\n\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels\n        }\n\n\nI initialized the dataset and set max_length to 512, the maximum token limit that BERT could handle. I then split the dataset into 70% training, 15% validation, and 15% test sets using the random_split function. To manage varying sequence lengths, I used the DataCollatorWithPadding class, which dynamically padded sequences to the maximum length within each batch. Finally, I created DataLoader instances for the training, validation, and test sets, setting the batch size to 16, enabling data shuffling, and applying the data collator for padding.\n\n\nView Code\ndataset = ResumeBertDataset(data, max_length=512)\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15])\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n\n\n\n\nModel Architecture\nThe BertResumeClassifier model comprised the pre-trained BERT base model, a dropout layer, and a linear output layer for resume classification. I utilized the bert-base-uncased pre-trained model to generate contextual embeddings from the input sequences. I extracted these embeddings by indexing the pooler_output key from the output dictionary. The embeddings were then passed through a dropout layer to mitigate overfitting, and subsequently fed into a fully connected linear layer that mapped the embeddings to the desired number of output classes for classification.\n\n\nView Code\nclass BertResumeClassifier(nn.Module):\n    def __init__(self, n_classes: int, dropout: float = 0.01):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )['pooler_output']\n        output = self.dropout(pooled_output)\n        output = self.out(output)\n        return output\n\n\n\n\nHyperparameters and Training\nSince BERT is a pre-trained model with a fixed input size of 512 tokens, there were fewer parameters to set for the model itself. The primary parameter to configure was the number of output classes, which, as before, was obtained from the Dataset class.\nI initialized the model, loss function, optimizer, and set the number of epochs. I used the Cross Entropy Loss function and the Adam optimizer, adjusting the learning rate to 2e-5, which had shown better performance in preliminary tests. The model was trained for 10 epochs.\n\n\nView Code\nn_classes = dataset.num_class()\n\nmodel = BertResumeClassifier(n_classes).to(device)\ncriterion = CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=2e-5)\nepochs = 10\n\ntrain_BERT(model, train_loader, val_loader, epochs, criterion, optimizer)\n\n\n\naccuracy\n    training             (min:    0.050, max:    0.991, cur:    0.991)\n    validation           (min:    0.120, max:    0.933, cur:    0.933)\nlog loss\n    training             (min:    0.081, max:    3.189, cur:    0.081)\n    validation           (min:    0.414, max:    3.002, cur:    0.428)\n------------------------------\nBest model saved:\nVal Loss: 0.4143 | Val Acc: 0.9190\n✅ Training complete!\nThe BERT model demonstrated a significant and steady decrease in both training and validation losses over the 10 epochs, indicating effective learning of data patterns. By the end of training, the training loss decreased from 3.1394 to 0.0589, while the validation loss fell from 2.7347 to 0.5070. This consistent reduction highlighted the model’s ability to capture and generalize the data without overfitting, as evidenced by the small gap between the training and validation losses by the end of the 10th epoch.\nIn terms of accuracy, both training and validation accuracies showed a steady increase throughout the 10 epochs. Training accuracy rose from 0.0774 to 0.9950, while validation accuracy improved from 0.3472 to 0.9028. These results indicated that the model effectively learned the data patterns and generalized well to unseen data. The minimal difference between the final training and validation accuracies underscored the model’s robustness and its ability to avoid overfitting, ensuring reliable performance on new data. The best saved model achieved a validation loss of 0.5070 and an accuracy of 0.9028.\n\n\nEvaluation\nAs before, I evaluate the model using the test_model function using the best saved model.\n\n\nView Code\naccuracy = test_BERT(model, test_loader, criterion)\n\n\nTest Loss: 0.3984 | Test Acc: 0.9167\n✅ Testing complete!\nThe BERT model achieved impressive performance on the test set, reaching an accuracy of 91.67% with a test loss of 0.3984. This result significantly outperformed all previous models tested. As anticipated from the training and validation performances, the model demonstrated robustness and excellent generalization to unseen data. This was further confirmed by the close alignment between test and validation accuracies, both of which represented datasets not previously seen by the model. The similar accuracy between the validation data and the test data indicated that the model could generalize effectively to new resumes and accurately classify them into the correct categories.\nAs with the previous models, I saved its performance using the save_performance function.\n\n\nView Code\nsave_performance(model_name='BERT',\n                 architecture='bert-base-uncased&gt;dropout-&gt;linear_layer',\n                 embed_size='768',\n                 learning_rate='2e-5',\n                 epochs='10',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=90\n                 )"
  },
  {
    "objectID": "portfolio/resume-classifier.html#results-and-discussion",
    "href": "portfolio/resume-classifier.html#results-and-discussion",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nBelow I discuss the performance of the classification models, the reasons behind their varying accuracies, and their implications in industry applications.\nAll four models achieved accuracies above 70%, in large part thanks to effective data preparation including, involving thorough text preprocessing, data balancing, and advanced vectorization techniques. The models can be categorized into two performance groups: Linear SVC and BERT, both achieving around 90% accuracy, and FNN and Transformer, with approximately 70% accuracy. Interestingly, the highest-performing models are at opposite ends of the complexity spectrum, with Linear SVC being the simplest and BERT the most complex, while the lower-performing models have higher complexity than the baseline. The reasons for these performance differences are discussed below.\n\n\nView Code\nevaluation_df = pd.read_json('./output/classifier_performance.json').sort_values(by='accuracy', ascending=False)\n\nax = sns.barplot(evaluation_df, x='model', y='accuracy', hue='model', palette='hls')\n[ax.bar_label(container, fmt=\"%0.2f%%\") for container in ax.containers]\nplt.show()\n\n\n\n\nBERT and Linear SVC: High Accuracy and Robust Performance\n\nLinear SVC: Achieved 84% accuracy, offering competitive performance due to its simplicity and effective feature representation10. using TF-IDF vectors. This simplicity reduces the risk of overfitting and the straightforward feature representation contributes to the model’s high accuracy and fast training times. This model is a reliable option for businesses that prioritize efficiency and simplicity without sacrificing too much accuracy.\nBERT: With an accuracy of 91.67%, BERT demonstrated the highest performance among the models. Its success is attributed to its pre-trained nature and high-quality embeddings. The model was pre-trained on a large corpus and thus has the ability to generate embeddings that encode deep semantic information11. Moreover, its bidirectional nature captures a wide range of contextual information across a long-range dependencies. Businesses with the resources to deploy complex models would benefit from BERT’s superior accuracy.\n\nFNN and Encoder Model: Moderate Performance with Complex Architectures\n\nFeedforward Neural Network: Achieved around 73.15% accuracy. While more advanced than linear models, the FNN lacks the capability to capture sequential dependencies and contextual nuances, which limits its effectiveness in tasks requiring a deeper understanding of text.\nEncoder Model: Achieved 74.54% accuracy. Despite its sophisticated architecture, the Transformer underperformed relative to expectations. This is attributed to its lack of pre-training on a large corpus and possibly insufficient fine-tuning. While Transformers have significant potential, their performance is heavily influenced by pre-training and proper hyperparameter tuning.\n\nIndustry Recommendations:\n\nOpt for BERT for Maximum Accuracy: For businesses prioritizing high accuracy and capable of supporting complex models, BERT is the best choice. Its high performance in classification tasks makes it ideal for scenarios where precision is critical.\nConsider Linear SVC for Simplicity and Efficiency: Linear SVC offers a balance of good performance and computational efficiency. It is suitable for applications needing quick deployment and easy interpretability.\nInvest in Transformer Models with Pre-Training: For those interested in advanced models, investing in pre-training or additional fine-tuning for Transformers could enhance their performance. Transformers show promise but require adequate preparation to reach their full potential.\n\n\n10 To read more on the efficiency of linear classifiers in text classification, see Lin, Y.-C. et al. (2023) ‘Linear Classifier: An Often-Forgotten Baseline for Text Classification’.11 See Devlin, J. et al. (2018) ‘BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding’."
  },
  {
    "objectID": "portfolio/resume-classifier.html#conclusion",
    "href": "portfolio/resume-classifier.html#conclusion",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this project, I tackled the challenge of resume classification using a range of machine learning and deep learning models. The process began with preprocessing the resume data, including tokenizing the resumes and preparing them for model input. I implemented and evaluated four different models: Linear Support Vector Classifier (SVC), Feedforward Neural Network (FNN), Transformer model, and BERT. Each model was trained on the resume dataset and assessed based on accuracy metrics.\nThe evaluation provided valuable insights into the impact of model complexity and feature representation on performance. BERT achieved the highest accuracy of 91.67%, followed by the Linear SVC model at 87.15%. The Feedforward Neural Network and Transformer models recorded accuracies of 73.15% and 74.54%, respectively. BERT’s outstanding performance is attributed to its pre-trained architecture, which effectively captures deep semantic relationships within text. On the other hand, Linear SVC’s strong performance is due to its simplicity and efficiency with high-dimensional data, utilizing interpretable features like TF-IDF vectors.\nTwo important observations arise from these results:\n\n\nState-of-the-art transformer models, such as BERT, combined with transfer learning, deliver superior performance.\nSimple models with high-quality, interpretable feature representations such as Linear SVC with TF-IDF vectors, can also achieve impressive.\n\n\nThese findings reveal that model complexity does not automatically translate to better performance. This is further supported by the fact that the two models with the lowest accuracies have more complex architectures than the baseline.\n\nQuality feature representations and capturing contextual information are more important factors in achieving high performing text classification models.\n\nUltimately, the choice of model depends on the task requirements and available resources. For applications where high performance is critical and ample resources are available, leveraging advanced transformers like BERT with transfer learning is recommended. For scenarios prioritizing simplicity and interpretability, Linear SVC with TF-IDF vectors provides a high-performing, resource-efficient solution."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nHi, I’m Marco Camilo, \n",
    "section": "",
    "text": "Hi, I’m Marco Camilo, \n\nI design, build, and operate deep learning systems that process natural language. I also specialize in data analysis and machine learning. Here are some of the projects in my portfolio:\n\n\nBERT, Encoders and Linear Models for Resume Text Classification This project evaluates the performance of advanced NLP models and vectorization techniques for text classifcation using a resume dataset. Implementing Linear SVC, FNN, Encoder models, and BERT, the project achieved an accuracy of 91.67% with BERT. The project demonstrates how to build efficient preprocessing pipelines, optimize feature representation to enhance resource usage, and develop high-performing text classification models using Scikit-Learn and PyTorch.\n\n\nAirline On-Time Performance The project explores on-time performance trends from 34 years of US domestic flight data, focusing on variations across carriers, routes, airports, and time. The exploratory data analysis (EDA) resulted in a comprehensive report with 35+ data insights and 25+ visualizations, converted into an interactive Streamlit dashboard. The analysis demonstrates how to extract critical performance trends from historical data, enabling stakeholders to make informed decisions and significantly boost operational efficiency in the aviation industry.\n\n\nSentiment-Optimized Stock Price Forecasting Using Modern RNNs This project focuses on optimizing the predictive capabilities of modern RNN models for stock price movements of TSLA, AAPL, and GOOG. The goal is to enhance forecasting accuracy by utilizing historical stock data and news sentiment data. The analysis evaluates the performance of LSTM, GRU, and Attention-CNN-LSTM models, tested with and without sentiment data, to determine their effectiveness in stock price prediction.\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Engineer &\nData Scientist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About\nHello! I’m Marco Camilo, a seasoned professional with a diverse background bridging linguistics, education, and cutting-edge technology.\nDuring my time as a lecturer, I found immense fulfillment in simplifying complex linguistic concepts and guiding students to exceed their own expectations. By analyzing student data—from test scores to speech lab performance and even Duolingo statistics—I employed advanced clustering techniques to tailor instructional strategies and unlock the full potential of every learner.\nMy academic journey, including coursework in machine learning and natural language processing (NLP), has equipped me with practical skills and deep insights into applying state-of-the-art solutions to real-world challenges. Projects in NLP provided compelling opportunities to excel in using cutting-edge technologies, preparing me for impactful contributions in the IT sector.\nMy transition from linguistics to data science and NLP stems from a lifelong fascination with language, mathematics, and algorithms. As a linguist, I delved into the intricate workings of language systems, craving to apply analytical and mathematical methods to linguistic inquiries. This quest led me to the field of NLP, where my passions converge, empowering me to derive innovative solutions to modern challenges—from both linguistic and computational perspectives.\nI am excited to leverage my unique background and expertise to tackle complex problems in NLP and contribute meaningfully to the evolving landscape of technology.\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Engineer &\nData Scientist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  },
  {
    "objectID": "portfolio/stock-sentiment-prediction.html",
    "href": "portfolio/stock-sentiment-prediction.html",
    "title": "Sentiment-Optimized Stock Price Forecasting Using Modern RNNs",
    "section": "",
    "text": "(This write up is currently in progress. In the meantime, please visit the GitHub repo for this project here)"
  },
  {
    "objectID": "portfolio/biodiversity.html",
    "href": "portfolio/biodiversity.html",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "",
    "text": "This project explores biodiversity data from the National Parks Service about endangered species in various parks. In particular, the project delves into the conservation statuses of endangered species to see if there are any patterns regarding the type of species the become endangered. The goal of this project will be to perform an Exploratory Data Analysis and explain findings from the analysis in a meaningful way.\nSources: Both Observations.csv and Species_info.csv was provided by Codecademy.com.\n\n\nThe project will analyze data from the National Parks Service, with the goal of understanding characteristics about species and their conservations status, and the relationship between those species and the national parks they inhabit.\nSome of the questions to be tackled include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nThe project makes use of two datasets. The first dataset contains data about different species and their conservation statuses. The second dataset holds recorded sightings of different species at several national parks for 7 days.\n\n\n\nThe analysis consists of the use of descriptive statistics and data visualization techniques to understand the data. Some of the key metrics that will be computed include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nLastly, the project will revisit its initial goals and summarize the findings using the research questions. This section will also suggest additional questions which may expand on limitations in the current analysis and further guide future analyses on the subject."
  },
  {
    "objectID": "portfolio/biodiversity.html#introduction",
    "href": "portfolio/biodiversity.html#introduction",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "",
    "text": "This project explores biodiversity data from the National Parks Service about endangered species in various parks. In particular, the project delves into the conservation statuses of endangered species to see if there are any patterns regarding the type of species the become endangered. The goal of this project will be to perform an Exploratory Data Analysis and explain findings from the analysis in a meaningful way.\nSources: Both Observations.csv and Species_info.csv was provided by Codecademy.com.\n\n\nThe project will analyze data from the National Parks Service, with the goal of understanding characteristics about species and their conservations status, and the relationship between those species and the national parks they inhabit.\nSome of the questions to be tackled include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nThe project makes use of two datasets. The first dataset contains data about different species and their conservation statuses. The second dataset holds recorded sightings of different species at several national parks for 7 days.\n\n\n\nThe analysis consists of the use of descriptive statistics and data visualization techniques to understand the data. Some of the key metrics that will be computed include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nLastly, the project will revisit its initial goals and summarize the findings using the research questions. This section will also suggest additional questions which may expand on limitations in the current analysis and further guide future analyses on the subject."
  },
  {
    "objectID": "portfolio/biodiversity.html#importing-modules-and-data-from-files",
    "href": "portfolio/biodiversity.html#importing-modules-and-data-from-files",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Importing Modules and Data from Files",
    "text": "Importing Modules and Data from Files\nFirst, we will import the preliminary modules for this project, along with the data from the two separate files provided for this analysis.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\n# Set default figure size\n# figsize = (15,9)\nfigsize = (10,6)\nplt.rcParams['figure.figsize'] = figsize\nsns.set(rc={'figure.figsize':figsize})\n\n# Set default float size\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nobservations = pd.read_csv('observations.csv')\nspecies = pd.read_csv('species_info.csv')\nImport successful"
  },
  {
    "objectID": "portfolio/biodiversity.html#preview-the-data",
    "href": "portfolio/biodiversity.html#preview-the-data",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Preview the Data",
    "text": "Preview the Data\nTo prepare for our exploratory data analysis, we’ll first conduct an initial preview of the data. This will involve sampling a subset of the data and inspecting its structure and characteristics.\n\nspecies.csv\nLet’s begin by examening the species dataset.\ndisplay(\"SAMPLE OF SPECIES DATASET:\")\ndisplay(species.sample(5))\ndisplay(\"INFORMATION ABOUT THE SPECIES DATASET:\")\ndisplay(species.info())\n'SAMPLE OF SPECIES DATASET:'\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n718\nVascular Plant\nPogonia ophioglossoides\nPogonia, Rose Pogonia\nNaN\n\n\n1361\nVascular Plant\nLespedeza stuevei\nTall Lespedeza\nNaN\n\n\n4725\nVascular Plant\nCalycadenia mollis\nSoft Western Rosinweed\nNaN\n\n\n2912\nNonvascular Plant\nThuidium allenii\nAllen's Thuidium Moss\nNaN\n\n\n1929\nVascular Plant\nPicea abies\nNorway Spruce\nNaN\n\n\n\n'INFORMATION ABOUT THE SPECIES DATASET:'\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5824 entries, 0 to 5823\nData columns (total 4 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   category             5824 non-null   object\n 1   scientific_name      5824 non-null   object\n 2   common_names         5824 non-null   object\n 3   conservation_status  191 non-null    object\ndtypes: object(4)\nmemory usage: 182.1+ KB\n\nNone\nThe species dataset shows 5824 entries with four variables:\n\ncategory: taxonomy for each species.\nscientific_name: scientific name of each species.\ncommon_names: common names of each species.\nconservation_status: species’ conservation status.\n\nUpon inspection with .info(), we observe that the conservation_status column contains 191 non-null entries, indicating a high presence of missing values. While the majority of columns may retain their data type as objects, an argument could be made for converting conservation_status to an ordinal variable. However, due to the presence of incomplete conservation statuses and the ambiguity surrounding the ordinal nature of in recovery, we’ll retain it as an object.\n\n\nobservations.csv\nWe’ll now move on to the observations dataset.\ndisplay(\"SAMPLE OF SPECIES DATASET:\")\ndisplay(observations.sample(5))\ndisplay(\"INFORMATION ABOUT THE SPECIES DATASET:\")\ndisplay(observations.info())\n'SAMPLE OF SPECIES DATASET:'\n\n\n\n\n\n\n\n\n\n\nscientific_name\npark_name\nobservations\n\n\n\n\n21462\nLepomis humilis\nYellowstone National Park\n222\n\n\n1305\nSaxifraga odontoloma\nYosemite National Park\n116\n\n\n1307\nPerdix perdix\nYosemite National Park\n162\n\n\n20947\nFraxinus profunda\nBryce National Park\n129\n\n\n10240\nMuhlenbergia andina\nYellowstone National Park\n235\n\n\n\n'INFORMATION ABOUT THE SPECIES DATASET:'\n\n\nRangeIndex: 23296 entries, 0 to 23295\nData columns (total 3 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   scientific_name  23296 non-null  object\n 1   park_name        23296 non-null  object\n 2   observations     23296 non-null  int64 \ndtypes: int64(1), object(2)\nmemory usage: 546.1+ KB\n\nNone\nThe observations dataset consists of three columns:\n\nscientific_name: scientific name of each species.\npark_name: name of the national park species are located in.\nobservations: number of observations in the past 7 days.\n\nBased on the information above, the columns don’t show any missing data, and the data types seem to be appropriate for the analysis."
  },
  {
    "objectID": "portfolio/biodiversity.html#exploratory-data-analysis",
    "href": "portfolio/biodiversity.html#exploratory-data-analysis",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nspecies.csv\nLet’s delve deeper into the species dataset to gain insights into its characteristics and identify any anomalies or patterns. We’ll begin by employing a custom function column_eda() to analyze each column:\ndef column_eda(dataset):\n    cols = list(dataset.columns)\n    for col in cols:\n        print(f'---------------{col}---------------')\n        print(f'Unique values:', dataset[col].nunique(), \n              f'Non-null values: {dataset[col].notnull().sum()}',\n              f'Missing values: {dataset[col].isnull().sum()}\\n', \n              sep='\\n')\n        print(dataset[col].value_counts().head(4))\n\ncolumn_eda(species)\n---------------category---------------\nUnique values:\n7\nNon-null values: 5824\nMissing values: 0\n\ncategory\nVascular Plant       4470\nBird                  521\nNonvascular Plant     333\nMammal                214\nName: count, dtype: int64\n---------------scientific_name---------------\nUnique values:\n5541\nNon-null values: 5824\nMissing values: 0\n\nscientific_name\nCastor canadensis       3\nCanis lupus             3\nHypochaeris radicata    3\nColumba livia           3\nName: count, dtype: int64\n---------------common_names---------------\nUnique values:\n5504\nNon-null values: 5824\nMissing values: 0\n\ncommon_names\nBrachythecium Moss    7\nDicranum Moss         7\nPanic Grass           6\nBryum Moss            6\nName: count, dtype: int64\n---------------conservation_status---------------\nUnique values:\n4\nNon-null values: 191\nMissing values: 5633\n\nconservation_status\nSpecies of Concern    161\nEndangered             16\nThreatened             10\nIn Recovery             4\nName: count, dtype: int64\nThe function shows there are 7 categories of species, 5541 species, 5504 common names and 4 conservation statuses. From the analysis, several insights emerge:\n\nMissing Conversation Statuses: the conservation_status column exhibits a high number of nan values (5633), which could be interpreted as ‘species of no concern’ or requiring ‘no intervention’.\n\nTo address this, we’ll impute the missing values with “No intervention”, expanding the conservation status categories to five.\nprint('Old conservation status:\\n', list(species.conservation_status.unique()))\n\nspecies.conservation_status = species.conservation_status.fillna('No intervention')\n\nprint('New conservation status:\\n', list(species.conservation_status.unique()))\nOld conservation status:\n [nan, 'Species of Concern', 'Endangered', 'Threatened', 'In Recovery']\nNew conservation status:\n ['No intervention', 'Species of Concern', 'Endangered', 'Threatened', 'In Recovery']\n{:start=“2”} 2. Duplicate Entries: there is a discrepancy between the number of unique values of scientific_name and common_names despite all entries having non-null values. This points to the presence of duplicate common names for different species.\nWe’ll confirm this by identifying and examining these duplicates.\nduplicates = species.duplicated().sum()\nprint(f'Overall duplicates (rows): {duplicates}')\n\nrepeated_scientific_names = species.duplicated(subset=['scientific_name']).sum()\nprint(f'Duplicated scientific names: {repeated_scientific_names}')\n\nrepeated_common_names = species.duplicated(subset=['common_names']).sum()\nprint(f'Duplicated common names: {repeated_common_names}')\nOverall duplicates (rows): 0\nDuplicated scientific names: 283\nDuplicated common names: 320\nTo illustrate, we’ll display the most frequent common name alongside its associated scientific names.\ndisplay(species.common_names.value_counts().reset_index()[:5])\ndisplay(species.query(\"common_names == 'Brachythecium Moss'\")[['common_names', 'scientific_name']])\n\n\n\n\ncommon_names\ncount\n\n\n\n\n0\nBrachythecium Moss\n7\n\n\n1\nDicranum Moss\n7\n\n\n2\nPanic Grass\n6\n\n\n3\nBryum Moss\n6\n\n\n4\nSphagnum\n6\n\n\n\n\n\n\n\ncommon_names\nscientific_name\n\n\n\n\n2812\nBrachythecium Moss\nBrachythecium digastrum\n\n\n2813\nBrachythecium Moss\nBrachythecium oedipodium\n\n\n2814\nBrachythecium Moss\nBrachythecium oxycladon\n\n\n2815\nBrachythecium Moss\nBrachythecium plumosum\n\n\n2816\nBrachythecium Moss\nBrachythecium rivulare\n\n\n2817\nBrachythecium Moss\nBrachythecium rutabulum\n\n\n2818\nBrachythecium Moss\nBrachythecium salebrosum\n\n\n\nAs seen above, the most frequent common name is Brachythecium Moss, with a total of 7 different species identified with this name. Organisms in this example all share the same genus (i.e. brachythecium, a genus of moss), but differ in species, thus the different scientific names.\nThis demonstrates instances where multiple species share identical common names but differ in scientific nomenclature.\n{:start=“3”} 3. Duplicate Scientific Names: the presence of duplicate scientific names suggests repeated observations of the same species, since the dataset should report the conservation status of each species, thus one observation per species.\nSince there are no overall duplicates in the dataset (see above), these duplicate names must have some difference at the row level. To confirm this, we’ll print out a sample of duplicates and inspect three random duplicates species, to see what kind of differences are there within the rows themselves.\nduplicated_species = species[species['scientific_name'].duplicated(keep=False)]\n\ndisplay('-------Sample of duplicated scientific names-------')\ndisplay(duplicated_species.head())\n\ndef display_duplicated_species(scientific_name):\n    duplicated_entries = duplicated_species[duplicated_species['scientific_name'] == scientific_name]\n    display(f'-------Duplicated \\'{scientific_name}\\'-------')\n    display(duplicated_entries)\n\nscientific_names_to_check = ['Cervus elaphus', 'Canis lupus', 'Odocoileus virginianus']\nfor scientific_name in scientific_names_to_check:\n    display_duplicated_species(scientific_name)\n'-------Sample of duplicated scientific names-------'\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n6\nMammal\nSus scrofa\nFeral Hog, Wild Pig\nNo intervention\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n10\nMammal\nUrocyon cinereoargenteus\nCommon Gray Fox, Gray Fox\nNo intervention\n\n\n\n\"-------Duplicated 'Cervus elaphus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n3017\nMammal\nCervus elaphus\nRocky Mountain Elk\nNo intervention\n\n\n\n\"-------Duplicated 'Canis lupus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n3020\nMammal\nCanis lupus\nGray Wolf, Wolf\nIn Recovery\n\n\n4448\nMammal\nCanis lupus\nGray Wolf, Wolf\nEndangered\n\n\n\n\"-------Duplicated 'Odocoileus virginianus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n3019\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer, White-Tailed Deer\nNo intervention\n\n\n\nIt seems that both the number of common names and the types of conservation statuses are different for duplicate observations. That is, the same species exhibits both different common names, as well as conservation statuses. To solve the question of duplicates, given the differences in conversation statuses do not affect our question on the likelihood of endangerment given a species’ protection status, I’ll retain the first instance of these duplicates.\nspecies = species.drop_duplicates(subset=['scientific_name'], keep='first')\n\nrepeated_scientific_names = species.scientific_name[species.scientific_name.duplicated()]\nprint(f'Duplicated scientific names: {len(repeated_scientific_names)}\\n')\n\nprint('-------Previously duplicated examples (now clean)-------')\nscientific_names_to_check = ['Cervus elaphus', 'Canis lupus', 'Odocoileus virginianus']\ndisplay(species[species['scientific_name'].isin(scientific_names_to_check)])\nDuplicated scientific names: 0\n\n-------Previously duplicated examples (now clean)-------\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n\n\n\nobservations.csv\nLet’s extend our exploratory analysis to the observations dataset, mirroring the approach applied to the species dataset. We’ll begin by employing the column_eda() function to analyze each column.\ncolumn_eda(observations)\n---------------scientific_name---------------\nUnique values:\n5541\nNon-null values: 23296\nMissing values: 0\n\nscientific_name\nMyotis lucifugus        12\nPuma concolor           12\nHypochaeris radicata    12\nHolcus lanatus          12\nName: count, dtype: int64\n---------------park_name---------------\nUnique values:\n4\nNon-null values: 23296\nMissing values: 0\n\npark_name\nGreat Smoky Mountains National Park    5824\nYosemite National Park                 5824\nBryce National Park                    5824\nYellowstone National Park              5824\nName: count, dtype: int64\n---------------observations---------------\nUnique values:\n304\nNon-null values: 23296\nMissing values: 0\n\nobservations\n84    220\n85    210\n91    206\n92    203\nName: count, dtype: int64\nThe column analysis revelas the following insights. There are 23296 observations of 5541 unique species documented in 4 parks. The number of species (scientific_name) in the observations datset coincides with the number of species in the species dataset. This suggest that the observations dataset contains observations of all species in the species dataset. To confirm this, we’ll check if the scientific_name column in the observations dataset is a subset of the scientific_name column in the species dataset.\nspecies_names = species.scientific_name\nobservations_names = observations.scientific_name\n\nprint(f'Is the observations dataset a subset of the species dataset? {observations_names.isin(species_names).all()}')\nIs the observations dataset a subset of the species dataset? True\nThe result confirms that the observations dataset is a subset of the species dataset, as all species in the observations dataset are also present in the species dataset.\nFurthermore, as observations is a numerical variable, its distribution provides insights into the frequency of species sightings. To better explore this column given its data type, we’ll visualize the distribution using a histogram.\nsns.histplot(x='observations', data=observations, kde=True)\nplt.show()\n\nThe distribution of in the number of observations seems to follow a multimodal distribution, with at least three discernible peaks in the data: one at 80, another at 150, and a third at 250. This may suggest that the overall distribution is a combination of several distributions, grouped by a certain variable. Given the low number of disceernible peaks, this variable might be the park_name variable. That is: the distribution in the number of observations may be influenced by the size of the parks they were made in.\nTo confirm this, we’ll plot the distribution of observations per park using the hue parameter in the seaborn histplot function.\nsns.histplot(x='observations', data=observations, kde=True, hue='park_name')\nplt.show()\n\nAs suspected, the distribution of observations is indeed influenced by the park in which they were made. The peaks in the distribution clearly correspond to each of the four parks in the dataset. This proves that the number of observations is influenced by the park in which they were made.\n\n\nSummary\nTo encapsulate the insights obtained from our Exploratory Data Analysis (EDA), we present the key characteristics of both datasets.\n\nspecies\n\nDataset Overview: the data comprises 5,824 entries with 4 variables—category, scientific_name, common_names, and conservation_status—offering a diverse array of taxonomic information.\nMissing Values: the conservation_status column contains 5,633 missing values, which were imputed with “No intervention” to account for species not under any conservation status.\nDuplicates: the dataset contains no overall duplicates, but does exhibit duplicate scientific names, which were resolved by retaining the first instance of each duplicate.\nCommon Names: the dataset contains 5541 species, with some sharing identical common names but differing in scientific nomenclature.\nConservation Status: the dataset reports 5 conservation statuses, with most species not under any conservation status.\n\n\n\nobservations\n\nDataset Overview: the data consists of 23,296 entries with 3 variables—scientific_name, park_name, and observations—documenting species sightings in 4 national parks over 7 days.\nUnique Species: the dataset contains observations of 5,541 unique species, all of which are present in the species dataset.\nMissing Values: the dataset contains no missing values, with all columns having non-null entries.\nDistribution: the number of observations followed a multimodal distribution, which was influenced by the park in which observations were conducted."
  },
  {
    "objectID": "portfolio/biodiversity.html#analysis-1",
    "href": "portfolio/biodiversity.html#analysis-1",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Analysis",
    "text": "Analysis\nIn this section, we aim to address the questions posed earlier by analyzing the species dataset and later exploring the observations dataset.\n\nQ: What is the distribution of conservation status for animals?\nTo gain insights into the distribution of conservation statuses among animal categories, we begin by aggregating the conservations statuses per species category and calculating both discrete and normalized counts. We then visualize the normalized counts using a stacked bar chart.\ncategory_conservation = pd.crosstab(species['conservation_status'], species['category']).drop(index='No intervention')\ndisplay(category_conservation)\n\ncategory_conservation_norm = pd.crosstab(species['conservation_status'], species['category'], normalize='index').drop(index='No intervention')\ndisplay(category_conservation_norm.style.background_gradient(cmap='Blues', axis=1, vmin=0, vmax=1))\n\nax = category_conservation_norm.plot(kind='bar', stacked=True)\nax.set_xlabel('Conservation Status')\nax.set_ylabel('Number of Species')\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.title(\"Distribution of Species Among Conservation Statuses\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\nAmphibian\nBird\nFish\nMammal\nNonvascular Plant\nReptile\nVascular Plant\n\n\n\n\nconservation_status\n\n\n\n\n\n\n\n\n\nEndangered\n1\n4\n3\n6\n0\n0\n1\n\n\nIn Recovery\n0\n3\n0\n0\n0\n0\n0\n\n\nSpecies of Concern\n4\n68\n4\n22\n5\n5\n43\n\n\nThreatened\n2\n0\n3\n2\n0\n0\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\nAmphibian\nBird\nFish\nMammal\nNonvascular Plant\nReptile\nVascular Plant\n\n\n\n\nconservation_status\n \n \n \n \n \n \n \n\n\nEndangered\n0.066667\n0.266667\n0.200000\n0.400000\n0.000000\n0.000000\n0.066667\n\n\nIn Recovery\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nSpecies of Concern\n0.026490\n0.450331\n0.026490\n0.145695\n0.033113\n0.033113\n0.284768\n\n\nThreatened\n0.222222\n0.000000\n0.333333\n0.222222\n0.000000\n0.000000\n0.222222\n\n\n\n\nThe table and stacked bar chart above reveal several insights about the distribution of conservation status among different categories of species.\nFirstly, the only animal in recovery are birds, of which there are 3 species making up 100% of this status. This points to the fact that birds are the only species in recovery at the time of the dataset. Moreover, mammals, birds and fish are the most endangered species in the dataset, making more than 85% of all endangered species. Furthermore, more than 70% of species of concern consist of birds and vascular plants. Lastly, the threatened status is almost equally distributed among all species categories, except birds, nonvascular plants and reptiles.\nOverall, the distribution of animals among conservations statuses support the following conclusions:\n\nThe most endangered animals in the dataset consist of mammals, birds and fishes.\nBirds are the only species in recovery, with only 3 species documented.\nThe most common conservation status is species of concern, with birds and vascular plants making up the majority of this category.\nThe threatened status is almost equally distributed among amphibians, fish, mammals and vascular plants.\n\n\n\nQ: Are certain types of species more likely to be endangered?\nThe next question concerns the relation between species and their conservation status. To answer this question requires establishing a definition of likelihood for endangerment. Given protection measures are not documented in the dataset, we can only establish a definition based on the available variables. Therefore, we consider species to be more likely to be engangered if they are classified as endangered, threatened, or species of concern and if no protection measures are placed in response to their endangerment.\nTo answer this question, we create a new protected column with True for all conservations statuses that are not No intervention nor In recovery. We then calculate the relative frequencies of protected and protected species per category. We visualize the results then using a bar chart.\nspecies['protected'] = species.conservation_status.isin(['No intervention', 'In Recovery'])\n\ncategory_protections = pd.crosstab(species['category'], species['protected'], normalize='index')\ndisplay(category_protections)\n\nax = sns.barplot(data = category_protections, y = category_protections.iloc[:, 0]*100, x = 'category')\nax.bar_label(ax.containers[0], fmt=\"%0.2f%%\")\nplt.title('Percentage of Likely Endangerement per Species Category')\nplt.ylabel('Percentage Not Protected')\nplt.xlabel('Category')\nplt.show()\n\n\n\nprotected\nFalse\nTrue\n\n\n\n\ncategory\n\n\n\n\nAmphibian\n0.09\n0.91\n\n\nBird\n0.15\n0.85\n\n\nFish\n0.08\n0.92\n\n\nMammal\n0.17\n0.83\n\n\nNonvascular Plant\n0.02\n0.98\n\n\nReptile\n0.06\n0.94\n\n\nVascular Plant\n0.01\n0.99\n\n\n\n\nBased on the information from the bar chart, we can see that mammals and birds have the highest percentage of no protection, with roughly 17% and 15% of species exhibiting some level of engangered, respectively. This suggests that mammals and birds are the most likely to be endangered among the categories.\n\n\nQ: Are the differences between species and their conservation status significant?\nThe question of statistical significance for categorical variables is answered in statistics by use of the chi-square test.\nCrosstabulating both variables would yield a complex result, thus it’s better to break down the question into pairs of species categories. Since based on the previous question mammals are the most likely category to be endangered, we’ll compare the significance of other category differences with mammals.\nWe’ll start by permutating the pairs of categories with mammals. Then I’ll loop over this list to perform the chi-square tests for each pair and plot the p-values to find the statistically significant differences among category pairs.\ncategories = list(species.category.unique())\ncombinations_mammal = [['Mammal', i] for i in categories][1:]\n\ncategory_protections_counts = pd.crosstab(species['category'], species['protected'])\n\nsignificance_data = {'Animal Pair': [], 'p-value': []}\nfor pair in combinations_mammal:\n  contingency_table = category_protections_counts.loc[pair]\n  chi2, pval, dof, expected = chi2_contingency(contingency_table)\n\n  significance_data['Animal Pair'].append(f'{pair[0]} vs {pair[1]}')\n  significance_data['p-value'].append(pval)\n\nsign_data = pd.DataFrame(significance_data)\nsign_data['p-value'] = sign_data['p-value']*100\n# display(sign_data)\n\n# Plot\nplt.subplots(figsize=(10,5))\nax =sns.barplot(data = sign_data, x = 'Animal Pair', y = 'p-value')\nplt.title('Statistical Significance of Protection Statuses per Animal\\n(difference with mammals)')\nplt.axhline(5, color='red', linestyle='--')\nax.set_xlabel(\"\")\nax.set_ylabel('p-value\\n(alpha = 5%)')\nplt.xticks(rotation=45)\nax.bar_label(ax.containers[0], fmt=\"%0.2f%%\")\nplt.show()\n\nThe above graph illustrates the p-values for the chi-square tests performed for each animal category against mammals. Given an alpha of 5%, the analysis shows that birds and amphibians display no statistically significant differences in their conservations statuses compared with mammals. However, all other categories such as reptiles, fishes and plants show statistically significant differences in their conservation statuses when comapred to mammals. This means that the conservation statuses of these categories are significantly different from mammals.\n\n\nQ: Which species were spotted the most at each park?\nLastly, we explore the observations dataset to identify the most frequently spotted species in each park.\nSince the dataset doesn’t include common names, we’ll map the common names from the species dataset to the scientific names in the observations dataset. Then, we’ll aggregate the data by park and by species, summing their observations to identify the most frequently spotted species in each park.\nmerged_df = observations.merge(species[['category', 'scientific_name', 'common_names']], how='left').drop_duplicates()\nmerged_df_grouped = merged_df.groupby(['park_name', 'scientific_name', 'common_names']).observations.sum().reset_index()\nmerged_df_grouped = merged_df_grouped.loc[merged_df_grouped.groupby('park_name')['observations'].idxmax()].sort_values(by = 'observations', ascending=False)\n\ndisplay(merged_df_grouped.head())\n\n\n\n\n\n\n\n\n\n\n\npark_name\nscientific_name\ncommon_names\nobservations\n\n\n\n\n13534\nYellowstone National Park\nHolcus lanatus\nCommon Velvet Grass, Velvetgrass\n805\n\n\n19178\nYosemite National Park\nHypochaeris radicata\nCat's Ear, Spotted Cat's-Ear\n505\n\n\n1359\nBryce National Park\nColumba livia\nRock Dove\n339\n\n\n10534\nGreat Smoky Mountains National Park\nStreptopelia decaocto\nEurasian Collared-Dove\n256\n\n\n\nBased on the aggregation above, in Yellowstone National Park, the species Holcus lanatus was the most commonly observed, with a total of 805 sightings. Meanwhile, Hypochaeris radicata was the predominant species in Yosemite National Park, with 505 observations. In Bryce National Park, Columba livia garnered the highest number of sightings, totaling 339. Finally, in Great Smoky Mountains National Park, Streptopelia decaocto was the most frequently spotted species, with 256 observations."
  },
  {
    "objectID": "portfolio/biodiversity.html#conclusions",
    "href": "portfolio/biodiversity.html#conclusions",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Conclusions",
    "text": "Conclusions\nThis project set out to explore biodiversity data from the National Parks Service, focusing on endangered species and their conservation statuses. Through a detailed exploratory data analysis, several key findings emerged, shedding light on the distribution of conservation statuses among different species categories, the likelihood of species endangerment, the significance of differences in conservation statuses among species categories, and the most frequently spotted species in each national park.\n\nDistribution of Conservation Statuses\nThe analysis revealed that mammals, birds, and fishes are the most endangered species categories, making up the majority of the endangered conservation status. Birds were the only category with species classified as in recovery, indicating a unique conservation status among all the categories. Out of 178 species marked with some conservation status other than no intervention, most species are under the status of species of concern, especially birds and vascular plants.\n\n\nLikelihood of Species Endangerment\nMammals and birds emerged as the most likely categories to be endangered, with approximately 17% and 15% of species not classified as either in recovery or no intervention. Without any protection measures, this suggests a higher vulnerability to endangerment among mammals and birds compared to other species categories.\n\n\nSignificance of Conservation Status Differences\nStatistical significance testing showed that birds and amphibians did not exhibit statistically significant differences in their conservation statuses compared with mammals. However, all other categories, including reptiles, fishes, and plants, displayed significant differences in conservation statuses compared with mammals. This highlights the importance of considering species-specific conservation measures based on their unique characteristics.\n\n\nMost Frequently Spotted Species\nThe analysis identified the most frequently spotted species in each national park. Species such as common velvet grass, a vascular plant, in Yellowstome National Park. Moreover, doves were the most commonly observed species both in Bryce and Great Smoky Mountains National Parks. Furthermore, the most observed species Yosemite National Park was the cat’s ear plant. This findings are examples of the rich biodiversity present in national parks.\nIn conclusion, this project contributes to our understanding of endangered species and their conservation statuses, highlighting the need for targeted conservation efforts to protect vulnerable species and preserve biodiversity in national parks. Further research could explore additional factors influencing species endangerment and conservation strategies tailored to specific species categories. By understanding and honoring the unique needs of each species category, we can forge a path towards sustainable coexistence and ensure the enduring legacy of our national parks for generations to come."
  }
]