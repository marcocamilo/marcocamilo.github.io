[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "NLP\n\n\n\n\n\nBERT, Encoders and Linear Models for Resume Text Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplores the performance of advanced NLP models and vectorization techniques on topic modeling, text classification and document similarity.\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\nAirline On-Time Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis project analyzes on-time performance trends from 34 years of US domestic flight data, focusing on variations across carriers, routes, airports, and time. The goal is to conduct a comprehensive exploratory data analysis (EDA) to address key industry questions and translate the findings into an interactive visualization dashboard.\n\n\n\n\n\nBiodiversity, Endangerement and Conversation in Data from National Parks Service\n\n\n\n\n\n\n\n\n\n\n\n\nEmbark on a captivating exploration of biodiversity with this data science project, delving into the conservation statuses of endangered species across national parks. Through meticulous analysis, uncover profound insights into the distribution of endangered species, their likelihood of endangerment, and the most frequently spotted species in each park, illuminating the intricate dynamics of wildlife preservation and ecological sustainability.\n\n\n\n\nMachine Learning\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Data Scientist &\nComputational Linguist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nHi, I’m Marco Camilo, \n",
    "section": "",
    "text": "Hi, I’m Marco Camilo, \n\nI design, build, and operate deep learning systems that process natural language. I also specialize in data analysis and machine learning. Here are some of the projects in my portfolio:\n\n\nBERT, Encoders and Linear Models for Resume Text Classification Explores the performance of advanced NLP models and vectorization techniques on topic modeling, text classification and document similarity.\n\n\nAirline On-Time Performance This project analyzes on-time performance trends from 34 years of US domestic flight data, focusing on variations across carriers, routes, airports, and time. The goal is to conduct a comprehensive exploratory data analysis (EDA) to address key industry questions and translate the findings into an interactive visualization dashboard.\n\n\nBiodiversity, Endangerement and Conversation in Data from National Parks Service Embark on a captivating exploration of biodiversity with this data science project, delving into the conservation statuses of endangered species across national parks. Through meticulous analysis, uncover profound insights into the distribution of endangered species, their likelihood of endangerment, and the most frequently spotted species in each park, illuminating the intricate dynamics of wildlife preservation and ecological sustainability.\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Data Scientist &\nComputational Linguist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  },
  {
    "objectID": "projects/resume-analyzer.html#executive-summary",
    "href": "projects/resume-analyzer.html#executive-summary",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Executive Summary",
    "text": "Executive Summary\nThe project explores the performance of advanced NLP models and vectorization techniques specifically for text classification using a dataset of resumes from various job categories. The algorithms tested include Linear SVC, Feedforward Neural Networks (FNN), Encoder models, and BERT, implemented using Scikit-Learn and PyTorch. The project aims to compare the performance of these algorithms on text classification and evaluate their effectiveness in analyzing resume data.\n\nKey Findings\n\nKey Factors in Model Performance: quality of the feature representations and the ability to capture contextual information across dependencies.\nModel performance\n\nBERT: Best performing model with an accuracy of 91.67%. Showcases the effectiveness of pre-trained models and transfer learning.\nLinear SVC: Achieved an accuracy of 87.15% with TF-IDF vectors. Attributed to the model’s simplicity and use of effective feature representation.\nEncoder Model: Achieved an accuracy of 74.54%. Suggests the need for further fine-tunning, give the difference with its cousin transformer model.\nFeedforward Neural Network: achieved an accuracy of 73.15%, the lowest of the four models. Indicates struggle with sequential dependencies and contextual nuances.\n\nPreprocessing and Vectorization: effective text preprocessing and robust vectorization techniques significantly enhanced overall model performance (&gt; 70%)."
  },
  {
    "objectID": "projects/resume-analyzer.html#dataset",
    "href": "projects/resume-analyzer.html#dataset",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Dataset",
    "text": "Dataset\nThe project primarily makes use of the Resume Dataset sourced from LiveCareer, available at Kaggle. The dataset consists of a collection of more than 2400 resumes in both string and HTML format, along with their respective labeled categories. The dataset consists of the following variables:\n\n\n\n\nID: Unique identifier for each resume\nResume_str: Text content of the resume\nResume_html: HTML content of the resume\nCategory: Categorized field of the resume (e.g. Information-Technology, Teacher, Advocate, Business-Development, Healthcare)"
  },
  {
    "objectID": "projects/resume-analyzer.html#preprocessing",
    "href": "projects/resume-analyzer.html#preprocessing",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Preprocessing",
    "text": "Preprocessing\nData preprocessing primarily involved two tasks: text preprocessing and data rebalancing.\nFor text cleaning, I used a custom preprocessing function that streamlines multiple text cleaning and preprocessing operations. The function is highly flexible, offering tunable parameters to adapt the preprocessing pipeline to include key operations such as lowercase conversion, HTML decoding, email and URL removal, special character removal, contraction expansion, custom regex cleaning, as well as tokenization, stemming, lemmatization, and stopwords removal.\nIn particular, I enhanced the text preprocessing by removing noise from the text data, such as non-existen words and frequent, document-specific stopwords like months, section headings and other common words in resumes.\n\n\nView Code\nimport re\nfrom bs4 import BeautifulSoup\nfrom unidecode import unidecode\nimport contractions\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\ndef preprocessing(\n    text, \n    tokenize=False,\n    stem=False,\n    lem=False,\n    html=False,\n    exist=False,\n    remove_emails=True,\n    remove_urls=True,\n    remove_digits=True,\n    remove_punct=True,\n    expand_contractions=True,\n    remove_special_chars=True,\n    remove_stopwords=True,\n    lst_stopwords=None,\n    lst_regex=None\n) -&gt; str | list[str]:\n    \n    # Lowercase conversion\n    cleaned_text = text.lower()\n\n    # HTML decoding\n    if html:\n        soup = BeautifulSoup(cleaned_text, \"html.parser\")\n        cleaned_text = soup.get_text()\n    \n    # Remove Emails\n    if remove_emails:\n        cleaned_text = re.sub(r\"([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)\", \" \", cleaned_text)\n    \n    # URL removal\n    if remove_urls:\n        cleaned_text = re.sub(r\"(http|https|ftp|ssh)://[\\w_-]+(?:\\.[\\w_-]+)+[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-]?\", \" \", cleaned_text)\n    \n    # Remove escape sequences and special characters\n    if remove_special_chars:\n        cleaned_text = re.sub(r\"[^\\x00-\\x7f]\", \" \", cleaned_text)\n        cleaned_text = unidecode(cleaned_text)\n    \n    # Remove multiple characters\n    cleaned_text = re.sub(r\"(.)\\1{3,}\", r\"\\1\", cleaned_text)\n    \n    # Expand contractions\n    if expand_contractions:\n        cleaned_text = contractions.fix(cleaned_text)\n        cleaned_text = re.sub(\"'(?=[Ss])\", \"\", cleaned_text)\n    \n    # Remove digits\n    if remove_digits:\n        cleaned_text = re.sub(r\"\\d\", \" \", cleaned_text)\n    \n    # Punctuation removal\n    if remove_punct:\n        cleaned_text = re.sub(\"[!\\\"#$%&\\\\'()*+\\,-./:;&lt;=&gt;?@\\[\\]\\^_`{|}~]\", \" \", cleaned_text)\n    \n    # Line break and tab removal\n    cleaned_text = re.sub(r\"[\\n\\t]\", \" \", cleaned_text)\n    \n    # Excessive spacing removal\n    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n    \n    # Regex (in case, before cleaning)\n    if lst_regex: \n        for regex in lst_regex:\n            compiled_regex = re.compile(regex)\n            cleaned_text = re.sub(compiled_regex, '', cleaned_text)\n\n    # Tokenization (if tokenization, stemming, lemmatization or custom stopwords is required)\n    if stem or lem or remove_stopwords or tokenize:\n        if isinstance(cleaned_text, str):\n            cleaned_text = cleaned_text.split()\n        \n        # Remove stopwords\n        if remove_stopwords:\n            if lst_stopwords is None:\n                lst_stopwords = set(stopwords.words('english'))\n            cleaned_text = [word for word in cleaned_text if word not in lst_stopwords]\n\n        # Remove non-existent words\n        if exist:\n            english_words = set(words.words())\n            cleaned_text = [word for word in cleaned_text if word in english_words]\n\n        # Stemming\n        if stem:\n            stemmer = PorterStemmer()\n            cleaned_text = [stemmer.stem(word) for word in cleaned_text]\n\n        # Lemmatization\n        if lem:\n            lemmatizer = WordNetLemmatizer()\n            cleaned_text = [lemmatizer.lemmatize(word) for word in cleaned_text]\n        \n        if not tokenize:\n            cleaned_text = ' '.join(cleaned_text)\n\n    return cleaned_text\n\n\nNext, I prepared the Category variable in the resumes dataset by converting it to numerical representation using LabelEncoder from Scikit-Learn, as these algorithms require numerical feature representations. After cleaning, I saved the text separately for topic modeling and document similarity tasks, where dataset balancing is unnecessary.\nIn particular for text classification, I addressed the imbalance in underrepresented categories by employing random resampling with Scikit-Learn’s resample method. This approach ensures that the model receives equitable representation across all categories, leading to enhanced accuracy and reduced bias, thereby improving overall performance."
  },
  {
    "objectID": "projects/resume-analyzer.html#linear-svc",
    "href": "projects/resume-analyzer.html#linear-svc",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Linear SVC",
    "text": "Linear SVC\n\n\n\n1 Linear Support Vector Classifier (SVC) is a classification algorithm that seeks to find the maximum-margin hyperplane, that is, the hyperplane that most clearly classifies observations2 Truncated Singular Value Decomposition (SVD) is a dimensionality reduction technique that decomposes a matrix into three smaller matrices, retaining only the most significant features of the original matrix.For this first model, I train a baseline Linear SVC1 using the TF-IDF vectors. I then performance Latent Semantic Analysis (LSA) by applying Truncated Singular Value Decomposition (SVD)2 to the TF-IDF matrix, to reduce the matrix size to a lower dimensional space. I evaluate the performance of both models, which will serve as the baseline to compare with more complex models in the upcoming sections.\n\nTakeaways\n\nLinear SVC achieved an accuracy of 84% on the test set.\nLinear SVC with Truncated SVD achieved an accuracy of 81% on the test set with only 5% of the original number of features.\nThe baseline model achieved a high level of accuracy and is more cost-effective when reducing the dimensionality of the feature matrix.\n\n\n\nImport Packages and Data\nAside from standard libraries, I import two custom functions: classifier_report to generate a classification report and confusion matrix, and save_performance, which saves the performance metrics of the model to a JSON file for later analysis. I also load the pre-trained Label Encoder to label the encoded categories in upcoming plots.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import TruncatedSVD\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom src.modules.ml import classifier_report\nfrom src.modules.utils import save_performance\n\n# Import the data with engineered features\ndf = pd.read_parquet('./data/3-processed/resume-features.parquet')\n\n# Label model to label categories\nle = joblib.load('./models/le-resumes.gz')\n\n\n\nBaseline LinearSVC\nI split the dataset into 70% training and 30% testing sets. I use the tfidf_vectors column as the feature matrix and stack the vectors into a single matrix using np.vstack. For the target variable I set the encoded Category column. I extract the variables using the .values method, which extracts the variables into a NumPy array, improving performance. To verify the split, I print the shape of the training and testing sets.\n\n\nView Code\nX = np.vstack(df['tfidf_vectors'].values)\ny = df['Category'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n\nprint(f\"Training set: {X_train.shape}, {y_train.shape}\")\nprint(f\"Testing set: {X_test.shape}, {y_test.shape}\")\n\n\nTraining set: (2016, 11618), (2016,)\nTesting set: (864, 11618), (864,)\nThen, I train a Linear SVC model with default hyperparameters and evaluate the model’s performance using the classifier_report function, which generates a classification report and confusion matrix.\n\n\nView Code\nsvc = LinearSVC(dual=\"auto\")\naccuracy = classifier_report([X_train, X_test, y_train, y_test], svc, le.classes_, True)\n\n\nSVC accruacy score 87.15%\n\nThe model achieves an 87% accuracy on the test set, which is already very good for a baseline model. However, TF-IDF vectors often result in sparse, high-dimensional representations with a low information ratio. To address this issue, I recur to Truncated Singular Value Decomposition (SVD).\n\n\nLinearSVC with Truncated SVD\n\n\n\n3 For an in depth explanation, see Manning, C.D., Raghavan, P. and Schütze, H. (2008) ‘Matrix decompositions and latent semantic indexing’, Introduction to information retrieval, 1.Transforming TF-IDF matrices by means of Truncated SVD is known as Latent Semantic Analysis (LSA). It takes the \\(n\\) largest eigenvalues and transforms the original matrix to capture the most significant semantic relationships between terms and documents, while discarding noise and low-information features3.\nI use TruncatedSVD to reduce the number of components to 500, which surprisingly is less than 5% of the original number of features. Below I apply the transformation and split the transformed feature matrix into training and testing sets before training the new model.\n\n\nView Code\nt_svd = TruncatedSVD(n_components=500, algorithm='arpack')\nX_svd = t_svd.fit_transform(X)\n\n\nX_train_svd, X_test_svd, y_train_svd, y_test_svd = train_test_split(X_svd, \n                                                                    y, \n                                                                    test_size=0.30, \n                                                                    random_state=42)\n\n\nprint(f\"Training set: {X_train_svd.shape}, {y_train_svd.shape}\")\nprint(f\"Testing set: {X_test_svd.shape}, {y_test_svd.shape}\")\n\n\nTraining set: (2016, 500), (2016,)\nTesting set: (864, 500), (864,)\nI now train a new Linear SVC model using the SVD-transformed feature matrix and generate a classification report and confusion matrix.\n\n\nView Code\nsvc_svd = LinearSVC(dual=\"auto\")\naccuracy = classifier_report([X_train_svd, X_test_svd, y_train_svd, y_test_svd], svc_svd, le.classes_, True)\n\n\nSVC accruacy score 84.94%\n\nThe model achieved an 84% accuracy on the test set, which is slightly lower than the baseline model. However, these results come from a model trained on less than 5% of the original number of features while still retaining a high level of accuracy. This demonstrates the sparcity of the TF-IDF vectors, while also showing that the model can still achieve an excellent level of accuracy with a substantially reduced feature matrix.\nBefore moving onto the next model, I save the performance metrics of the baseline model for later comparison.\n\n\nView Code\nsave_performance(model_name='LinearSVC',\n                 architecture='default',\n                 embed_size='n/a',\n                 learning_rate='n/a',\n                 epochs='n/a',\n                 optimizer='n/a',\n                 criterion='n/a',\n                 accuracy=87.15\n                 )"
  },
  {
    "objectID": "projects/resume-analyzer.html#feedforward-neural-network",
    "href": "projects/resume-analyzer.html#feedforward-neural-network",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Feedforward Neural Network",
    "text": "Feedforward Neural Network\n\n\n\n4 Feedforward Neural Networks (FNN) are a type of neural networks where information moves in only one direction—forward—from the input nodes, through hidden layers, and to the output nodesThe next model is a Feedforward Neural Network (FNN)4 built using PyTorch. I create an iterator for the dataset using the DataLoader class, which tokenizes and numericalized the resumes, dynamically pads the sequences, and batches the data for training, saving memory and computation time. Then, I construct a simple neural network architecture with an embedding layer, followed by three fully connected layers with ReLU activation functions. The model is trained using the Adam optimizer and CrossEntropyLoss criterion, and its performance is evaluated on the test set using accuracy as the evaluation metric.\n\nTakeaways\n\nThe Feedforward Neural Network achieved an accuracy of 73.15% with a loss of 1.1444 on the test set.\nPerformance suggests minimal overfitting, given the small gap between training and validation.\nModel demonstrates robust generalization, with test accuracy aligning closely with validation accuracy.\n\n\n\nImport Packages and Data\nAside from the standard PyTorch and pandas imports, I also import three custom functions:\n\ntrain_model: trains the model based on the hyperparameters and data provided, prints the training and validation loss and accuracy in real-time, and saves the best model based on the iteration with the lowest validation loss. It also provides an option to visualize the training progress using the PlotLosses library or matplotlib.\ntest_model: evaluates the model on the test set using the best model saved during training and returns the testing accuracy.\nsave_performance: saves the performance metrics of the model to a json file for future analysis.\n\n\n\nView Code\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom livelossplot import PlotLosses\nfrom tqdm import tqdm\n\nfrom src.modules.dl import train_model, test_model\nfrom src.modules.utils import save_performance\n\ndata = pd.read_parquet('./data/3-processed/resume-features.parquet', columns=['Category', 'cleaned_resumes'])\n\n\nWhen training deep learning models, I always code the option to use a GPU if available and set the device variable accordingly. This not only allows the model to leverage the parallel computing if available, but also makes the code reproducible across different device setups. The snippet below checks if a GPU is available and sets the device variable accordingly, which is later used by the DataLoader and model.\n\n\nView Code\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device('cpu')\nprint(\"Using {}.\".format(device))\n\n\nUsing cpu.\n\n\nDataset and DataLoader\nBefore constructing the Dataset class, I define a tokenization function that instantiates the tokenizer, tokenizes the text data, and builds a vocabulary using PyTorch’s get_tokenizer and build_vocab_from_iterator functions. The function returns the tokenized texts to be indexed by the DataLoader during training, and the vocabulary, which will be used to determine the vocabulary size.\n\n\nView Code\ndef tokenization(texts, tokenizer_type='basic_english', specials=['&lt;unk&gt;'], device=device):\n    # Instantiate tokenizer\n    tokenizer = get_tokenizer(tokenizer_type)\n    # Tokenize text data\n    tokens = [tokenizer(text) for text in texts]\n    # Build vocabulary\n    vocab = build_vocab_from_iterator(tokens, specials=specials)\n    # Set default index for unknown tokens\n    vocab.set_default_index(vocab['&lt;unk&gt;'])\n\n    # Convert tokenized texts to a tensor\n    tokenized_texts = [torch.tensor([vocab[token] for token in text], dtype=torch.int64, device=device) for text in tokens]\n\n    return tokenized_texts, vocab\n\n\nNext. I construct the ResumeDataset iterator, which preprocesses the text data using the tokenization function and indexes samples for the DataLoader during training. The __len__ method returns the length of the dataset, the vocab_size method returns the size of the vocabulary, the num_class method returns the number of unique classes in the dataset, and the __getitem__ method returns a sample of text and label from the dataset.\n\n\nView Code\nclass ResumeDataset(Dataset):\n    # Dataset initialization and preprocessing\n    def __init__(self, data):\n        # Initialize dataset attributes\n        super().__init__()\n        self.text = data.iloc[:,1]\n        self.labels = data.iloc[:,0]\n        \n        self.tokenized_texts, self.vocab = tokenization(self.text)\n\n    # Get length of dataset\n    def __len__(self):\n        return len(self.labels)\n\n    # Get vocabulary size\n    def vocab_size(self):\n        return len(self.vocab)\n\n    # Get number of classes\n    def num_class(self):\n        return len(self.labels.unique())\n\n    # Get item from dataset\n    def __getitem__(self, idx):\n        sequence = self.tokenized_texts[idx]\n        label = self.labels[idx]\n        return sequence, label\n\n\nI also define a collate_fn function to use dynamic padding when batching the data. Dynamic padding is a technique used to pad sequences to the length of the longest sequence in a batch, as opposed to the longest sequence in the entire dataset. Because the model expects uniform dimensions to perform operations, sequences need to be padded to ensure each one has the same length. Dynamic padding allows the model to process sequences of varying lengths more efficiently, saving memory and computation time.\n\n\nView Code\ndef collate_fn(batch):\n    sequences, labels = zip(*batch)\n    # Pad sequences to the longest sequence in the batch\n    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n    # Convert labels to tensor\n    labels = torch.tensor(labels, dtype=torch.long)\n    return sequences_padded, labels\n\n\nFinally, I instantiate the ResumeDataset class and split the dataset into 70% training, 15% validation, and 15% test sets using the random_split function. I create DataLoader iterators for each set, using the collate_fn function to apply dynamic padding to the sequences.\n\n\nView Code\ndataset = ResumeDataset(data)\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15])\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\n\n\n\nModel Architecture\n\n\n\nThe model is a simple Feedforward Neural Network with an embedding layer, followed by two fully connected layers with ReLU activation functions, and a final fully connected layer with the number of classes as the output size. The model architecture is defined in the SimpleNN class, which takes the vocabulary size, embedding size, number of classes as parameters. The expansion_factor is defined to determine the hidden dimension size, here set to 2.\nThe EmbeddingBag function efficiently computes the embeddings by performing a two-step operation: first, it creates embeddings for the input indices, adn then reduces the embedding output using the mean across the sequence dimension. This is useful for sequences of varying lengths, as it allows the model to process them more efficiently.\n\n\nView Code\nclass SimpleNN(nn.Module):\n    def __init__(self, vocab_size, embed_size, num_class, expansion_factor=2, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_size, sparse=False)\n        self.hidden_dim = embed_size * expansion_factor\n        self.layer1 = nn.Linear(embed_size, self.hidden_dim)\n        self.layer2 = nn.Linear(self.hidden_dim, embed_size)\n        self.layer3 = nn.Linear(embed_size, num_class)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = F.relu(self.layer1(x))\n        x = self.dropout(x)\n        x = F.relu(self.layer2(x))\n        x = self.dropout(x)\n        x = self.layer3(x)\n        return x\n\n\n\n\nHyperparameters and Training\nBefore training, I set the hyperparameters for training the neural network. The vocabulary size and number of classes are obtained from the ResumeDataset class.The embedding size is set to 60 and the learning rate is set to 1e-3. The model is trained for 40 epochs.\n\n\nView Code\nvocab_size = dataset.vocab_size()\nnum_class = dataset.num_class()\nembed_size = 60\nlr=0.001\nepochs = 40\n\n\nI then instantiate the model, sending it to the available device, and define the loss function and optimizer. The loss function is set to CrossEntropyLoss, which is suitable for multi-class classification tasks. The optimizer is Adam, an adaptive learning rate optimization algorithm well-suited for training deep neural networks. In addition, I define a learning rate scheduler that reduces the learning rate by a factor of 0.1 if the validation loss does not improve for patience number of epochs. This prevents the model from overfitting and improves generalization. The model and hyperparameters are then passed for training using the train_model function5.\n5 During fine-tunning, I found the model converged with better accuracy when using a dropout rate of 0.4.To visualize the training progress, I set the visualize parameter to ‘liveloss’, which uses the PlotLosses library to create a dynamicallly updating plot that visulalized the training and validation loss and accuracy in real-time. This allows me to monitor the model’s performance and make adjustments to the hyperparameters if necessary.\n\n\nView Code\nmodel = SimpleNN(vocab_size, embed_size, num_class, dropout=0.4).to(device)\ncriterion = CrossEntropyLoss()\nloss = Adam(model.parameters(), lr=lr)\nscheduler = ReduceLROnPlateau(loss, patience=2)\n\ntrain_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler, visualize='liveloss')\n\n\n\naccuracy\n  training           (min:    0.036, max:    0.712, cur:    0.707)\n  validation         (min:    0.037, max:    0.694, cur:    0.685)\nlog loss\n  training           (min:    0.873, max:    3.187, cur:    0.921)\n  validation         (min:    1.274, max:    3.184, cur:    1.330)\n------------------------------\nBest model saved:\nVal Loss: 1.3300 | Val Acc: 0.6852\n✅ Training complete!\nStarting with the loss, the training and validation losses decrease steadily until 30 epochs, showing that the model is effectively learning the patterns in the data. After this, the training loss continues to decrease, while the validation loss plateaus. However, the model finishes with only a small gap between the training and validation loss, with the training loss at 0.92 and the validation loss at 1.33. The small size of the gap indicates that the model is not overfitting the training data and generalizes well to unseen data.\nRegarding accuracy, the training and validation accuracy increase steadily until 40 epochs, after which they converge. At convergence, the training accuracy is slightly higher than the validation accuracy, with the model achieving a training accuracy of 71% and a validation accuracy of 69%. This small difference between the training and validation accuracy indicates that the model does not overfit the training data and generalizes well to unseen data. The best saved model has a validation loss of 1.33 and a validation accuracy of 68.52%.\n\n\nEvaluation\nAfter training the model, I evaluate its performance on the test set using the test_model function. The function takes the trained model, test data loader, and criterion as input and returns the accuracy of the model on the test set.\n\n\nView Code\naccuracy = test_model(model, test_loader, criterion)\n\n\nTest Loss: 1.1444 | Test Acc: 0.7315\n✅ Testing complete!\nThe Feedforward Neural Network model achieves an accuracy of 73.15% and a loss of 1.1444 on the test set. As expected from the training and validation plots, the model performs reasonably well on unseen data, with the test accuracy aligning closely with the validation accuracy observed during training. The consistency between validation and test accuracies suggests that the model successfully generalizes to new data and demonstrates robustness in its predictions.\nCompared to the baseline model, the Feedforward Neural Network model achieved a lower accuracy on the test set. However, the model’s performance is still quite impressive considering the simplicity of the architecture. The model’s performance can likely be improved by introducing more advanced regularization techniques, using pre-trained word embeddings6, or increasing the complexity of the model architecture. This last point is what I explore in the next section, where I implement a more advanced model architecture using a Transformer-based neural network that leverages self-attention mechanisms to capture long-range dependencies in the data.\n6 Examples of pre-trained embeddings include: Word2Vec, GloVe, and fastText.To conclude this section, I save the performance metrics of the Feedforward Neural Network model for later analysis.\n\n\nView Code\nsave_performance(model_name='Feedforward Neural Network',\n                 architecture='embed_layer-&gt;dropout-&gt;120-&gt;dropout-&gt;60-&gt;dropout-&gt;num_classes',\n                 embed_size='60',\n                 learning_rate='1e-3',\n                 epochs='50',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=73.15,\n                 )"
  },
  {
    "objectID": "projects/resume-analyzer.html#encoder-model",
    "href": "projects/resume-analyzer.html#encoder-model",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Encoder Model",
    "text": "Encoder Model\n\n\n\nThe following model utilizes the encoder component of the Transformer architecture for text classification. Unlike decoders, which convert dense representations into output sequences, encoders instead transform input sequences into dense representations. Their ability to extract sequence information and convert them to a dense representation makes them particularly useful for tasks such as sentiment analysis, named entity recognition, and text classification.\nThe encoder model follows the Transformer architecture described in Attention is All You Need7 and is used as a baseline for transformer-based models. I construct the encoder architecture with an embedding layer, a stack of encoder layers, and a feed-forward neural network for classification. I then initialize the hyperparameters for training and train the model using the Adam optimizer and CrossEntropyLoss criterion. The model’s performance is evaluated on the test set using accuracy as the evaluation metric.\n7 Attention Is All You NeedThe imported packages as well as the step in building the DataLoader are the same as those for the Feedforward Neural Network model (see packages and data preparation. Therefore, I skip directly to the model architecture.\n\nTakeaways\n\nThe Transformer Encoder model achieves an accuracy of 75% on the test set.\nThis model serves as a robust baseline for transformer-based models in text classification tasks.\n\n\n\nModel Architecture\n\n\n\n\n\nModified image from Sebastian Raschka\n\n\n8 The multi-head self-attention mechanism is a component of the transformer model that weights the importance of different elements in a sequence by computing attention scores multiple times in parallel across different linear projections of the input.9 Layer normalization is a normalization technique that uses the mean and variance statistics calculated across all features.The Encoder model consists of an embedding layer, a stack of encoder layers, and a fully connected neural network for classifying the outputs. The embedding layer converts the input sequences into dense representations, which are then passed through the encoder layers. Each encoder layer consists of a multi-head self-attention mechanism8 followed by a residual connection with layer normalization9 and a feed-forward neural network, followed by another residual connection with layer normalization. The output is finally passed through a feed-forward neural network for classification.\nI decide to build the encoder model from scratch, as it allows me to better understand the architecture and the components of the model. I opt for a modular approach, where I construct each component of the model as a separate class and then combine them in the TransformerEncoder class.\n\nFurther Reading:\nThe implementation of this model was in great part inspired by the following resources: - The Annotated Transformer - Coding a Transformer from Scratch on PyTorch (YouTube) - Text Classification with Transformer Encoders\n\n\n\nView Code\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        # Dimensions of embedding layer\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        # Embedding dimension\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, dropout: float = 0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        # Initialize positional embedding matrix (vocab_size, d_model)\n        pe = torch.zeros(vocab_size, d_model)\n        # Positional vector (vocab_size, 1)\n        position = torch.arange(0, vocab_size).unsqueeze(1)\n        # Frequency term\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000) / d_model))\n        # Sinusoidal functions\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # Add batch dimension\n        pe = pe.unsqueeze(0)\n        # Save to class\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, d_model: int, eps: float = 1e-6):\n        super().__init__()\n        # Learnable parameters\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.ones(d_model))\n        # Numerical stability in case of 0 denominator\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        # Linear combination of layer norm with parameters gamma and beta\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1):\n        super().__init__()\n        # Layer normalization for residual connection\n        self.norm = LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x1, x2):\n        return self.dropout(self.norm(x1 + x2))\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1):\n        super().__init__()\n        # Linear layers and dropout\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float =0.1, qkv_bias: bool = False, is_causal: bool = False):\n        super().__init__()\n        assert d_model % num_heads == 0,  \"d_model is not divisible by num_heads\"\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.dropout = dropout\n        self.is_causal = is_causal\n\n        self.qkv = nn.Linear(d_model, 3 * d_model, bias=qkv_bias)\n        self.linear = nn.Linear(num_heads * self.head_dim, d_model)\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_length = x.shape[:2]\n\n        # Linear transformation and split into query, key, and value\n        qkv = self.qkv(x)  # (batch_size, seq_length, 3 * embed_dim)\n        qkv = qkv.view(batch_size, seq_length, 3, self.num_heads, self.head_dim)  # (batch_size, seq_length, 3, num_heads, head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_length, head_dim)\n        queries, keys, values = qkv  # 3 * (batch_size, num_heads, seq_length, head_dim)\n\n        # Scaled Dot-Product Attention\n        context_vec = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask, dropout_p=self.dropout, is_causal=self.is_causal)\n\n        # Combine heads, where self.d_model = self.num_heads * self.head_dim\n        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n        context_vec = self.dropout_layer(self.linear(context_vec))\n\n        return context_vec\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, hidden_dim: int, dropout: float = 0.1):\n        super().__init__()\n        # Multi-head self-attention mechanism\n        self.multihead_attention = MultiHeadAttention(d_model, num_heads, dropout)\n        # First residual connection and layer normalization\n        self.residual1 = ResidualConnection(d_model, dropout)\n        # Feed-forward neural network\n        self.feed_forward = FeedForward(d_model, hidden_dim, dropout)\n        # Second residual connection and layer normalization\n        self.residual2 = ResidualConnection(d_model, dropout)\n\n    def forward(self, x, mask=None):\n        x = self.residual1(x, self.multihead_attention(x, mask))\n        x = self.residual2(x, self.feed_forward(x))\n        return x\n\nclass EncoderStack(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, hidden_dim: int, num_layers: int, dropout: float = 0.1):\n        super().__init__()\n        # Stack of encoder layers\n        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, hidden_dim, dropout) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, num_heads: int, hidden_dim: int, num_layers: int, out_features: int, dropout: float = 0.1):\n        super().__init__()\n        self.embedding = EmbeddingLayer(vocab_size, d_model)\n        self.positional_embedding = PositionalEmbedding(vocab_size, d_model, dropout)\n        self.encoder = EncoderStack(d_model, num_heads, hidden_dim, num_layers, dropout)\n        self.classifier = nn.Linear(d_model, out_features)\n\n    def forward(self, x, mask=None):\n        x = self.embedding(x)\n        x = self.positional_embedding(x)\n        x = self.encoder(x, mask)\n        x = x.mean(dim=1)\n        x = self.classifier(x)\n        return x\n\n\n\n\nHyperparameters and Training\nWith the model constructed, I initialize the hyperparameters for training. As with the previous model, I obtain the vocabulary size and number of output features from the ResumeDataset class. The embedding size is fixed at 80, while the hidden dimension is set to 180. For the multi-head attention mechanism, I use 4 heads, with an the encoder stack comprising of 4 layers. I train the model for 20 epochs at a learning rate of 1e-3.\n\n\nView Code\nvocab_size = dataset.vocab_size()\nd_model = 80\nnum_heads = 4\nhidden_dim = 180\nnum_layers = 4\nout_features = dataset.num_class()\nlr = 0.001\nepochs = 20\n\n\nI instantiate the model with the hyperparameters and move it to the device. The criterion and optimizer are left unchanged from the previous model, with the optimizer set to the Adam optimizer and the criterion set to the CrossEntropyLoss, suitable for multi-class classification tasks. I also initialize the learning rate scheduler with a patience of 2, to prevent the model from overfitting. The model is then trained using the train_model function.\n\n\nView Code\nmodel = TransformerClassifier(vocab_size, d_model, num_heads, \n                                hidden_dim, num_layers, out_features, dropout=0).to(device)\ncriterion = CrossEntropyLoss()\nloss = Adam(model.parameters(), lr=lr)\nscheduler = ReduceLROnPlateau(loss, patience=2)\n\ntrain_model(model, train_loader, val_loader, epochs, criterion, loss, scheduler)\n\n\n\naccuracy\n  training           (min:    0.043, max:    1.000, cur:    1.000)\n  validation         (min:    0.035, max:    0.794, cur:    0.785)\nlog loss\n  training           (min:    0.027, max:    3.276, cur:    0.029)\n  validation         (min:    1.023, max:    3.273, cur:    1.071)\n------------------------------\nBest model saved:\nVal Loss: 1.0709 | Val Acc: 0.7847\n✅ Training complete!\nAccording to the performance plots, despite the encoder model achieving a better validation accuracy than the Feedforward Neural Network, the model exhibits a large gap between the training and validation performance, indicating that the model is overfitting. The model rapidly decreases the training and validation losses, and rapidly incrases their accuracies during the first 14 epochs. For the following 6 epochs, however, the training loss continues to rapidly decrease from 1.25 to near 0, while the validation loss stagnates at around 1.07. Similarly, the training accuracy rapidly increases to 100%, while the validation accuracy remains at 78%. The model converges 20 epochs in with a significant gap between training and validation models, indicating that the model is overfitting.\nNevertheless, the model achieves a validation accuracy of 78%, which is slightly higher than the Feedforward Neural Network model. I decide to evaluate the model on the test set to obtain the final accuracy.\n\n\nEvaluation\n\n\nView Code\naccuracy = test_model(model, test_loader, criterion)\n\n\nTest Loss: 1.2526 | Test Acc: 0.7454\n✅ Testing complete!\nDespite having implemented a more advanced model architecture with the addition of the multi-head self-attention mechanism, the encoder model achieves an accuracy of 74.5%, similar to the Feedforward Neural Network model. As discussed earlier, the model exhibits a large gap between the training and validation performance, which could be hindering the model’s generalization capabilities.\nGiven a better training and validation performance, the model could potentially achieve a higher accuracy on the test set. The model’s performance could likely be improved by means of data augmentation, modifying hyperparameters such as embedding size, hidden dimension, and number of layers, or by using a different optimizer or learning rate scheduler.\nAs with the previous models, I save the performance metrics for later analysis.\n\n\nView Code\nsave_performance(model_name='Transformer',\n                 architecture='embed_layer-&gt;encoder-&gt;linear_layer',\n                 embed_size='64',\n                 learning_rate='1e-3',\n                 epochs='20',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=80\n                 )"
  },
  {
    "objectID": "projects/resume-analyzer.html#bert",
    "href": "projects/resume-analyzer.html#bert",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "BERT",
    "text": "BERT\n\n\n\nThe last model is also an encoder-based model called the Bidirectional Encoder Representations from Transformers (BERT). BERT is a pre-trained transformer-based model that can be fine-tuned for a wide range of NLP tasks by adding task-specific output layers. The model is bidirectional, meaning that it can take into account the context of a word by looking at both the left and right context. This allows the model to capture a wider range of contextual information, which is particularly useful for tasks such as text classification. This allows the model to capture rich semantic relationships and dependencies within text sequences, which is particularly useful for tasks such as text classification.\nAs with previous PyTorch models, I create an iterable dataset using the Dataset and DataLoader classes, to tokenize the resumes using the BERT tokenizer, pad sequences to equal lengths, split the data and batch the data for training. I then construct the model architecture, consisting of the pre-trained BERT base model, an added dropout layer and a linear output layer for classification. I initialize the hyperparameters and train the model using Cross Entropy Loss along with the Adam optimizer. The model is evaluated as with other deep learning models using accuracy.\n\n### Takeaways\n\n\n\n\n\n\nImport Packages\nIn addition to the standard deep learning packages used so far, I import three classes from the transformers package:\n\nBertModel: loads the pre-trained BERT model.\nBertTokenizer: constructs a BERT tokenizer.\nDataCollatorWithPadding: builds a batch with dynamically padded sequences.\n\nBecause of BERT’s output format, I also import a custom train_BERT and test_BERT function, which are specifically tailored to return the model’s training and test performances using BERT’s output, including input IDs and attention masks.\n\n\nView Code\nfrom transformers import BertModel, BertTokenizer, DataCollatorWithPadding\n\nfrom src.modules.dl import train_BERT, test_BERT\n\n\n\n\nDataset and DataLoader\nBefore creating the dataset and dataloader, I initizalize the tokenizer and define the pre-trained BERT model. I then create the ResumeBertDataset, which tokenizes resumes and prepares them for model input.\nIn contrast to the previous model, I configure the tokenizer using the .encode_plus method, which returns a dictionary of the batch encodings, including tokenized input sequences and attention masks. I set the padding parameter to False to avoid padding, as this will be handled dynamically by the data collator. Additionally, I set truncation to True to truncate sequences that exceed the maximum length. The method also adds the special tokens [CLS] and [SEP] to the input sequences, required by BERT. I set the return_tensors parameter to 'pt' to return PyTorch tensors. Finally, I return a dictionary with the processed input sequences, attention masks, and labels.\n\n\nView Code\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nclass ResumeBertDataset(Dataset):\n    def __init__(self, data, max_length, tokenizer=tokenizer, device=device):\n        super().__init__()\n        self.texts = data.iloc[:,1].values\n        self.labels = torch.tensor(data.iloc[:,0])\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def __len__(self):\n        return len(self.labels)\n\n    def num_class(self):\n        return len(self.labels.unique())\n\n    def __getitem__(self, idx):\n        resumes = self.texts[idx]\n        labels = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            resumes,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            truncation=True,\n            padding=False,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ).to(self.device)\n\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels\n        }\n\n\nI initialize the dataset and set max_length to 512, which is the maximum number of tokens that BERT can process. I then split the dataset into 70% training, 15% validation, and 15% test sets using the random_split function. I use the DataCollatorWithPadding class to dynamically pad sequences to the maximum length in each batch. Finally, I create dataloaders for the training, validation, and test sets using the DataLoader class, setting the batch size to 16, shuffling the data, and assigning the data collator.\n\n\nView Code\ndataset = ResumeBertDataset(data, max_length=512)\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.15, 0.15])\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n\n\n\n\nModel Architecture\nThe BertResumeClassifier model consists of the pre-trained BERT base model, a dropout layer, and a linear output layer to classify the resumes. The BERT model uses the bert-base-uncased pre-trained model to generate contextual embeddings from the input sequences. I extract the embeddings by indexing the pooler_output key from the output dictionary. These embeddings are then passed through a dropout layer to prevent overfitting, and subsequently fed into a fully connected linear layer which maps the embeddings to the desired number of output classes for classification.\n\n\nView Code\nclass BertResumeClassifier(nn.Module):\n    def __init__(self, n_classes: int, dropout: float = 0.01):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )['pooler_output']\n        output = self.dropout(pooled_output)\n        output = self.out(output)\n        return output\n\n\n\n\nHyperparameters and Training\nSince BERT is a pre-trained model and it accepts a fixed input size of 512 tokens, the are less parameters to set from the model itself. The only parameter that needs to be set is the number of classes—which, as before, is obtained from the Dataset class.\nI initialize the model, loss function, optimizer, and number of epochs. I use as before the Cross Entropy Loss function and the Adam optimizer, although this time with a learning rate of 2e-5, since it seemed to result in better performance. I train the model for only 10 epochs.\n\n\nView Code\nn_classes = dataset.num_class()\n\nmodel = BertResumeClassifier(n_classes).to(device)\ncriterion = CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=2e-5)\nepochs = 10\n\ntrain_BERT(model, train_loader, val_loader, epochs, criterion, optimizer)\n\n\n\naccuracy\n    training             (min:    0.050, max:    0.991, cur:    0.991)\n    validation           (min:    0.120, max:    0.933, cur:    0.933)\nlog loss\n    training             (min:    0.081, max:    3.189, cur:    0.081)\n    validation           (min:    0.414, max:    3.002, cur:    0.428)\n------------------------------\nBest model saved:\nVal Loss: 0.4143 | Val Acc: 0.9190\n✅ Training complete!\nThe BERT model shows a significant and steady decrease in both training and validation losses over the 10 epochs, indicating effective learning of data patterns. By the end of training, the model’s train loss decreased from 3.1394 to 0.0589, while the validation loss decreased from 2.7347 to 0.5070. This consistent reduction highlights the model’s ability to capture and generalize the data without overfitting, as demonstrated by the small gap between the training and validation losses by the end of the 10th epoch.\nRegarding accuracy, the training and validation accuracies also show a steady increase over the 10 epochs. The training accuracy increased from 0.0774 to 0.9950, with validation accuracy improving from 0.3472 to 0.9028. This indicates that the model effectively learned the patterns in the data and generalizes well to unseen data. The small difference between the final training and validation accuracies demonstrates the model’s robustness and ability to avoid overfitting, ensuring reliable performance on new data. The best saved model has a validation loss of 0.5070 and an accuracy of 0.9028.\n\n\nEvaluation\nAs before, I evaluate the model using the test_model function using the best saved model.\n\n\nView Code\naccuracy = test_BERT(model, test_loader, criterion)\n\n\nTest Loss: 0.3984 | Test Acc: 0.9167\n✅ Testing complete!\nThe BERT model achieved an impressive performance on the test set, reaching an accuracy of 91.67% with a test loss of 0.3984. This significantly outperforms all previous models tested so far. As expected from the training and validation performances, the model is robust and generalizes very well to unseen data. This is further confirmed by the very close alignement between test and validation accuracies, both of which represent datasets not previously seen by the model. The close accuracy between the validation data and the test data shows the model is capable to generalize to new resumes and effectively classify them into the correct categories.\nAs with previous models, I save its performance using the save_performance function.\n\n\nView Code\nsave_performance(model_name='BERT',\n                 architecture='bert-base-uncased&gt;dropout-&gt;linear_layer',\n                 embed_size='768',\n                 learning_rate='2e-5',\n                 epochs='10',\n                 optimizer='Adam',\n                 criterion='CrossEntropyLoss',\n                 accuracy=90\n                 )"
  },
  {
    "objectID": "projects/resume-analyzer.html#results-and-discussion",
    "href": "projects/resume-analyzer.html#results-and-discussion",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nIn this section of the project, I explored four different models for resume classification: Linear SVC, FNN, Transformer, and BERT. I evaluated the performance of each model using the accuracy metric and collected the results after each deployment. Below I plot the accuracy of each model and discuss the results.\n\n\nView Code\nevaluation_df = pd.read_json('./output/classifier_performance.json').sort_values(by='accuracy', ascending=False)\n\nax = sns.barplot(evaluation_df, x='model', y='accuracy', hue='model', palette='hls')\n[ax.bar_label(container, fmt=\"%0.2f%%\") for container in ax.containers]\nplt.show()\n\n\n\n\n\nAll four models performed well in classifying resumes, achieving accuracies above 70%. This success can be largely attributed to the effectiveness of the data preparation process, including text preprocessing, data balancing, and robust vectorization techniques. These preprocessing steps provided the models with high-quality input features that significantly enhanced their performance.\nUpon closer evaluation, the models can be grouped into two categories based on their performance. The first group, achieving around 90% accuracy, includes the Linear SVC and BERT models. The second group, with accuracies around 70%, includes the FNN and Transformer models. Interestingly, the two best-performing models feature both the simplest and most complex architectures respectively, while the models with the lowest performance have more complex architectures than the baseline model. I discuss the reasons behind these results below.\nLinear SVC’s high performance can be attributed its simplicity and the effective feature representation10. The model is a classical machine learning algorithm that uses a linear kernel and no deep learning, which yields a simple architecture that is easy to train. Additionally, the model was trained on TF-IDF vectors, which result in a matrix with simple, but interpretable and informative features. This simplicity reduces the risk of overfitting and the straightforward feature representation contributes to the model’s high accuracy and fast training times.\n10 To read more on the efficiency of linear classifiers in text classification, see Lin, Y.-C. et al. (2023) ‘Linear Classifier: An Often-Forgotten Baseline for Text Classification’.11 See Devlin, J. et al. (2018) ‘BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding’.In contrast, BERT’s performance strems from its pre-trained nature and high-quality embeddings. The model was pre-trained on a large corpus and thus has the ability to generate embeddings that encode deep semantic information11. Moreover, its bidirectional nature captures a wide range of contextual information across a long-range dependencies. Its suitability for Transfer Learning allows maximizing its pre-trained weights and easily fine-tuning on resume classification task, resulting in the high accuracy achieved.\nThe FNN and Transformer models, despite their increased complexity, achieved lower accuracies of around 70%. The Feedforward Neural Network, while more advanced than a linear model, lacks the ability to capture sequential dependencies and contextual nuances in the data, thus expected to perform worse than the transformer models.\nHowever, the Transformer model should have been able to capture the sequential dependencies in the resume data, given its state-of-the-art architecture and use of multi-head self-attention. But contrary to BERT, the Transformer used in this project was not pre-trained on a large corpus. This limited its ability to generate high-quality dense representations of the texts. Additionally, insufficient fine-tuning may have prevented the Transformer from reaching its full potential. Given the results of a similar, more complex model such as BERT, additional hyperparameter tuning and training time could improve its performance."
  },
  {
    "objectID": "projects/resume-analyzer.html#conclusion",
    "href": "projects/resume-analyzer.html#conclusion",
    "title": "BERT, Encoders and Linear Models for Resume Text Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this project, I explored the task of resume classification using machine learning and deep learning models. The process began with preprocessing the resume data, including tokenizing the resumes and preparing them for model input. I implemented four models: a Linear Support Vector Classifier, a Feedforward Neural Network, a Transformer model, and a BERT model. These models were trained and evaluated on the resume dataset, with performance compared based on accuracy.\nThe results were very insightful and provoked interesting observations on the role of model complexity and feature representation in achieving high performance. The BERT model achieved the highest accuracy of 91.67%, closely followed by the LinearSVC model at 87.15%. The Feedforward Neural Network and Transformer models achieved lower accuracies of 73.15% and 74.54%, respectively. BERT’s superior performance can be attributed to its pre-trained transformer architecture, which captures rich semantic relationships and dependencies within text sequences. The strong performance of the LinearSVC model can be attributed to its simplicity and efficiency in handling high-dimensional data, leveraging high-quality, interpretable feature representations such as TF-IDF vectors.\nTwo important observations arise from these results:\n\n\nState-of-the-art transformer models, combined with transfer learning from pre-trained models like BERT, yield the best performance.\nSimple models with high-quality, interpretable feature representations such as TF-IDF vectors, can also achieve high performance.\n\n\nThese contrasting observations indicate that model complexity alone does not guarantee high performance. This is further supported by the fact that the two models with the lowest accuracies have more complex architectures than the baseline.\n\nThe quality of the feature representation and the ability to capture contextual information across dependencies are more important factors in achieving high performance.\n\nUltimately, the best model depends on the requirements of the task at hand and the resources available for development. For tasks where high performance is critical and ample resources are available, using state-of-the-art transformer models such as BERT with transfer learning are recommended. For tasks prioritizing simplicity and interpretability, Linear SVC with TF-IDF vectors still offers a high-performance, resource-efficient alternative."
  },
  {
    "objectID": "projects/airline-performance.html#executive-summary",
    "href": "projects/airline-performance.html#executive-summary",
    "title": "Airline On-Time Performance",
    "section": "Executive Summary",
    "text": "Executive Summary\nThis project analyzes on-time performance trends from 30 years of US domestic flight data, focusing on variations across carriers, routes, airports, and time. The goal is to conduct a comprehensive exploratory data analysis (EDA) to address key industry questions, such as identifying carriers and airports with the highest frequency of flight delays, understanding the impact of departure delays on arrival times, and examining trends in flight delays within the US aviation sector. The findings from the EDA will be translated into an interactive visualization dashboard using Streamlit."
  },
  {
    "objectID": "projects/airline-performance.html#dataset",
    "href": "projects/airline-performance.html#dataset",
    "title": "Airline On-Time Performance",
    "section": "Dataset",
    "text": "Dataset\nThe Airline Reporting Carrier On-Time Performance Dataset, sourced from the U.S. Department of Transportation’s Bureau of Transportation Statistics, contains scheduled and actual departure and arrival times reported by certified U.S. air carriers between 1987 and 2020. The version used in this project, available at IBM Developer1, contains a 2 million record sample of the full dataset (&lt;1%).\n1 The dataset is available in three sizes: the original dataset of 194,385,636 flights, a 2 million sample version and a 2 thousand sample of flights from LAX to JFK airport. All three versions are available as gzip compressed tar or csv files.Key features reported by carriers include scheduled and actual event times, flight dates, carrier, origin and destination information, cancelation and diversion information, as well as summary statistics such as elapsed time, distance and delay causes. A full description of the variables in the dataset can be found here. Below is a summary of the main variable grouppings.\n\n\nFeature glossary\n\n\nTemporal variables: Year, Quarter, Month, DayofMonth, DayOfWeek, FlightDate\nFlight variables: Reporting_Airline, Tail_Number, Flight_Number_Reporting_Airline\nOrigin/Destination variables:\nOriginAirportID, Origin, OriginCityName, OriginState, OriginStateName,\n\nDestAirportID, Dest, DestCityName, DestState, DestStateName\n\nDeparture/Arrival time variables:\nCRSDepTime, DepTime, DepDelay, DepDelayMinutes, DepDel15, DepartureDelayGroups\n\nCRSArrTime, ArrTime, ArrDelay, ArrDelayMinutes, ArrDel15, ArrivalDelayGroups\n\nTaxi variables: TaxiOut, WheelsOff, WheelsOn, TaxiIn\nCancellation variables: Cancelled, CancellationCode, Diverted\nFlight summary variables: CRSElapsedTime, ActualElapsedTime, AirTime, Flights, Distance, DistanceGroup\nCause of Delay (Data starts 6/2003): CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay"
  },
  {
    "objectID": "projects/airline-performance.html#preprocessing-and-feature-engineering",
    "href": "projects/airline-performance.html#preprocessing-and-feature-engineering",
    "title": "Airline On-Time Performance",
    "section": "Preprocessing and Feature Engineering",
    "text": "Preprocessing and Feature Engineering\nThe strategy for preprocessing interleaves univariate analysis with data cleaning and feature engineering for each variable group, utilizing the former to contextualize preprocessing decisions in the latter. For this project, this approach encompases three major aspects: conducting a general univariate analysis, preprocessing flight info variables, and (re)engineering time variables.\n\nUnivariate Analysis\nFor each variable group, I conducted a univariate analysis to understand the distribution, spread, and missing values of the variables using two custom functions:\n\n\ndf_overview prints the shape, sample head and tail, and non-null counts.\n\ndef df_overview(df):\n    print(f\"Shape: {df.shape}\\n\")\n    print(f\"Head and tail preview:\")\n    display(df)\n    print(f\"Df info:\")\n    print(df.info(verbose=True), \"\\n\")\n    print(\"-\"*70)\n\n\n\nunivariate_preview provides a compact report with the first five rows, data types, unique values, top 5 values, null value percentage, and summary statistics for continuous variables.\n\ndef univariate_preview(df, cols, describe=True):\n    display(\"Data Preview\")\n    display(df[cols].head())\n    \n    display(\"Value Counts\")\n    list = []\n    for col in cols:\n        list.append(\n            [col,\n            df[col].dtypes,\n            df[col].nunique(),\n            df[col].value_counts().iloc[:5].index.tolist(),\n            \"{:.2f}%\".format(df[col].isna().mean()*100)]\n            )\n    display(pd.DataFrame(list, \n                         columns = ['columns', 'dtypes', 'nunique', 'top5', 'na%']\n                         ).sort_values('nunique', ascending=False))\n    \n    if describe:\n        display(\"Summary Stats\")\n        display(pd.concat([\n            df[cols].describe(),\n            df[cols].skew().to_frame('skewness').T,\n            df[cols].kurtosis().to_frame('kurtosis').T,\n        ]))\n\nAdditionally, I visualized missing values using the missingno package2, while certain distributions and value counts I visualized using matplotlib and seaborn.\n2 The missingno package offers visualization options such as heatmaps, bar charts, and dendrograms to identify patterns and distributions of missing data.\n\nPreprocessing Flight Info Variables\nThe first half of all preprocessing focused on the flight information variables, including date, flight, origin/destination, and departure/arrival variables. Based on the findings of each univariate analysis, I performed the following preprocessing steps:\n\nData Cleaning\n\nFlightDate to datetime64[ns]: allowed to perform date operations, aggregations, and visualizations.\nFilling Missing State and State Names: imputed missing state and state name values to reduce missing data.\nStandardizing City Names: standardized city names to ensure uniformity and remove single occurrences.\nDropping Taxi, Cancelled, and Diverted Variables: removed variables due to high occurrence of missing values.\n\nFeature Engineering\n\nAirline Names: used an airlines dataset to map the airline codes to their respective airline names to provide more readable and insightful information.\nUnique Flight Identifier: explored creating a unique flight identifier by combining carrier code and flight number, to reduce ambiguity and identify unique flights more easily.\n\n\n\n\n(Re)Engineering Time Variables\nThe second phase of preprocessing addresses a critical issue found in the dataset’s handling of time-related variables, particularly concerning flight delays. The delay variables were calculated from the raw timestamps, without accounting for time zone discrepancies, daylight savings time, and cross-midnight flights. This required developing an approach to impute time zone information, correct timestamps, and recompute delays accurately.\nTo address these issues, I implemented the following steps:\n\nImputing Time Zone Information: assigned time zones to all timestamps using a dictionary of airport codes and their respective time zones sourced from two datasets.\nReverse Engineering Arrival Dates: converted all times to UTC (Coordinated Universal Time), recalculated timestamps based on flight departure dates and time differences, and reconverted times to local time zones.\nFilter Remaining Negative Delays: removed the remaining negative delays after the reverse engineering process to ensure accurate delay calculations.\nDelay Calculation: calculated delays based on the difference between scheduled and actual departure and arrival times, accounting for time zone differences and daylight savings time.\n\nIn addition to this, I also performed the following data cleaning and feature engineering steps:\n\nData Cleaning:\n\nStandardizing 0s, 2400s, and missing values: converted 0s to Nan values and 2400s to 0s for consistency in time representation.\nFilling Missing CRS Values: imputed missing values in the CRS (Scheduled) times from the difference between actual times and delay minutes.\nChanging Data Types: converted data types where necessary to improve computational efficiency.\n\nFeature Engineering:\n\nDatetime Conversion: created UTC versions of time variables for time calculations such as elapsed time.\nTime Recalculations: using the time zone marked timestamps, recalculated delays and elapsed times more accurately than the original dataset."
  },
  {
    "objectID": "projects/airline-performance.html#exploratory-data-analysis",
    "href": "projects/airline-performance.html#exploratory-data-analysis",
    "title": "Airline On-Time Performance",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe Exploratory Data Analysis in this project was carried in two modalities: a methodical univariate analysis of each variable during preprocessing and a targeted bivariate and multivariate analysis focused on answering the research questions of the project3. I framed the questions around five distinct themes:\n3 Most questions in this bivariate analysis adopt a longitudinal perspective, comparing the frequency of delays with different variables across the entire span of the dataset. This approach provides a historical view of delays within the US aviation sector. However, further insights lie within a cross-sectional analysis, which compares the frequency of delays across different variables at specific points in time. This would be more suitable for a multivariate analysis, which I leave for future work.\nLongitudinal: How do delays vary across time?\nCorporate: How do delays vary across carriers? (distribution and longitudinal)\nDirectional: How do delays vary by route?\nGeographical: how do delays vary by airport?\nCorrelation: are departures delays correlated with arrival delays? (or how frequent do flights make up for late departures?)\n\nBefore diving into the findings for each question, I will provide a brief overview of the findings from the univariate analysis.\n\nUnivariate Analysis\nThe univariate analysis revealed several key insights about the dataset:\n\n\nDate columns: The flights span 34 years, with the distribution slightly shifted left (2005), indicating growth in the airline industry in the last 15 years. Other columns such as months, days, and weekdays, show an even distribution.\n\n\n\n\n\nFlight info columns: The dataset includes 33 unique carriers, with Southwest Airlines (WN), Delta Air Lines (DL), and American Airlines (AA) being the most frequent.\n\n\n\n\n\nOrigin/Destination columns: Flights originate and land in all 50 US states and five overseas territories, including Puerto Rico, US Virgin Islands, and Guam. The top destinations in the dataset are California (CA), Texas (TX), Florida (FL), Illinois (IL), Georgia (GA), and New York (NY).\n\n\n\n\n\nTime Variables: Departures are more frequent in the morning hours, while arrivals peak in the evening. The distribution of CRS and actual time variables for departures and arrivals are almost identical, indicating that the average size of flight delays is small across the dataset.\n\n\n\n\n\nDelay Variables: Approximately 80% of flights operate on-time, while around 20% experience delays (as by the industry-defined standard of exceeding 15 minutes). The distribution shows a right skew, with over 40% of flights arriving early by less than 15 minutes and about 30% arriving exactly on time. Out of all delayed flights, most fall within the 15 to 40-minute range.\n\n\n\n\n\nFlight Summary Variables: Flights typically cover distances under 1000 miles, with a right-skewed distribution observed in the data. The majority of flights have durations under around 160 minutes or 2 hours and 40 minutes, as indicated by the right-skewed distributions of the elapsed time variables.\n\n\n\n\n\n\n\n\nBivariate Analysis\n\nHow do delays vary across time?\nThis question involves a longitudinal analysis of the evolution of delayed flights over 34 years. It compares the continuous variable “number of delayed flights” with the categorical time variable, examining both departure and arrival delays\nTo answer this question, I first defined the time step by which to evaluate the evolution of delays. I chose to aggregate the data by month-year date, which provides a balance between granularity, interpretability and computational efficiency. Next, I decided on the metric to evaluate delays. Although the raw sum of delays is intuitive, normalizing the counts by the total flights per month offers a more accurate representation of delay frequency, as it adjusts for variations in monthly flight numbers.\nWith these definitions, I began by plotting the evolution in the number of delayed flights per month over the years, as shown in the figure below.\n\nThe normalized plots above reveal several insights about the evolution of delays from a cross-sectional and longitudinal perspective:\n\nCross-sectional observations:\n\nThe line plot reveals periodic fluctuations in both departure and arrival delays, reflecting seasonal changes in delays within each year.\nA noticeable drop in delays occurs between approximately 2001 and 2003, potentially influenced by external economic factors4.\n\nLongitudinal observations:\n\nThe regression plot shows an initial gap between departure and arrival delays, which converges over time to approximately 20% by the end of the dataset.\nThe line plot demonstrates a reduction in the volatility of delays over the years, suggesting a slight stabilization in the frequency of delays.\n\n\n4 One potential cause for this temporary drop in delays is the contraction in the airline industry that followed 9/11, reducing the amount of delays together with the number of flights (see this article)This narrowing gap and reduction in delay volatility would have suggested an improvement in airline on-time performance, if it were not for the fact that the reduction is cause by an increase in departure delays rather than a decrease in arrival delays. The regression analysis demonstrates this: arrival delays have decreased only by 1%, while departure delays have risen by over 5% in the span of 30 years. This confirms that airlines have been unable to improve on-time performance at arrivals and have become less efficient in managing departure delays.\n\n\nAre departures delays correlated with arrival delays?\nThis question is a correlation analysis examining the connection between the two continuous variables: departure and arrival delays.\nTo answer this, I calculated the Pearson correlation coefficient between departure delays and arrival delays for delayed flights5. This coefficient ranges from -1 indicating a perfect negative linear relationship, 0 indicating no linear relationship, up to 1 indicating a perfect positive linear relationship. To visualize the relationship, I plot the correlation using a custom jointplot function, which combines a scatter plot with a box plot to show the distribution of the data across both variables. This function also computes and displays the Pearson correlation coefficient.\n5 I filtered the dataset for flights with delays greater than 15 minutes, the industry standard for defining a delayed flight. This prevents the distribution from factoring in flights that were not delayed, which account for the majority of the dataset and may skew the results.Before plotting the jointplot, I cleaned the data by removing the remaining extreme cases, such as flights with negative delays which, due to times crossing the change to daylight savings time, were not corrected in the preprocessing. After this, I plotted the jointplot, which is shown below.\n\nThe Pearson correlation coefficient between departure and arrival delays is 0.93, indicating a strong positive linear relationship between the two variables. However, there are three observations that can be made from the jointplot:\n\nOutliers and Extreme Cases: The scatter plot exhibits a broad spectrum of outliers that extend well beyond the interquartile range, yet notably adhere to the linear relationship between departure and arrival delays.\nLinear Relationship: The distribution shows that the relationship follows a more linear pattern the more extreme their values are, whereas the relationship becomes more dispersed as delays become less extreme.\nInterquartile Range: The interquartile range for both departure and arrival delays is notably narrower compared to the full range of the data, which underscores the concentration of delays within 60 minutes, as seen in the univariate analysis.\n\nTo understand the nature of this relationship for the core of flights, I focused on the interquartile range and adjusted the analysis: recalculating the correlation coefficient and focusing the jointplot on this range.\n\nThe Pearson correlation coefficient is now 0.67 within the interquartile range, which indicates a weaker but still positive linear relationship between departure and arrival delays. This relationship is depicted in the jointplot by a broader dispersion of data points. The empty square in the lower left corner of the plot represents flights filtered out by the industry standard definition of a delay (&gt;15 minutes). The boxplots along the edges of the scatterplot further confirm the findings from the univariate analysis, namely that the majority of delays range between 15 and 40 minutes.\nThe filtered scatterplot has two clear boundaries that define the relationship between departure and arrival delays. I calculated approximate regression lines for these boundaries to understand the relationship more clearly.\n\nThere are two observations that can be made about the correlation of departure and arrival delays for the interquartile range:\n\nShallow Bound: Indicates the minimum threshold to which arrival delays can be minimized.\n\nFollows a linear relationship: \\(y = 1.15x - 45\\), where \\(y\\) is the arrival delay and \\(x\\) is the departure delay.\nImplies that the arrival delay can only be reduced by up to 45 minutes relative to a departure delay.\nHighlights a practical limit: minimizing arrival delays becomes increasingly constrained as departure delays increase.\n\nSteep Bound: Indicates how arrival delays can increase relative to departure delays.\n\nFollows a steeper linear trend: \\(y = 6.5x + 120\\), where \\(y\\) is the arrival delay and \\(x\\) is the departure delay.\nReflects that arrival delays can significantly exceed departure delays due to extrenal factors like diversions, weather, or arrival airport congestion.\nBound shows an upper limit of 250 minutes for on-time departures (see first scatterplot), then progressively aligning with the general trend as departure delays increase.\n\n\nIn conclusion, the correlation analysis reveals a moderately positive linear relationship between departure and arrival delays when focusing on the interquartile range. The broad dispersion of data points within the interquartile range points to the influence external factors affecting arrival delays relative to departure delays. This dispersion is marked by two boundaries: arrival delays can be minimized by up to 45 minutes relative to departure delays, yet can exceed them by up to 250 minutes. Otherwise, when considering the full range of data, the analysis demonstrates a strong positive linear relationship between departure and arrival delays.\n\n\nHow do delays vary across carriers?\nThis is one of the most frequently asked questions in the airline industry, as it provides insights into the performance of individual carriers and their overall competitiveness in the market. It requires a bivariate analysis comparing the frequency of delays across carriers. Most reports approach this question by comparing the total number of delays per carrier, but this approach can be misleading for two reasons:\n\nFlight Volume: Carriers with higher flight volumes are more likely to have higher delay counts, which can skew the results.\nCarrier Age: Older carriers have more historical data, which can also skew the results.\n\nThis analysis uses a more accurate metric: the normalized delay frequency. This metric adjusts the delay counts by the total number of flights per carrier6. This approach provides a more accurate representation of delay frequency, as it accounts for variations in flight volume and, to some extent, carrier age. Below are the plots of the normalized delay frequencies across carriers.\n\n\n6 An alternative approach would be to normalize the counts by the age of the carrier. However, this would require data mapping each carrier to its age in the industry, which, at the time of writing this report, was not available for the carriers in this dataset.\nThe normalized delay counts show a more robust comparison of on-time performance between airlines, even among those with higher flight volumes or varying ages. Each plot reveals several insights about carrier performance, however, the most revealing insights come from comparing the rankings of carriers across both departure and arrival delays.\n\nConsistent Rankings: JetBlue Airways, ExpressJet Airlines, and Frontier Airlines consistently rank among the top carriers with the highest frequency of delays, both at departure and arrival.\nTop Departure Delays: JetBlue Airways, ExpressJet Airlines, Frontier Airlines, Allegiant Air, and Southwest Airlines have the highest proportion of delays at departure.\nTop Arrival Delays: Piedmont Airlines, JetBlue Airways, ExpressJet Airlines, Conair, and Frontier Airlines have the highest proportion of delays at arrival.\nLowest Delays: carriers with the lowest frequency of delays are either historical carriers with shorter operational histories recorded in the dataset, such as Pan American World Airways (operating long before the dataset’s start date), Midway Airlines, and Northwest Airlines, or regional carriers such as Hawaiian Airlines, Aloha Air Cargo, Endeavor Air, and SkyWest Airlines.\nLargest Differences: the largest difference between departure and arrival delays is observed for Southwest Airlines, which ranks 5th in departure delays but 20th in arrival delays.\n\nFor measuring carrier performance, the proportion of departure delays is probably the most relevant metric for several reasons. Departure delays are more under the carrier’s control, as they are influenced by factors such as boarding, fueling, and maintenance, which are all managed by the carrier. Furthermore, as confirmed by the correlation analysis, departure delays have a positive linear relationship with arrival delays, thus making them more likely to cascade into arrival delays. This causes colinearity between the two metrics, which can be misleading when comparing carriers. In addition to this, arrival delays are more influenced by external factors such as weather, air traffic control, and airport congestion, which are beyond the carrier’s control. Even with these factors aside, a carrier may still be able to mitigate arrival delays, making the metric less indicative of carrier performance.\nConsidering these factors, the carriers with the highest proportion of departure delays, which are also consistent across arrival delays, are JetBlue Airways, ExpressJet Airlines, and Frontier Airlines.\n\n\nHow do delays vary by airport?\nSimilar to the carrier analysis, this question involves comparing delay frequencies across airports. Also as one of the most posed questions in the airline industry, the goal is to identify airports with the highest frequency of delays, which can provide insights into the airport performance and their impact on the overall aviation sector.\nThere are two available metrics to evaluate delay frequencies across airports: total delays and normalized delay frequency. However, choosing a metric to answer this particular question proves challenging, as each metric reveals a completely different answer due to the univariate distribution of each metric.\n\nTotal delays are skewed towards airport size: larger airports have more flights and thus more delays, which skews the distribution by airport size.\n\n\nProvides a more differentiated view of airport performance.\nBiased towards larger airports.\n\n\nMean delays show high kurtosis: the distribution of delays across airports is highly concentrated around the mean, with most airports having a similar frequency of delays, which makes it difficult to distinguish between them when visualizing the data.\n\n\nProvides a more accurate representation of delay frequency.\nFails to distinguish airports close to the mean and may maximize the effects of airports with few flights.\n\n\nFor the purposes of visualizing the data, I chose to use the total delay counts for arrival delays due to several reasons. Firstly, it provides a more differentiated view of airport performance. Moreover, the question is usually posed in terms of identifying the most known airports with the highest frequency of delays, a question better answered by the total delay counts, which already favor larger airports. Furthermore, the choice of considering only arrival delays responds to the fact that departure delays are more susceptible to carrier operations7, whereas arrival delays are more influenced by external factors, including airport operations. This makes arrival delays a better metric for evaluating airport performance8. Below are the plots of the total delay counts across airports.\n7 According to the Airline On-Time Statistics and Delay Causes, air carrier delays accounted for nearly 25% of national delays from January 2010 to March 20208 Of course, as stated in the previous section, airport management is not the only cause for arrival delays. There is also a moderate correlation between departure and arrival delays which increases after as certain threshold of departure delays. However, when comparing airports, arrival delays abstract the role of the carrier and focus on the external factors, among which is airport management.\n\nArrival Delays by Airport\n\n\n\n\nThe bubble map shows the distribution of arrival delays across airports, with the size of the bubble representing the total number of delays. Given the nature of the metric, the size of the bubbles favor larger airports, which have higher flight volumes and correspondingly higher delay counts. Several observations can be made from the visualization:\n\nTop Airports: the top five airports with the highest total delay counts are Chicago O’Hare International Airport (ORD), Hartsfield-Jackson Atlanta International Airport (ATL), Dallas/Fort Worth International Airport (DFW), Los Angeles International Airport (LAX), and San Francisco International Airport (SFO).\nCostal Airports: coastal airports such as Los Angeles (LAX), San Francisco (SFO), Newark (EWR), and Boston (BOS) also rank high among the top 15 airports, randing from 13783 to 8297.\nNew York Area Congestion: Newark (EWR) and LaGuardia (LGA) are among the top ten, with 11,119 (7th) and 8,723 (9th) delays respectively. This underscores the heavy congestion and operational difficulties in the New York metropolitan area.\nGeographical Spread: the top 20 airports with the highest total delay counts are spread across the country,showing that delays are a widespread issue.\nJFK: insteresingly, JFK ranks 20th with 6095 delays, which is significantly lower than the top airports including those in the New York Metropolitan area. The causes of this difference would be an interesting question for further analysis.\n\nOverall, analyzing arrival delays across airports reveals crucial insights into the distribution and frequency of delays within the aviation network. Larger airports such as Chicago O’Hare International Airport, Hartsfield-Jackson Atlanta International Airport, and Dallas/Fort Worth International Airport have the highest total delay counts, which is expected due to their higher flight volumes. Coastal airports and those in the New York area also show significant delay counts, highlighting congestion and operational challenges in these regions. The geographical spread of the top 20 airports indicates that delays are not confined to specific areas but are a nationwide issue. Overall, this analysis underscores the importance of addressing delays at both major hubs and regional airports to improve the efficiency of the U.S. aviation system."
  },
  {
    "objectID": "projects/airline-performance.html#conclusion",
    "href": "projects/airline-performance.html#conclusion",
    "title": "Airline On-Time Performance",
    "section": "Conclusion",
    "text": "Conclusion\nIn this project, I analyzed 30 years of US domestic flight data to uncover trends in on-time performance across carriers, airports, and time. The analysis aimed to address four key questions: how delays vary over time, the correlation between departure and arrival delays, how delays differ among carriers, and how delays are distributed across airports. The analysis yielded the following findings:\n\nLongitudinal Analysis\n\nThe evolution of delays over 34 years shows a narrowing gap between departure and arrival delays, driven by an increase in departure delays rather than a decrease in arrival delays.\nThe volatility of delays has slightly decreased over the years, suggesting a stabilization in the seasonal patterns of delays.\nArrival delays have decreased by only 1%, while departure delays have increased by over 5% in 34 years, confirming that airlines and airports have been unable to improve on-time performance.\n\nCorrelation Analysis\n\nDeparture and arrival delays exhibit a strong positive linear relationship, with a Pearson correlation coefficient of 0.93.\nThe relationship is more dispersed within the interquartile range, with a weaker correlation coefficient of 0.67.\nThe dispersion in delays is bounded by two linear trends: arrival delays can be minimized by up to 45 minutes relative to departure delays, yet can exceed them by up to 250 minutes.\nThe disparity in the correlation responds to a difference in the causes of delays: while departure delays are more under the carrier’s control, arrival delays are more influenced by external factors, including weather, diversion, or airport congestion.\n\nCarrier Analysis\n\nJetBlue Airways, ExpressJet Airlines, and Frontier Airlines consistently rank among the top carriers with the highest frequency of delays both at departure and arrival.\nSouthwest Airlines has the largest difference between departure and arrival delays, ranking 5th in departure delays but 20th in arrival delays.\nDeparture delays are more indicative of carrier performance, as they are more under the carrier’s control, influenced by factors such as boarding, fueling, and maintenance.\n\nAirport Analysis\n\nThe top five airports with the highest total delay counts are Chicago O’Hare International Airport, Hartsfield-Jackson Atlanta International Airport, Dallas/Fort Worth International Airport, Los Angeles International Airport, and San Francisco International Airport.\nCoastal airports such as Los Angeles, San Francisco, Newark, and Boston also rank high among the top 15 airports, indicating congestion and operational difficulties in these regions.\nThe geographical spread of the top 20 airports indicates that delays are not confined to specific areas but are a nationwide issue.\n\n\nThis analysis highlights ongoing challenges in improving on-time performance, particularly with departure delays. The strong correlation between departure and arrival delays emphasizes the importance of addressing issues at the departure stage to enhance overall punctuality. Significant variations in delay frequencies across carriers and routes suggest that tailored strategies are needed for different airlines and specific airports The findings from this project have been effectively translated into an interactive visualization dashboard, providing valuable insights for stakeholders to inform decision-making and improve operational efficiency in the aviation industry."
  },
  {
    "objectID": "projects/biodiversity.html",
    "href": "projects/biodiversity.html",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "",
    "text": "This project explores biodiversity data from the National Parks Service about endangered species in various parks. In particular, the project delves into the conservation statuses of endangered species to see if there are any patterns regarding the type of species the become endangered. The goal of this project will be to perform an Exploratory Data Analysis and explain findings from the analysis in a meaningful way.\nSources: Both Observations.csv and Species_info.csv was provided by Codecademy.com.\n\n\nThe project will analyze data from the National Parks Service, with the goal of understanding characteristics about species and their conservations status, and the relationship between those species and the national parks they inhabit.\nSome of the questions to be tackled include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nThe project makes use of two datasets. The first dataset contains data about different species and their conservation statuses. The second dataset holds recorded sightings of different species at several national parks for 7 days.\n\n\n\nThe analysis consists of the use of descriptive statistics and data visualization techniques to understand the data. Some of the key metrics that will be computed include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nLastly, the project will revisit its initial goals and summarize the findings using the research questions. This section will also suggest additional questions which may expand on limitations in the current analysis and further guide future analyses on the subject."
  },
  {
    "objectID": "projects/biodiversity.html#introduction",
    "href": "projects/biodiversity.html#introduction",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "",
    "text": "This project explores biodiversity data from the National Parks Service about endangered species in various parks. In particular, the project delves into the conservation statuses of endangered species to see if there are any patterns regarding the type of species the become endangered. The goal of this project will be to perform an Exploratory Data Analysis and explain findings from the analysis in a meaningful way.\nSources: Both Observations.csv and Species_info.csv was provided by Codecademy.com.\n\n\nThe project will analyze data from the National Parks Service, with the goal of understanding characteristics about species and their conservations status, and the relationship between those species and the national parks they inhabit.\nSome of the questions to be tackled include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nThe project makes use of two datasets. The first dataset contains data about different species and their conservation statuses. The second dataset holds recorded sightings of different species at several national parks for 7 days.\n\n\n\nThe analysis consists of the use of descriptive statistics and data visualization techniques to understand the data. Some of the key metrics that will be computed include:\n\nWhat is the distribution of conservation status for animals?\nAre certain types of species more likely to be endangered?\nAre the differences between species and their conservation status significant?\nWhich species were spotted the most at each park?\n\n\n\n\nLastly, the project will revisit its initial goals and summarize the findings using the research questions. This section will also suggest additional questions which may expand on limitations in the current analysis and further guide future analyses on the subject."
  },
  {
    "objectID": "projects/biodiversity.html#importing-modules-and-data-from-files",
    "href": "projects/biodiversity.html#importing-modules-and-data-from-files",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Importing Modules and Data from Files",
    "text": "Importing Modules and Data from Files\nFirst, we will import the preliminary modules for this project, along with the data from the two separate files provided for this analysis.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\n# Set default figure size\n# figsize = (15,9)\nfigsize = (10,6)\nplt.rcParams['figure.figsize'] = figsize\nsns.set(rc={'figure.figsize':figsize})\n\n# Set default float size\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nobservations = pd.read_csv('observations.csv')\nspecies = pd.read_csv('species_info.csv')\nImport successful"
  },
  {
    "objectID": "projects/biodiversity.html#preview-the-data",
    "href": "projects/biodiversity.html#preview-the-data",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Preview the Data",
    "text": "Preview the Data\nTo prepare for our exploratory data analysis, we’ll first conduct an initial preview of the data. This will involve sampling a subset of the data and inspecting its structure and characteristics.\n\nspecies.csv\nLet’s begin by examening the species dataset.\ndisplay(\"SAMPLE OF SPECIES DATASET:\")\ndisplay(species.sample(5))\ndisplay(\"INFORMATION ABOUT THE SPECIES DATASET:\")\ndisplay(species.info())\n'SAMPLE OF SPECIES DATASET:'\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n718\nVascular Plant\nPogonia ophioglossoides\nPogonia, Rose Pogonia\nNaN\n\n\n1361\nVascular Plant\nLespedeza stuevei\nTall Lespedeza\nNaN\n\n\n4725\nVascular Plant\nCalycadenia mollis\nSoft Western Rosinweed\nNaN\n\n\n2912\nNonvascular Plant\nThuidium allenii\nAllen's Thuidium Moss\nNaN\n\n\n1929\nVascular Plant\nPicea abies\nNorway Spruce\nNaN\n\n\n\n'INFORMATION ABOUT THE SPECIES DATASET:'\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5824 entries, 0 to 5823\nData columns (total 4 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   category             5824 non-null   object\n 1   scientific_name      5824 non-null   object\n 2   common_names         5824 non-null   object\n 3   conservation_status  191 non-null    object\ndtypes: object(4)\nmemory usage: 182.1+ KB\n\nNone\nThe species dataset shows 5824 entries with four variables:\n\ncategory: taxonomy for each species.\nscientific_name: scientific name of each species.\ncommon_names: common names of each species.\nconservation_status: species’ conservation status.\n\nUpon inspection with .info(), we observe that the conservation_status column contains 191 non-null entries, indicating a high presence of missing values. While the majority of columns may retain their data type as objects, an argument could be made for converting conservation_status to an ordinal variable. However, due to the presence of incomplete conservation statuses and the ambiguity surrounding the ordinal nature of in recovery, we’ll retain it as an object.\n\n\nobservations.csv\nWe’ll now move on to the observations dataset.\ndisplay(\"SAMPLE OF SPECIES DATASET:\")\ndisplay(observations.sample(5))\ndisplay(\"INFORMATION ABOUT THE SPECIES DATASET:\")\ndisplay(observations.info())\n'SAMPLE OF SPECIES DATASET:'\n\n\n\n\n\n\n\n\n\n\nscientific_name\npark_name\nobservations\n\n\n\n\n21462\nLepomis humilis\nYellowstone National Park\n222\n\n\n1305\nSaxifraga odontoloma\nYosemite National Park\n116\n\n\n1307\nPerdix perdix\nYosemite National Park\n162\n\n\n20947\nFraxinus profunda\nBryce National Park\n129\n\n\n10240\nMuhlenbergia andina\nYellowstone National Park\n235\n\n\n\n'INFORMATION ABOUT THE SPECIES DATASET:'\n\n\nRangeIndex: 23296 entries, 0 to 23295\nData columns (total 3 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   scientific_name  23296 non-null  object\n 1   park_name        23296 non-null  object\n 2   observations     23296 non-null  int64 \ndtypes: int64(1), object(2)\nmemory usage: 546.1+ KB\n\nNone\nThe observations dataset consists of three columns:\n\nscientific_name: scientific name of each species.\npark_name: name of the national park species are located in.\nobservations: number of observations in the past 7 days.\n\nBased on the information above, the columns don’t show any missing data, and the data types seem to be appropriate for the analysis."
  },
  {
    "objectID": "projects/biodiversity.html#exploratory-data-analysis",
    "href": "projects/biodiversity.html#exploratory-data-analysis",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nspecies.csv\nLet’s delve deeper into the species dataset to gain insights into its characteristics and identify any anomalies or patterns. We’ll begin by employing a custom function column_eda() to analyze each column:\ndef column_eda(dataset):\n    cols = list(dataset.columns)\n    for col in cols:\n        print(f'---------------{col}---------------')\n        print(f'Unique values:', dataset[col].nunique(), \n              f'Non-null values: {dataset[col].notnull().sum()}',\n              f'Missing values: {dataset[col].isnull().sum()}\\n', \n              sep='\\n')\n        print(dataset[col].value_counts().head(4))\n\ncolumn_eda(species)\n---------------category---------------\nUnique values:\n7\nNon-null values: 5824\nMissing values: 0\n\ncategory\nVascular Plant       4470\nBird                  521\nNonvascular Plant     333\nMammal                214\nName: count, dtype: int64\n---------------scientific_name---------------\nUnique values:\n5541\nNon-null values: 5824\nMissing values: 0\n\nscientific_name\nCastor canadensis       3\nCanis lupus             3\nHypochaeris radicata    3\nColumba livia           3\nName: count, dtype: int64\n---------------common_names---------------\nUnique values:\n5504\nNon-null values: 5824\nMissing values: 0\n\ncommon_names\nBrachythecium Moss    7\nDicranum Moss         7\nPanic Grass           6\nBryum Moss            6\nName: count, dtype: int64\n---------------conservation_status---------------\nUnique values:\n4\nNon-null values: 191\nMissing values: 5633\n\nconservation_status\nSpecies of Concern    161\nEndangered             16\nThreatened             10\nIn Recovery             4\nName: count, dtype: int64\nThe function shows there are 7 categories of species, 5541 species, 5504 common names and 4 conservation statuses. From the analysis, several insights emerge:\n\nMissing Conversation Statuses: the conservation_status column exhibits a high number of nan values (5633), which could be interpreted as ‘species of no concern’ or requiring ‘no intervention’.\n\nTo address this, we’ll impute the missing values with “No intervention”, expanding the conservation status categories to five.\nprint('Old conservation status:\\n', list(species.conservation_status.unique()))\n\nspecies.conservation_status = species.conservation_status.fillna('No intervention')\n\nprint('New conservation status:\\n', list(species.conservation_status.unique()))\nOld conservation status:\n [nan, 'Species of Concern', 'Endangered', 'Threatened', 'In Recovery']\nNew conservation status:\n ['No intervention', 'Species of Concern', 'Endangered', 'Threatened', 'In Recovery']\n{:start=“2”} 2. Duplicate Entries: there is a discrepancy between the number of unique values of scientific_name and common_names despite all entries having non-null values. This points to the presence of duplicate common names for different species.\nWe’ll confirm this by identifying and examining these duplicates.\nduplicates = species.duplicated().sum()\nprint(f'Overall duplicates (rows): {duplicates}')\n\nrepeated_scientific_names = species.duplicated(subset=['scientific_name']).sum()\nprint(f'Duplicated scientific names: {repeated_scientific_names}')\n\nrepeated_common_names = species.duplicated(subset=['common_names']).sum()\nprint(f'Duplicated common names: {repeated_common_names}')\nOverall duplicates (rows): 0\nDuplicated scientific names: 283\nDuplicated common names: 320\nTo illustrate, we’ll display the most frequent common name alongside its associated scientific names.\ndisplay(species.common_names.value_counts().reset_index()[:5])\ndisplay(species.query(\"common_names == 'Brachythecium Moss'\")[['common_names', 'scientific_name']])\n\n\n\n\ncommon_names\ncount\n\n\n\n\n0\nBrachythecium Moss\n7\n\n\n1\nDicranum Moss\n7\n\n\n2\nPanic Grass\n6\n\n\n3\nBryum Moss\n6\n\n\n4\nSphagnum\n6\n\n\n\n\n\n\n\ncommon_names\nscientific_name\n\n\n\n\n2812\nBrachythecium Moss\nBrachythecium digastrum\n\n\n2813\nBrachythecium Moss\nBrachythecium oedipodium\n\n\n2814\nBrachythecium Moss\nBrachythecium oxycladon\n\n\n2815\nBrachythecium Moss\nBrachythecium plumosum\n\n\n2816\nBrachythecium Moss\nBrachythecium rivulare\n\n\n2817\nBrachythecium Moss\nBrachythecium rutabulum\n\n\n2818\nBrachythecium Moss\nBrachythecium salebrosum\n\n\n\nAs seen above, the most frequent common name is Brachythecium Moss, with a total of 7 different species identified with this name. Organisms in this example all share the same genus (i.e. brachythecium, a genus of moss), but differ in species, thus the different scientific names.\nThis demonstrates instances where multiple species share identical common names but differ in scientific nomenclature.\n{:start=“3”} 3. Duplicate Scientific Names: the presence of duplicate scientific names suggests repeated observations of the same species, since the dataset should report the conservation status of each species, thus one observation per species.\nSince there are no overall duplicates in the dataset (see above), these duplicate names must have some difference at the row level. To confirm this, we’ll print out a sample of duplicates and inspect three random duplicates species, to see what kind of differences are there within the rows themselves.\nduplicated_species = species[species['scientific_name'].duplicated(keep=False)]\n\ndisplay('-------Sample of duplicated scientific names-------')\ndisplay(duplicated_species.head())\n\ndef display_duplicated_species(scientific_name):\n    duplicated_entries = duplicated_species[duplicated_species['scientific_name'] == scientific_name]\n    display(f'-------Duplicated \\'{scientific_name}\\'-------')\n    display(duplicated_entries)\n\nscientific_names_to_check = ['Cervus elaphus', 'Canis lupus', 'Odocoileus virginianus']\nfor scientific_name in scientific_names_to_check:\n    display_duplicated_species(scientific_name)\n'-------Sample of duplicated scientific names-------'\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n6\nMammal\nSus scrofa\nFeral Hog, Wild Pig\nNo intervention\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n10\nMammal\nUrocyon cinereoargenteus\nCommon Gray Fox, Gray Fox\nNo intervention\n\n\n\n\"-------Duplicated 'Cervus elaphus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n3017\nMammal\nCervus elaphus\nRocky Mountain Elk\nNo intervention\n\n\n\n\"-------Duplicated 'Canis lupus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n3020\nMammal\nCanis lupus\nGray Wolf, Wolf\nIn Recovery\n\n\n4448\nMammal\nCanis lupus\nGray Wolf, Wolf\nEndangered\n\n\n\n\"-------Duplicated 'Odocoileus virginianus'-------\"\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n3019\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer, White-Tailed Deer\nNo intervention\n\n\n\nIt seems that both the number of common names and the types of conservation statuses are different for duplicate observations. That is, the same species exhibits both different common names, as well as conservation statuses. To solve the question of duplicates, given the differences in conversation statuses do not affect our question on the likelihood of endangerment given a species’ protection status, I’ll retain the first instance of these duplicates.\nspecies = species.drop_duplicates(subset=['scientific_name'], keep='first')\n\nrepeated_scientific_names = species.scientific_name[species.scientific_name.duplicated()]\nprint(f'Duplicated scientific names: {len(repeated_scientific_names)}\\n')\n\nprint('-------Previously duplicated examples (now clean)-------')\nscientific_names_to_check = ['Cervus elaphus', 'Canis lupus', 'Odocoileus virginianus']\ndisplay(species[species['scientific_name'].isin(scientific_names_to_check)])\nDuplicated scientific names: 0\n\n-------Previously duplicated examples (now clean)-------\n\n\n\n\n\n\n\n\n\n\n\ncategory\nscientific_name\ncommon_names\nconservation_status\n\n\n\n\n4\nMammal\nCervus elaphus\nWapiti Or Elk\nNo intervention\n\n\n5\nMammal\nOdocoileus virginianus\nWhite-Tailed Deer\nNo intervention\n\n\n8\nMammal\nCanis lupus\nGray Wolf\nEndangered\n\n\n\n\n\nobservations.csv\nLet’s extend our exploratory analysis to the observations dataset, mirroring the approach applied to the species dataset. We’ll begin by employing the column_eda() function to analyze each column.\ncolumn_eda(observations)\n---------------scientific_name---------------\nUnique values:\n5541\nNon-null values: 23296\nMissing values: 0\n\nscientific_name\nMyotis lucifugus        12\nPuma concolor           12\nHypochaeris radicata    12\nHolcus lanatus          12\nName: count, dtype: int64\n---------------park_name---------------\nUnique values:\n4\nNon-null values: 23296\nMissing values: 0\n\npark_name\nGreat Smoky Mountains National Park    5824\nYosemite National Park                 5824\nBryce National Park                    5824\nYellowstone National Park              5824\nName: count, dtype: int64\n---------------observations---------------\nUnique values:\n304\nNon-null values: 23296\nMissing values: 0\n\nobservations\n84    220\n85    210\n91    206\n92    203\nName: count, dtype: int64\nThe column analysis revelas the following insights. There are 23296 observations of 5541 unique species documented in 4 parks. The number of species (scientific_name) in the observations datset coincides with the number of species in the species dataset. This suggest that the observations dataset contains observations of all species in the species dataset. To confirm this, we’ll check if the scientific_name column in the observations dataset is a subset of the scientific_name column in the species dataset.\nspecies_names = species.scientific_name\nobservations_names = observations.scientific_name\n\nprint(f'Is the observations dataset a subset of the species dataset? {observations_names.isin(species_names).all()}')\nIs the observations dataset a subset of the species dataset? True\nThe result confirms that the observations dataset is a subset of the species dataset, as all species in the observations dataset are also present in the species dataset.\nFurthermore, as observations is a numerical variable, its distribution provides insights into the frequency of species sightings. To better explore this column given its data type, we’ll visualize the distribution using a histogram.\nsns.histplot(x='observations', data=observations, kde=True)\nplt.show()\n\nThe distribution of in the number of observations seems to follow a multimodal distribution, with at least three discernible peaks in the data: one at 80, another at 150, and a third at 250. This may suggest that the overall distribution is a combination of several distributions, grouped by a certain variable. Given the low number of disceernible peaks, this variable might be the park_name variable. That is: the distribution in the number of observations may be influenced by the size of the parks they were made in.\nTo confirm this, we’ll plot the distribution of observations per park using the hue parameter in the seaborn histplot function.\nsns.histplot(x='observations', data=observations, kde=True, hue='park_name')\nplt.show()\n\nAs suspected, the distribution of observations is indeed influenced by the park in which they were made. The peaks in the distribution clearly correspond to each of the four parks in the dataset. This proves that the number of observations is influenced by the park in which they were made.\n\n\nSummary\nTo encapsulate the insights obtained from our Exploratory Data Analysis (EDA), we present the key characteristics of both datasets.\n\nspecies\n\nDataset Overview: the data comprises 5,824 entries with 4 variables—category, scientific_name, common_names, and conservation_status—offering a diverse array of taxonomic information.\nMissing Values: the conservation_status column contains 5,633 missing values, which were imputed with “No intervention” to account for species not under any conservation status.\nDuplicates: the dataset contains no overall duplicates, but does exhibit duplicate scientific names, which were resolved by retaining the first instance of each duplicate.\nCommon Names: the dataset contains 5541 species, with some sharing identical common names but differing in scientific nomenclature.\nConservation Status: the dataset reports 5 conservation statuses, with most species not under any conservation status.\n\n\n\nobservations\n\nDataset Overview: the data consists of 23,296 entries with 3 variables—scientific_name, park_name, and observations—documenting species sightings in 4 national parks over 7 days.\nUnique Species: the dataset contains observations of 5,541 unique species, all of which are present in the species dataset.\nMissing Values: the dataset contains no missing values, with all columns having non-null entries.\nDistribution: the number of observations followed a multimodal distribution, which was influenced by the park in which observations were conducted."
  },
  {
    "objectID": "projects/biodiversity.html#analysis-1",
    "href": "projects/biodiversity.html#analysis-1",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Analysis",
    "text": "Analysis\nIn this section, we aim to address the questions posed earlier by analyzing the species dataset and later exploring the observations dataset.\n\nQ: What is the distribution of conservation status for animals?\nTo gain insights into the distribution of conservation statuses among animal categories, we begin by aggregating the conservations statuses per species category and calculating both discrete and normalized counts. We then visualize the normalized counts using a stacked bar chart.\ncategory_conservation = pd.crosstab(species['conservation_status'], species['category']).drop(index='No intervention')\ndisplay(category_conservation)\n\ncategory_conservation_norm = pd.crosstab(species['conservation_status'], species['category'], normalize='index').drop(index='No intervention')\ndisplay(category_conservation_norm.style.background_gradient(cmap='Blues', axis=1, vmin=0, vmax=1))\n\nax = category_conservation_norm.plot(kind='bar', stacked=True)\nax.set_xlabel('Conservation Status')\nax.set_ylabel('Number of Species')\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.title(\"Distribution of Species Among Conservation Statuses\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\nAmphibian\nBird\nFish\nMammal\nNonvascular Plant\nReptile\nVascular Plant\n\n\n\n\nconservation_status\n\n\n\n\n\n\n\n\n\nEndangered\n1\n4\n3\n6\n0\n0\n1\n\n\nIn Recovery\n0\n3\n0\n0\n0\n0\n0\n\n\nSpecies of Concern\n4\n68\n4\n22\n5\n5\n43\n\n\nThreatened\n2\n0\n3\n2\n0\n0\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory\nAmphibian\nBird\nFish\nMammal\nNonvascular Plant\nReptile\nVascular Plant\n\n\n\n\nconservation_status\n \n \n \n \n \n \n \n\n\nEndangered\n0.066667\n0.266667\n0.200000\n0.400000\n0.000000\n0.000000\n0.066667\n\n\nIn Recovery\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nSpecies of Concern\n0.026490\n0.450331\n0.026490\n0.145695\n0.033113\n0.033113\n0.284768\n\n\nThreatened\n0.222222\n0.000000\n0.333333\n0.222222\n0.000000\n0.000000\n0.222222\n\n\n\n\nThe table and stacked bar chart above reveal several insights about the distribution of conservation status among different categories of species.\nFirstly, the only animal in recovery are birds, of which there are 3 species making up 100% of this status. This points to the fact that birds are the only species in recovery at the time of the dataset. Moreover, mammals, birds and fish are the most endangered species in the dataset, making more than 85% of all endangered species. Furthermore, more than 70% of species of concern consist of birds and vascular plants. Lastly, the threatened status is almost equally distributed among all species categories, except birds, nonvascular plants and reptiles.\nOverall, the distribution of animals among conservations statuses support the following conclusions:\n\nThe most endangered animals in the dataset consist of mammals, birds and fishes.\nBirds are the only species in recovery, with only 3 species documented.\nThe most common conservation status is species of concern, with birds and vascular plants making up the majority of this category.\nThe threatened status is almost equally distributed among amphibians, fish, mammals and vascular plants.\n\n\n\nQ: Are certain types of species more likely to be endangered?\nThe next question concerns the relation between species and their conservation status. To answer this question requires establishing a definition of likelihood for endangerment. Given protection measures are not documented in the dataset, we can only establish a definition based on the available variables. Therefore, we consider species to be more likely to be engangered if they are classified as endangered, threatened, or species of concern and if no protection measures are placed in response to their endangerment.\nTo answer this question, we create a new protected column with True for all conservations statuses that are not No intervention nor In recovery. We then calculate the relative frequencies of protected and protected species per category. We visualize the results then using a bar chart.\nspecies['protected'] = species.conservation_status.isin(['No intervention', 'In Recovery'])\n\ncategory_protections = pd.crosstab(species['category'], species['protected'], normalize='index')\ndisplay(category_protections)\n\nax = sns.barplot(data = category_protections, y = category_protections.iloc[:, 0]*100, x = 'category')\nax.bar_label(ax.containers[0], fmt=\"%0.2f%%\")\nplt.title('Percentage of Likely Endangerement per Species Category')\nplt.ylabel('Percentage Not Protected')\nplt.xlabel('Category')\nplt.show()\n\n\n\nprotected\nFalse\nTrue\n\n\n\n\ncategory\n\n\n\n\nAmphibian\n0.09\n0.91\n\n\nBird\n0.15\n0.85\n\n\nFish\n0.08\n0.92\n\n\nMammal\n0.17\n0.83\n\n\nNonvascular Plant\n0.02\n0.98\n\n\nReptile\n0.06\n0.94\n\n\nVascular Plant\n0.01\n0.99\n\n\n\n\nBased on the information from the bar chart, we can see that mammals and birds have the highest percentage of no protection, with roughly 17% and 15% of species exhibiting some level of engangered, respectively. This suggests that mammals and birds are the most likely to be endangered among the categories.\n\n\nQ: Are the differences between species and their conservation status significant?\nThe question of statistical significance for categorical variables is answered in statistics by use of the chi-square test.\nCrosstabulating both variables would yield a complex result, thus it’s better to break down the question into pairs of species categories. Since based on the previous question mammals are the most likely category to be endangered, we’ll compare the significance of other category differences with mammals.\nWe’ll start by permutating the pairs of categories with mammals. Then I’ll loop over this list to perform the chi-square tests for each pair and plot the p-values to find the statistically significant differences among category pairs.\ncategories = list(species.category.unique())\ncombinations_mammal = [['Mammal', i] for i in categories][1:]\n\ncategory_protections_counts = pd.crosstab(species['category'], species['protected'])\n\nsignificance_data = {'Animal Pair': [], 'p-value': []}\nfor pair in combinations_mammal:\n  contingency_table = category_protections_counts.loc[pair]\n  chi2, pval, dof, expected = chi2_contingency(contingency_table)\n\n  significance_data['Animal Pair'].append(f'{pair[0]} vs {pair[1]}')\n  significance_data['p-value'].append(pval)\n\nsign_data = pd.DataFrame(significance_data)\nsign_data['p-value'] = sign_data['p-value']*100\n# display(sign_data)\n\n# Plot\nplt.subplots(figsize=(10,5))\nax =sns.barplot(data = sign_data, x = 'Animal Pair', y = 'p-value')\nplt.title('Statistical Significance of Protection Statuses per Animal\\n(difference with mammals)')\nplt.axhline(5, color='red', linestyle='--')\nax.set_xlabel(\"\")\nax.set_ylabel('p-value\\n(alpha = 5%)')\nplt.xticks(rotation=45)\nax.bar_label(ax.containers[0], fmt=\"%0.2f%%\")\nplt.show()\n\nThe above graph illustrates the p-values for the chi-square tests performed for each animal category against mammals. Given an alpha of 5%, the analysis shows that birds and amphibians display no statistically significant differences in their conservations statuses compared with mammals. However, all other categories such as reptiles, fishes and plants show statistically significant differences in their conservation statuses when comapred to mammals. This means that the conservation statuses of these categories are significantly different from mammals.\n\n\nQ: Which species were spotted the most at each park?\nLastly, we explore the observations dataset to identify the most frequently spotted species in each park.\nSince the dataset doesn’t include common names, we’ll map the common names from the species dataset to the scientific names in the observations dataset. Then, we’ll aggregate the data by park and by species, summing their observations to identify the most frequently spotted species in each park.\nmerged_df = observations.merge(species[['category', 'scientific_name', 'common_names']], how='left').drop_duplicates()\nmerged_df_grouped = merged_df.groupby(['park_name', 'scientific_name', 'common_names']).observations.sum().reset_index()\nmerged_df_grouped = merged_df_grouped.loc[merged_df_grouped.groupby('park_name')['observations'].idxmax()].sort_values(by = 'observations', ascending=False)\n\ndisplay(merged_df_grouped.head())\n\n\n\n\n\n\n\n\n\n\n\npark_name\nscientific_name\ncommon_names\nobservations\n\n\n\n\n13534\nYellowstone National Park\nHolcus lanatus\nCommon Velvet Grass, Velvetgrass\n805\n\n\n19178\nYosemite National Park\nHypochaeris radicata\nCat's Ear, Spotted Cat's-Ear\n505\n\n\n1359\nBryce National Park\nColumba livia\nRock Dove\n339\n\n\n10534\nGreat Smoky Mountains National Park\nStreptopelia decaocto\nEurasian Collared-Dove\n256\n\n\n\nBased on the aggregation above, in Yellowstone National Park, the species Holcus lanatus was the most commonly observed, with a total of 805 sightings. Meanwhile, Hypochaeris radicata was the predominant species in Yosemite National Park, with 505 observations. In Bryce National Park, Columba livia garnered the highest number of sightings, totaling 339. Finally, in Great Smoky Mountains National Park, Streptopelia decaocto was the most frequently spotted species, with 256 observations."
  },
  {
    "objectID": "projects/biodiversity.html#conclusions",
    "href": "projects/biodiversity.html#conclusions",
    "title": "Biodiversity, Endangerement and Conversation in Data from National Parks Service",
    "section": "Conclusions",
    "text": "Conclusions\nThis project set out to explore biodiversity data from the National Parks Service, focusing on endangered species and their conservation statuses. Through a detailed exploratory data analysis, several key findings emerged, shedding light on the distribution of conservation statuses among different species categories, the likelihood of species endangerment, the significance of differences in conservation statuses among species categories, and the most frequently spotted species in each national park.\n\nDistribution of Conservation Statuses\nThe analysis revealed that mammals, birds, and fishes are the most endangered species categories, making up the majority of the endangered conservation status. Birds were the only category with species classified as in recovery, indicating a unique conservation status among all the categories. Out of 178 species marked with some conservation status other than no intervention, most species are under the status of species of concern, especially birds and vascular plants.\n\n\nLikelihood of Species Endangerment\nMammals and birds emerged as the most likely categories to be endangered, with approximately 17% and 15% of species not classified as either in recovery or no intervention. Without any protection measures, this suggests a higher vulnerability to endangerment among mammals and birds compared to other species categories.\n\n\nSignificance of Conservation Status Differences\nStatistical significance testing showed that birds and amphibians did not exhibit statistically significant differences in their conservation statuses compared with mammals. However, all other categories, including reptiles, fishes, and plants, displayed significant differences in conservation statuses compared with mammals. This highlights the importance of considering species-specific conservation measures based on their unique characteristics.\n\n\nMost Frequently Spotted Species\nThe analysis identified the most frequently spotted species in each national park. Species such as common velvet grass, a vascular plant, in Yellowstome National Park. Moreover, doves were the most commonly observed species both in Bryce and Great Smoky Mountains National Parks. Furthermore, the most observed species Yosemite National Park was the cat’s ear plant. This findings are examples of the rich biodiversity present in national parks.\nIn conclusion, this project contributes to our understanding of endangered species and their conservation statuses, highlighting the need for targeted conservation efforts to protect vulnerable species and preserve biodiversity in national parks. Further research could explore additional factors influencing species endangerment and conservation strategies tailored to specific species categories. By understanding and honoring the unique needs of each species category, we can forge a path towards sustainable coexistence and ensure the enduring legacy of our national parks for generations to come."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About\nHello! I’m Marco Camilo, a seasoned professional with a diverse background bridging linguistics, education, and cutting-edge technology.\nDuring my time as a lecturer, I found immense fulfillment in simplifying complex linguistic concepts and guiding students to exceed their own expectations. By analyzing student data—from test scores to speech lab performance and even Duolingo statistics—I employed advanced clustering techniques to tailor instructional strategies and unlock the full potential of every learner.\nMy academic journey, including coursework in machine learning and natural language processing (NLP), has equipped me with practical skills and deep insights into applying state-of-the-art solutions to real-world challenges. Projects in NLP provided compelling opportunities to excel in using cutting-edge technologies, preparing me for impactful contributions in the IT sector.\nMy transition from linguistics to data science and NLP stems from a lifelong fascination with language, mathematics, and algorithms. As a linguist, I delved into the intricate workings of language systems, craving to apply analytical and mathematical methods to linguistic inquiries. This quest led me to the field of NLP, where my passions converge, empowering me to derive innovative solutions to modern challenges—from both linguistic and computational perspectives.\nI am excited to leverage my unique background and expertise to tackle complex problems in NLP and contribute meaningfully to the evolving landscape of technology.\n\n\n\n\n\n\nI am Marco Camilo,\nan NLP Data Scientist &\nComputational Linguist.\n\n\n  LinkedIn \n\n\n  Twitter \n\n\n  Github \n\n\n  Email"
  }
]